{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SilverLingua","text":"<p>A type-safe framework for building AI agents using atomic design patterns and hierarchical memory.</p>"},{"location":"#why-silverlingua","title":"Why SilverLingua?","text":"<p>Unlike LangChain's complex chains and callbacks, SilverLingua provides:</p> <ul> <li>Type Safety: Every component is a Pydantic model - no more runtime surprises</li> <li>Atomic Design: Build complex agents from simple, composable pieces</li> <li>Clean Interfaces: Consistent patterns across the entire framework</li> <li>Provider Agnostic: Switch between LLMs without changing your agent logic</li> </ul>"},{"location":"#core-architecture","title":"Core Architecture","text":""},{"location":"#memory-system","title":"Memory System","text":"<pre><code># Base memory unit - simple and type-safe\nclass Memory(BaseModel):\n    content: str\n\n# Role-aware memory with validation\nclass Notion(Memory):\n    role: str\n    persistent: bool = False\n</code></pre>"},{"location":"#idearium-extensible-memory-management","title":"Idearium - Extensible Memory Management","text":"<pre><code># LangChain's ConversationBufferMemory - Complex inheritance, mixed concerns\nclass ConversationBufferMemory(BaseChatMemory):\n    return_messages: bool = False\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n\n    @property\n    def buffer(self) -&gt; Any:  # Type hints lost\n        if self.return_messages: ...\n        else: ...\n\n# SilverLingua's Idearium - Clean, composable, token-aware\nclass Idearium(BaseModel):\n    tokenizer: Tokenizer\n    max_tokens: int\n    notions: List[Notion]\n    persistent_indices: set\n\n    def _trim(self):\n        \"\"\"Extension point for custom trimming strategies\"\"\"\n        while self.total_tokens &gt; self.max_tokens:\n            # Default implementation\n            pass\n\n# Easy to extend for different memory strategies\nclass SummarizingIdearium(Idearium):\n    def _trim(self):\n        \"\"\"Custom trimming with automatic summarization\"\"\"\n        if self.total_tokens &gt; self.max_tokens:\n            # Summarize oldest non-persistent memories\n            summary = self.summarize_notions(self.get_trim_candidates())\n            self.replace_with_summary(summary)\n\nclass RAGIdearium(Idearium):\n    def __init__(self, vector_store, **kwargs):\n        super().__init__(**kwargs)\n        self.vector_store = vector_store\n\n    def _trim(self):\n        \"\"\"Store trimmed memories in vector store\"\"\"\n        for notion in self.get_trim_candidates():\n            self.vector_store.add(notion.content)\n        super()._trim()\n\n    def get_context(self, query: str) -&gt; List[Notion]:\n        \"\"\"Retrieve relevant memories\"\"\"\n        return self.vector_store.similarity_search(query)\n</code></pre>"},{"location":"#provider-integration","title":"Provider Integration","text":"<pre><code># Clean model interface\nclass OpenAIModel:\n    def generate(self, prompt: str) -&gt; str:\n        # OpenAI-specific implementation\n        pass\n\n# Consistent agent pattern\nclass OpenAIChatAgent(Agent):\n    def _bind_tools(self) -&gt; None:\n        # Automatic OpenAI function calling format\n        self.model.completion_params.tools = [\n            {\"type\": \"function\", \"function\": tool.description}\n            for tool in self.tools\n        ]\n</code></pre>"},{"location":"#why-this-matters","title":"Why This Matters","text":""},{"location":"#langchain-memory","title":"LangChain Memory","text":"<pre><code># Complex setup, multiple inheritance, mixed concerns\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True,\n    output_key=\"output\",\n    input_key=\"input\",\n)\nchain = LLMChain(\n    llm=llm,\n    memory=memory,\n    prompt=prompt,\n    verbose=True\n)\n</code></pre>"},{"location":"#silverlingua-memory","title":"SilverLingua Memory","text":"<pre><code># Clean, composable, type-safe\nidearium = RAGIdearium(\n    vector_store=vector_store,\n    tokenizer=tokenizer,\n    max_tokens=4096\n)\n\n# Easy to use\nidearium.append(Notion(\"Important fact\", persistent=True))\nidearium.extend(new_memories)  # Auto-manages tokens\n\n# Automatic context management\nrelevant_context = idearium.get_context(\"query\")\n</code></pre>"},{"location":"#key-benefits","title":"Key Benefits","text":"<ol> <li> <p>Extensible Memory</p> </li> <li> <p>Clear extension points (<code>_trim</code>, etc.)</p> </li> <li>Type-safe customization</li> <li> <p>Easy integration with external systems</p> </li> <li> <p>Token Management</p> </li> <li> <p>Automatic token tracking</p> </li> <li>Smart memory trimming</li> <li> <p>Persistent memory support</p> </li> <li> <p>Clean Architecture</p> </li> <li>Single responsibility principle</li> <li>Consistent interfaces</li> <li>Type-safe operations</li> </ol>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>API Reference</li> <li>Examples</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>options: show_root_heading: true show_source: true</p>"},{"location":"api/#SilverLingua","title":"<code>SilverLingua</code>","text":"<p>SilverLingua - An AI agent framework</p>"},{"location":"api/#SilverLingua-classes","title":"Classes","text":""},{"location":"api/#SilverLingua.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A wrapper around a model that utilizes an Idearium and a set of Tools.</p> <p>This is a base class not meant to be used directly. It is meant to be subclassed by specific model implementations.</p> <p>However, there is limited boilerplate. The only thing that needs to be redefined in subclasses is the <code>_bind_tools</code> method.</p> <p>Additionally, the <code>_use_tools</code> method is a common method to redefine.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>class Agent(BaseModel):\n    \"\"\"\n    A wrapper around a model that utilizes an Idearium and a set of Tools.\n\n    This is a base class not meant to be used directly. It is meant to be\n    subclassed by specific model implementations.\n\n    However, there is limited boilerplate. The only thing that needs to be\n    redefined in subclasses is the `_bind_tools` method.\n\n    Additionally, the `_use_tools` method is a common method to redefine.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    model: Model\n    idearium: Idearium\n    \"\"\"\n    The Idearium used by the agent.\n    \"\"\"\n    tools: List[Tool]\n    \"\"\"\n    The tools used by the agent.\n\n    WARNING: Do not modify this list directly. Use `add_tool`, `add_tools`,\n    and `remove_tool` instead.\n    \"\"\"\n    auto_append_response: bool = True\n    \"\"\"\n    Whether to automatically append the response to the idearium after\n    generating a response.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        idearium: Optional[Idearium] = None,\n        tools: Optional[List[Tool]] = None,\n        auto_append_response: bool = True,\n    ):\n        \"\"\"\n        Initializes the agent.\n\n        Args:\n            model (Model): The model to use.\n            idearium (Idearium, optional): The idearium to use.\n                If None, a new one will be created.\n            tools (List[Tool], optional): The tools to use.\n        \"\"\"\n        super().__init__(\n            model=model,\n            idearium=idearium\n            or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n            tools=tools or [],\n            auto_append_response=auto_append_response,\n        )\n\n    def model_post_init(self, __content):\n        self._bind_tools()\n\n    @property\n    def model(self) -&gt; Model:\n        \"\"\"\n        The model used by the agent.\n        \"\"\"\n        return self.model\n\n    @property\n    def role(self) -&gt; ChatRole:\n        \"\"\"\n        The ChatRole object for the model.\n        \"\"\"\n        return self.model.role\n\n    def _find_tool(self, name: str) -&gt; Tool | None:\n        \"\"\"\n        Finds a tool by name.\n        \"\"\"\n        for t in self.tools:\n            if t.name == name:\n                return t\n        return None\n\n    def _use_tools(self, tool_calls: ToolCalls) -&gt; List[Notion]:\n        \"\"\"\n        Uses Tools based on the given ToolCalls, returning Notions\n        containing ToolCallResponses.\n\n        Args:\n            tool_calls (ToolCalls): The ToolCalls to use.\n\n        Returns:\n            List[Notion]: The Notions containing ToolCallResponses.\n                Each Notion will have a role of ChatRole.TOOL_RESPONSE.\n        \"\"\"\n        responses: List[Notion] = []\n        for tool_call in tool_calls.list:\n            tool = self._find_tool(tool_call.function.name)\n            if tool is not None:\n                tc_function_response = {}\n                with contextlib.suppress(json.JSONDecodeError):\n                    tc_function_response = json.loads(tool_call.function.arguments)\n\n                tc_response = ToolCallResponse.from_tool_call(\n                    tool_call=tool_call, response=tool(**tc_function_response)\n                )\n                responses.append(\n                    Notion(\n                        content=tc_response.model_dump_json(exclude_none=True),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n            else:\n                responses.append(\n                    Notion(\n                        content=json.dumps(\n                            {\n                                \"tool_call_id\": tool_call.id,\n                                \"content\": \"Tool not found\",\n                                \"name\": \"error\",\n                            }\n                        ),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n        return responses\n\n    def _bind_tools(self) -&gt; None:\n        \"\"\"\n        Called at the end of __init__ to bind the tools to the model.\n\n        This MUST be redefined in subclasses to dictate how\n        the tools are bound to the model.\n\n        Example:\n        ```python\n        # From OpenAIChatAgent\n        def _bind_tools(self) -&gt; None:\n            m_tools: List[ChatCompletionToolParam] = [\n                {\"type\": \"function\", \"function\": tool.description}\n                for tool in self.tools\n            ]\n\n            if len(m_tools) &gt; 0:\n                self.model.tools = m_tools\n        ```\n        \"\"\"\n        pass\n\n    def add_tool(self, tool: Tool) -&gt; None:\n        \"\"\"\n        Adds a tool to the agent.\n        \"\"\"\n        self.tools.append(tool)\n        self._bind_tools()\n\n    def add_tools(self, tools: List[Tool]) -&gt; None:\n        \"\"\"\n        Adds a list of tools to the agent.\n        \"\"\"\n        self.tools.extend(tools)\n        self._bind_tools()\n\n    def remove_tool(self, name: str) -&gt; None:\n        \"\"\"\n        Removes a tool from the agent.\n        \"\"\"\n        for i, tool in enumerate(self.tools):\n            if tool.name == name:\n                self.tools.pop(i)\n                break\n        self._bind_tools()\n\n    def _process_messages(self, messages: Messages) -&gt; List[Notion]:\n        \"\"\"Convert various message types into a list of Notions.\"\"\"\n        if isinstance(messages, str):\n            return [Notion(content=messages, role=str(self.role.HUMAN.value))]\n        elif isinstance(messages, Notion):\n            return [messages]\n        elif isinstance(messages, Idearium):\n            return messages.notions\n        elif isinstance(messages, list):\n            return [\n                (\n                    Notion(content=msg, role=str(self.role.HUMAN.value))\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        raise ValueError(f\"Unsupported message type: {type(messages)}\")\n\n    def _process_generation(\n        self, responses: List[Notion], is_async=False\n    ) -&gt; List[Notion]:\n        \"\"\"Wrapper around shared logic between generate and agenerate.\"\"\"\n        response = responses[0]\n        # logger.debug(f\"Response: {response}\")\n        if response.chat_role == ChatRole.TOOL_CALL:\n            # logger.debug(\"Tool call detected\")\n            # Add the tool call to the idearium\n            self.idearium.append(response)\n            # Call generate again with the tool response\n            tool_calls = ToolCalls.model_validate_json(\n                '{\"list\": ' + response.content + \"}\"\n            )\n            tool_response = self._use_tools(tool_calls)\n            # logger.debug(f\"Tool response: {tool_response}\")\n            if is_async:\n                return self.agenerate(tool_response)\n            else:\n                return self.generate(tool_response)\n        else:\n            return responses\n\n    def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Generates a response to the given messages by calling the\n        underlying model's generate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = self.model.generate(self.idearium, **kwargs)\n        result = self._process_generation(responses)\n\n        if self.auto_append_response:\n            self.idearium.extend(result)\n\n        return result\n\n    async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Asynchronously generates a response to the given messages by calling the\n        underlying model's agenerate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = await self.model.agenerate(self.idearium, **kwargs)\n        result = self._process_generation(responses, True)\n        r = await result if asyncio.iscoroutine(result) else result\n\n        if self.auto_append_response:\n            self.idearium.extend(r)\n\n        return r\n\n    def _process_tool_calls(self, tool_calls: ToolCalls):\n        \"\"\"\n        Processes tool calls and returns the tool response.\n\n        Args:\n            tool_calls (ToolCalls): The tool calls to process.\n\n        Returns:\n            Optional[ToolCalls]: The tool response. If None, no tool calls were found.\n        \"\"\"\n        for i, tool_call in enumerate(tool_calls.list):\n            if not tool_call.id.startswith(\"call_\"):\n                # Something went wrong and this tool call is not valid\n                tool_calls.list.pop(i)\n                logger.error(\n                    \"Invalid tool call: \"\n                    + f\"{tool_call.model_dump_json(exclude_none=True)}\"\n                )\n\n        tc_dump = tool_calls.model_dump(exclude_none=True)\n        if tc_dump.get(\"list\"):\n            logger.debug(f\"Tool calls: {tc_dump}\")\n\n            # Create a new notion from the tool calls\n            tc_notion = Notion(\n                content=json.dumps(tc_dump.get(\"list\")),\n                role=str(ChatRole.TOOL_CALL.value),\n            )\n\n            # Add the tool call to the idearium\n            self.idearium.append(tc_notion)\n            # Call stream again with the tool response\n            tool_response = self._use_tools(tool_calls)\n            return tool_response\n        else:\n            logger.error(\"No tool calls found\")\n            return None\n\n    def stream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Streams a response to the given prompt by calling the\n        underlying model's stream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.stream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in stream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                for r in self.stream(tool_response):\n                    yield r\n\n    async def astream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Asynchronously streams a response to the given prompt by calling the\n        underlying model's astream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.astream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        async for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in astream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                async for r in self.astream(tool_response):\n                    yield r\n</code></pre>"},{"location":"api/#SilverLingua.Agent-attributes","title":"Attributes","text":""},{"location":"api/#SilverLingua.Agent.auto_append_response","title":"<code>auto_append_response: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to automatically append the response to the idearium after generating a response.</p>"},{"location":"api/#SilverLingua.Agent.idearium","title":"<code>idearium: Idearium</code>  <code>instance-attribute</code>","text":"<p>The Idearium used by the agent.</p>"},{"location":"api/#SilverLingua.Agent.model","title":"<code>model: Model</code>  <code>property</code>","text":"<p>The model used by the agent.</p>"},{"location":"api/#SilverLingua.Agent.role","title":"<code>role: ChatRole</code>  <code>property</code>","text":"<p>The ChatRole object for the model.</p>"},{"location":"api/#SilverLingua.Agent.tools","title":"<code>tools: List[Tool]</code>  <code>instance-attribute</code>","text":"<p>The tools used by the agent.</p> <p>WARNING: Do not modify this list directly. Use <code>add_tool</code>, <code>add_tools</code>, and <code>remove_tool</code> instead.</p>"},{"location":"api/#SilverLingua.Agent-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.Agent.__init__","title":"<code>__init__(model, idearium=None, tools=None, auto_append_response=True)</code>","text":"<p>Initializes the agent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use.</p> required <code>idearium</code> <code>Idearium</code> <p>The idearium to use. If None, a new one will be created.</p> <code>None</code> <code>tools</code> <code>List[Tool]</code> <p>The tools to use.</p> <code>None</code> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def __init__(\n    self,\n    model: Model,\n    idearium: Optional[Idearium] = None,\n    tools: Optional[List[Tool]] = None,\n    auto_append_response: bool = True,\n):\n    \"\"\"\n    Initializes the agent.\n\n    Args:\n        model (Model): The model to use.\n        idearium (Idearium, optional): The idearium to use.\n            If None, a new one will be created.\n        tools (List[Tool], optional): The tools to use.\n    \"\"\"\n    super().__init__(\n        model=model,\n        idearium=idearium\n        or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n        tools=tools or [],\n        auto_append_response=auto_append_response,\n    )\n</code></pre>"},{"location":"api/#SilverLingua.Agent.add_tool","title":"<code>add_tool(tool)</code>","text":"<p>Adds a tool to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tool(self, tool: Tool) -&gt; None:\n    \"\"\"\n    Adds a tool to the agent.\n    \"\"\"\n    self.tools.append(tool)\n    self._bind_tools()\n</code></pre>"},{"location":"api/#SilverLingua.Agent.add_tools","title":"<code>add_tools(tools)</code>","text":"<p>Adds a list of tools to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tools(self, tools: List[Tool]) -&gt; None:\n    \"\"\"\n    Adds a list of tools to the agent.\n    \"\"\"\n    self.tools.extend(tools)\n    self._bind_tools()\n</code></pre>"},{"location":"api/#SilverLingua.Agent.agenerate","title":"<code>agenerate(messages, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously generates a response to the given messages by calling the underlying model's agenerate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Asynchronously generates a response to the given messages by calling the\n    underlying model's agenerate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = await self.model.agenerate(self.idearium, **kwargs)\n    result = self._process_generation(responses, True)\n    r = await result if asyncio.iscoroutine(result) else result\n\n    if self.auto_append_response:\n        self.idearium.extend(r)\n\n    return r\n</code></pre>"},{"location":"api/#SilverLingua.Agent.astream","title":"<code>astream(messages, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously streams a response to the given prompt by calling the underlying model's astream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def astream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Asynchronously streams a response to the given prompt by calling the\n    underlying model's astream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.astream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    async for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in astream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            async for r in self.astream(tool_response):\n                yield r\n</code></pre>"},{"location":"api/#SilverLingua.Agent.generate","title":"<code>generate(messages, **kwargs)</code>","text":"<p>Generates a response to the given messages by calling the underlying model's generate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Generates a response to the given messages by calling the\n    underlying model's generate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = self.model.generate(self.idearium, **kwargs)\n    result = self._process_generation(responses)\n\n    if self.auto_append_response:\n        self.idearium.extend(result)\n\n    return result\n</code></pre>"},{"location":"api/#SilverLingua.Agent.remove_tool","title":"<code>remove_tool(name)</code>","text":"<p>Removes a tool from the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def remove_tool(self, name: str) -&gt; None:\n    \"\"\"\n    Removes a tool from the agent.\n    \"\"\"\n    for i, tool in enumerate(self.tools):\n        if tool.name == name:\n            self.tools.pop(i)\n            break\n    self._bind_tools()\n</code></pre>"},{"location":"api/#SilverLingua.Agent.stream","title":"<code>stream(messages, **kwargs)</code>","text":"<p>Streams a response to the given prompt by calling the underlying model's stream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def stream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Streams a response to the given prompt by calling the\n    underlying model's stream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.stream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in stream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            for r in self.stream(tool_response):\n                yield r\n</code></pre>"},{"location":"api/#SilverLingua.Config","title":"<code>Config</code>","text":"Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>class Config:\n    modules: List[Module] = []\n    chat_roles: List[type[ChatRole]] = [ChatRole]\n    react_roles: List[type[ReactRole]] = [ReactRole]\n    tools: List[Tool] = []\n\n    @classmethod\n    def get_chat_role(self, role: str) -&gt; Optional[ChatRole]:\n        \"\"\"\n        Attempts to get the standardized ChatRole enum from 'role'.\n        If not, returns None.\n\n        This is usually used internally for maintaining\n        consistency in Notions across different LLM backends.\n        \"\"\"\n        for enum_class in self.chat_roles:\n            for enum_member in enum_class:\n                if str(enum_member.value).lower() == str(role).lower():\n                    return ChatRole[enum_member.name]\n        return None\n\n    @classmethod\n    def get_react_role(self, role: str) -&gt; Optional[ReactRole]:\n        \"\"\"\n        Attempts to get the standardized ReactRole enum from 'role'.\n        If not, returns None.\n\n        This is usually used internally for maintaining\n        consistency in Notions across different LLM backends.\n        \"\"\"\n        for enum_class in self.react_roles:\n            for enum_member in enum_class:\n                if enum_member.value == role:\n                    return ReactRole[enum_member.name]\n        return None\n\n    @classmethod\n    def get_tool(self, name: str) -&gt; Optional[Tool]:\n        \"\"\"\n        Attempts to get the tool with the given name.\n        If not, returns None.\n        \"\"\"\n        for tool in self.tools:\n            if tool.name == name:\n                return tool\n        return None\n\n    @classmethod\n    def add_tool(self, tool: Tool) -&gt; None:\n        \"\"\"\n        Adds a tool to the config.\n        \"\"\"\n        self.tools.append(tool)\n\n    @classmethod\n    def add_chat_role(self, role: Type[Enum]) -&gt; None:\n        \"\"\"\n        Adds a chat role to the config.\n        \"\"\"\n        if not issubclass(role, Enum):\n            raise TypeError(\"Expected an enum\")\n        self.chat_roles.append(role)\n\n    @classmethod\n    def add_react_role(self, role: Type[Enum]) -&gt; None:\n        \"\"\"\n        Adds a react role to the config.\n        \"\"\"\n        if not issubclass(role, Enum):\n            raise TypeError(\"Expected an enum\")\n        self.react_roles.append(role)\n\n    @classmethod\n    def register_module(self, module: Module) -&gt; None:\n        \"\"\"\n        Registers a module.\n        \"\"\"\n        self.modules.append(module)\n        for tool in module.tools:\n            self.add_tool(tool)\n        for chat_role in module.chat_roles:\n            self.add_chat_role(chat_role)\n        for react_role in module.react_roles:\n            self.add_react_role(react_role)\n\n        logger.debug(\n            f'Registered module {module.name}@{module.version}: \"{module.description}\"'\n        )\n</code></pre>"},{"location":"api/#SilverLingua.Config-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.Config.add_chat_role","title":"<code>add_chat_role(role)</code>  <code>classmethod</code>","text":"<p>Adds a chat role to the config.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef add_chat_role(self, role: Type[Enum]) -&gt; None:\n    \"\"\"\n    Adds a chat role to the config.\n    \"\"\"\n    if not issubclass(role, Enum):\n        raise TypeError(\"Expected an enum\")\n    self.chat_roles.append(role)\n</code></pre>"},{"location":"api/#SilverLingua.Config.add_react_role","title":"<code>add_react_role(role)</code>  <code>classmethod</code>","text":"<p>Adds a react role to the config.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef add_react_role(self, role: Type[Enum]) -&gt; None:\n    \"\"\"\n    Adds a react role to the config.\n    \"\"\"\n    if not issubclass(role, Enum):\n        raise TypeError(\"Expected an enum\")\n    self.react_roles.append(role)\n</code></pre>"},{"location":"api/#SilverLingua.Config.add_tool","title":"<code>add_tool(tool)</code>  <code>classmethod</code>","text":"<p>Adds a tool to the config.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef add_tool(self, tool: Tool) -&gt; None:\n    \"\"\"\n    Adds a tool to the config.\n    \"\"\"\n    self.tools.append(tool)\n</code></pre>"},{"location":"api/#SilverLingua.Config.get_chat_role","title":"<code>get_chat_role(role)</code>  <code>classmethod</code>","text":"<p>Attempts to get the standardized ChatRole enum from 'role'. If not, returns None.</p> <p>This is usually used internally for maintaining consistency in Notions across different LLM backends.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef get_chat_role(self, role: str) -&gt; Optional[ChatRole]:\n    \"\"\"\n    Attempts to get the standardized ChatRole enum from 'role'.\n    If not, returns None.\n\n    This is usually used internally for maintaining\n    consistency in Notions across different LLM backends.\n    \"\"\"\n    for enum_class in self.chat_roles:\n        for enum_member in enum_class:\n            if str(enum_member.value).lower() == str(role).lower():\n                return ChatRole[enum_member.name]\n    return None\n</code></pre>"},{"location":"api/#SilverLingua.Config.get_react_role","title":"<code>get_react_role(role)</code>  <code>classmethod</code>","text":"<p>Attempts to get the standardized ReactRole enum from 'role'. If not, returns None.</p> <p>This is usually used internally for maintaining consistency in Notions across different LLM backends.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef get_react_role(self, role: str) -&gt; Optional[ReactRole]:\n    \"\"\"\n    Attempts to get the standardized ReactRole enum from 'role'.\n    If not, returns None.\n\n    This is usually used internally for maintaining\n    consistency in Notions across different LLM backends.\n    \"\"\"\n    for enum_class in self.react_roles:\n        for enum_member in enum_class:\n            if enum_member.value == role:\n                return ReactRole[enum_member.name]\n    return None\n</code></pre>"},{"location":"api/#SilverLingua.Config.get_tool","title":"<code>get_tool(name)</code>  <code>classmethod</code>","text":"<p>Attempts to get the tool with the given name. If not, returns None.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef get_tool(self, name: str) -&gt; Optional[Tool]:\n    \"\"\"\n    Attempts to get the tool with the given name.\n    If not, returns None.\n    \"\"\"\n    for tool in self.tools:\n        if tool.name == name:\n            return tool\n    return None\n</code></pre>"},{"location":"api/#SilverLingua.Config.register_module","title":"<code>register_module(module)</code>  <code>classmethod</code>","text":"<p>Registers a module.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef register_module(self, module: Module) -&gt; None:\n    \"\"\"\n    Registers a module.\n    \"\"\"\n    self.modules.append(module)\n    for tool in module.tools:\n        self.add_tool(tool)\n    for chat_role in module.chat_roles:\n        self.add_chat_role(chat_role)\n    for react_role in module.react_roles:\n        self.add_react_role(react_role)\n\n    logger.debug(\n        f'Registered module {module.name}@{module.version}: \"{module.description}\"'\n    )\n</code></pre>"},{"location":"api/#SilverLingua.Idearium","title":"<code>Idearium</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of <code>Notions</code> that is automatically trimmed to fit within a maximum number of tokens.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>class Idearium(BaseModel):\n    \"\"\"\n    A collection of `Notions` that is automatically trimmed to fit within a maximum\n    number of tokens.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    tokenizer: Tokenizer\n    max_tokens: int\n    notions: List[Notion] = Field(default_factory=list)\n    tokenized_notions: List[List[int]] = Field(default_factory=list)\n    persistent_indices: set = Field(default_factory=set)\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        max_tokens: int,\n        notions: List[Notion] = None,\n        **kwargs,\n    ):\n        # Initialize with empty notions if None\n        notions = notions or []\n\n        # Initialize tokenized_notions\n        tokenized_notions = [tokenizer.encode(notion.content) for notion in notions]\n\n        # Call parent init with all values\n        super().__init__(\n            tokenizer=tokenizer,\n            max_tokens=max_tokens,\n            notions=notions,\n            tokenized_notions=tokenized_notions,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_notions(cls, values):\n        notions = values.notions\n        for notion in notions:\n            cls.validate_notion(notion, values.max_tokens, values.tokenizer)\n        return values\n\n    @classmethod\n    def validate_notion(cls, notion: Notion, max_tokens: int, tokenizer: Tokenizer):\n        if len(notion.content) == 0:\n            raise ValueError(\"Notion content cannot be empty.\")\n\n        tokenized_notion = tokenizer.encode(notion.content)\n        if len(tokenized_notion) &gt; max_tokens:\n            raise ValueError(\"Notion exceeds maximum token length\")\n\n        return tokenized_notion\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"The total number of tokens in the Idearium.\"\"\"\n        return sum(len(notion) for notion in self.tokenized_notions)\n\n    @property\n    def _non_persistent_indices(self) -&gt; set:\n        \"\"\"The indices of non-persistent notions.\"\"\"\n        return set(range(len(self.notions))) - self.persistent_indices\n\n    def index(self, notion: Notion) -&gt; int:\n        \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n        return self.notions.index(notion)\n\n    def append(self, notion: Notion):\n        \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n        logger.debug(f\"Appending notion: {notion.content!r}\")\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        if self.notions:\n            logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n        if (\n            self.notions\n            and self.notions[-1].role == notion.role\n            and self.notions[-1].persistent == notion.persistent\n        ):\n            combined_content = self.notions[-1].content + notion.content\n            combined_notion = Notion(\n                content=combined_content,\n                role=notion.role,\n                persistent=notion.persistent,\n            )\n            self.replace(len(self.notions) - 1, combined_notion)\n            logger.debug(\n                f\"After replace, about to return combined content: {combined_content!r}\"\n            )\n            return\n\n        logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n        self.notions.append(notion)\n        self.tokenized_notions.append(tokenized_notion)\n\n        if notion.persistent:\n            # Modify the set in place instead of reassigning\n            self.persistent_indices.add(len(self.notions) - 1)\n\n        self._trim()\n\n    def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n        \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n        if isinstance(notions, Idearium):\n            notions = notions.notions\n\n        for notion in notions:\n            self.append(notion)\n\n    def insert(self, index: int, notion: Notion):\n        \"\"\"Inserts the given notion at the given index.\"\"\"\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        self.notions.insert(index, notion)\n        self.tokenized_notions.insert(index, tokenized_notion)\n\n        # Update persistent_indices in place\n        new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n        if notion.persistent:\n            self.persistent_indices.add(index)\n\n        self._trim()\n\n    def remove(self, notion: Notion):\n        \"\"\"Removes the first occurrence of the given notion.\"\"\"\n        index = self.index(notion)\n        self.pop(index)\n\n    def pop(self, index: int) -&gt; Notion:\n        \"\"\"Removes and returns the notion at the given index.\"\"\"\n        ret = self.notions.pop(index)\n        self.tokenized_notions.pop(index)\n\n        # Update persistent_indices\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.discard(index)\n        new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n\n        return ret\n\n    def replace(self, index: int, notion: Notion):\n        \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n        self.notions[index] = notion\n        self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n        # Update persistent_indices based on the replaced notion\n        if notion.persistent:\n            self.persistent_indices.add(index)\n        else:\n            self.persistent_indices.discard(index)\n\n        self._trim()\n\n    def copy(self) -&gt; \"Idearium\":\n        \"\"\"Returns a copy of the Idearium.\"\"\"\n        return Idearium(\n            tokenizer=self.tokenizer,\n            max_tokens=self.max_tokens,\n            notions=self.notions.copy(),\n            tokenized_notions=self.tokenized_notions.copy(),\n            persistent_indices=self.persistent_indices.copy(),\n        )\n\n    def _trim(self):\n        \"\"\"\n        Trims the Idearium to fit within the maximum number of tokens, called\n        after every modification.\n\n        This is the primary point of extension for Idearium subclasses, as it\n        allows for custom trimming behavior.\n        \"\"\"\n        while self.total_tokens &gt; self.max_tokens:\n            non_persistent_indices = self._non_persistent_indices\n\n            # Check if there's only one non-persistent user message\n            if len(non_persistent_indices) == 1:\n                single_index = next(iter(non_persistent_indices))\n                tokenized_notion = self.tokenized_notions[single_index]\n\n                # Trim the only non-persistent notion to fit within the token limit\n                tokenized_notion = tokenized_notion[\n                    : self.max_tokens - (self.total_tokens - len(tokenized_notion))\n                ]\n                trimmed_content = self.tokenizer.decode(tokenized_notion)\n                trimmed_notion = Notion(\n                    content=trimmed_content,\n                    role=self.notions[single_index].role,\n                    persistent=self.notions[single_index].persistent,\n                )\n                self.replace(single_index, trimmed_notion)\n                return\n\n            # Attempt to remove the first non-persistent notion\n            for i in non_persistent_indices:\n                self.pop(i)\n                break\n            else:\n                # If all notions are persistent and\n                # the max token length is still exceeded\n                raise ValueError(\n                    \"Persistent notions exceed max_tokens.\"\n                    + \" Reduce the content or increase max_tokens.\"\n                )\n\n    def __len__(self) -&gt; int:\n        return len(self.notions)\n\n    def __getitem__(self, index: int) -&gt; Notion:\n        return self.notions[index]\n\n    def __setitem__(self, index: int, notion: Notion):\n        self.replace(index, notion)\n\n    def __delitem__(self, index: int):\n        self.pop(index)\n\n    def __iter__(self) -&gt; Iterator[Notion]:\n        return iter(self.notions)\n\n    def __contains__(self, notion: Notion) -&gt; bool:\n        return notion in self.notions\n\n    def __str__(self) -&gt; str:\n        return str(self.notions)\n\n    def __repr__(self) -&gt; str:\n        return repr(self.notions)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Idearium):\n            return NotImplemented\n        return self.notions == other.notions\n</code></pre>"},{"location":"api/#SilverLingua.Idearium-attributes","title":"Attributes","text":""},{"location":"api/#SilverLingua.Idearium.total_tokens","title":"<code>total_tokens: int</code>  <code>property</code>","text":"<p>The total number of tokens in the Idearium.</p>"},{"location":"api/#SilverLingua.Idearium-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.Idearium.append","title":"<code>append(notion)</code>","text":"<p>Appends the given notion to the end of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def append(self, notion: Notion):\n    \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n    logger.debug(f\"Appending notion: {notion.content!r}\")\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    if self.notions:\n        logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n    if (\n        self.notions\n        and self.notions[-1].role == notion.role\n        and self.notions[-1].persistent == notion.persistent\n    ):\n        combined_content = self.notions[-1].content + notion.content\n        combined_notion = Notion(\n            content=combined_content,\n            role=notion.role,\n            persistent=notion.persistent,\n        )\n        self.replace(len(self.notions) - 1, combined_notion)\n        logger.debug(\n            f\"After replace, about to return combined content: {combined_content!r}\"\n        )\n        return\n\n    logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n    self.notions.append(notion)\n    self.tokenized_notions.append(tokenized_notion)\n\n    if notion.persistent:\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.add(len(self.notions) - 1)\n\n    self._trim()\n</code></pre>"},{"location":"api/#SilverLingua.Idearium.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def copy(self) -&gt; \"Idearium\":\n    \"\"\"Returns a copy of the Idearium.\"\"\"\n    return Idearium(\n        tokenizer=self.tokenizer,\n        max_tokens=self.max_tokens,\n        notions=self.notions.copy(),\n        tokenized_notions=self.tokenized_notions.copy(),\n        persistent_indices=self.persistent_indices.copy(),\n    )\n</code></pre>"},{"location":"api/#SilverLingua.Idearium.extend","title":"<code>extend(notions)</code>","text":"<p>Extends the Idearium with the given list of notions.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n    \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n    if isinstance(notions, Idearium):\n        notions = notions.notions\n\n    for notion in notions:\n        self.append(notion)\n</code></pre>"},{"location":"api/#SilverLingua.Idearium.index","title":"<code>index(notion)</code>","text":"<p>Returns the index of the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def index(self, notion: Notion) -&gt; int:\n    \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n    return self.notions.index(notion)\n</code></pre>"},{"location":"api/#SilverLingua.Idearium.insert","title":"<code>insert(index, notion)</code>","text":"<p>Inserts the given notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def insert(self, index: int, notion: Notion):\n    \"\"\"Inserts the given notion at the given index.\"\"\"\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    self.notions.insert(index, notion)\n    self.tokenized_notions.insert(index, tokenized_notion)\n\n    # Update persistent_indices in place\n    new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n    if notion.persistent:\n        self.persistent_indices.add(index)\n\n    self._trim()\n</code></pre>"},{"location":"api/#SilverLingua.Idearium.pop","title":"<code>pop(index)</code>","text":"<p>Removes and returns the notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def pop(self, index: int) -&gt; Notion:\n    \"\"\"Removes and returns the notion at the given index.\"\"\"\n    ret = self.notions.pop(index)\n    self.tokenized_notions.pop(index)\n\n    # Update persistent_indices\n    # Modify the set in place instead of reassigning\n    self.persistent_indices.discard(index)\n    new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n\n    return ret\n</code></pre>"},{"location":"api/#SilverLingua.Idearium.remove","title":"<code>remove(notion)</code>","text":"<p>Removes the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def remove(self, notion: Notion):\n    \"\"\"Removes the first occurrence of the given notion.\"\"\"\n    index = self.index(notion)\n    self.pop(index)\n</code></pre>"},{"location":"api/#SilverLingua.Idearium.replace","title":"<code>replace(index, notion)</code>","text":"<p>Replaces the notion at the given index with the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def replace(self, index: int, notion: Notion):\n    \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n    self.notions[index] = notion\n    self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n    # Update persistent_indices based on the replaced notion\n    if notion.persistent:\n        self.persistent_indices.add(index)\n    else:\n        self.persistent_indices.discard(index)\n\n    self._trim()\n</code></pre>"},{"location":"api/#SilverLingua.Link","title":"<code>Link</code>","text":"<p>               Bases: <code>Memory</code></p> <p>A memory that can have a parent and children Links, forming a hierarchical structure of interconnected memories.</p> <p>The content can be either a Notion or a Memory. (You can still use the content as a string via str(link.content).</p> Source code in <code>src\\SilverLingua\\core\\molecules\\link.py</code> <pre><code>class Link(Memory):\n    \"\"\"\n    A memory that can have a parent and children Links,\n    forming a hierarchical structure of interconnected memories.\n\n    The content can be either a Notion or a Memory.\n    (You can still use the content as a string via str(link.content).\n    \"\"\"\n\n    content: Union[Notion, Memory]\n    parent: Optional[\"Link\"] = None\n    children: List[\"Link\"] = Field(default_factory=list)\n\n    def add_child(self, child: \"Link\") -&gt; None:\n        self.children.append(child)\n        child.parent = self\n\n    def remove_child(self, child: \"Link\") -&gt; None:\n        self.children.remove(child)\n        child.parent = None\n\n    @property\n    def path(self) -&gt; List[\"Link\"]:\n        \"\"\"\n        Returns the path from the root to this Link.\n        \"\"\"\n        path = [self]\n        while path[-1].parent is not None:\n            path.append(path[-1].parent)\n        return path\n\n    @property\n    def root(self) -&gt; \"Link\":\n        \"\"\"\n        Returns the root Link of this Link.\n        \"\"\"\n        return self.path[-1]\n\n    @property\n    def depth(self) -&gt; int:\n        \"\"\"\n        Returns 1 based depth of this Link.\n        \"\"\"\n        return len(self.path)\n\n    @property\n    def is_root(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a root Link.\n        \"\"\"\n        return self.parent is None\n\n    @property\n    def is_leaf(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a leaf Link.\n        \"\"\"\n        return len(self.children) == 0\n\n    @property\n    def is_branch(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a branch Link.\n        \"\"\"\n        return not self.is_leaf\n\n    @property\n    def path_string(self) -&gt; str:\n        \"\"\"\n        Returns the path from the root to this Link as a string.\n\n        Example:\n        \"root&gt;child&gt;grandchild\"\n        \"\"\"\n        path_str = f\"{self.content}\"\n        if not self.is_root:\n            path_str = f\"{self.parent.path_string}&gt;{path_str}\"\n        return path_str\n</code></pre>"},{"location":"api/#SilverLingua.Link-attributes","title":"Attributes","text":""},{"location":"api/#SilverLingua.Link.depth","title":"<code>depth: int</code>  <code>property</code>","text":"<p>Returns 1 based depth of this Link.</p>"},{"location":"api/#SilverLingua.Link.is_branch","title":"<code>is_branch: bool</code>  <code>property</code>","text":"<p>Returns whether this Link is a branch Link.</p>"},{"location":"api/#SilverLingua.Link.is_leaf","title":"<code>is_leaf: bool</code>  <code>property</code>","text":"<p>Returns whether this Link is a leaf Link.</p>"},{"location":"api/#SilverLingua.Link.is_root","title":"<code>is_root: bool</code>  <code>property</code>","text":"<p>Returns whether this Link is a root Link.</p>"},{"location":"api/#SilverLingua.Link.path","title":"<code>path: List[Link]</code>  <code>property</code>","text":"<p>Returns the path from the root to this Link.</p>"},{"location":"api/#SilverLingua.Link.path_string","title":"<code>path_string: str</code>  <code>property</code>","text":"<p>Returns the path from the root to this Link as a string.</p> <p>Example: \"root&gt;child&gt;grandchild\"</p>"},{"location":"api/#SilverLingua.Link.root","title":"<code>root: Link</code>  <code>property</code>","text":"<p>Returns the root Link of this Link.</p>"},{"location":"api/#SilverLingua.Memory","title":"<code>Memory</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Memory is the smallest unit of storage information, and is the base class for all other storage information.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\memory.py</code> <pre><code>class Memory(BaseModel):\n    \"\"\"\n    A Memory is the smallest unit of storage information, and\n    is the base class for all other storage information.\n    \"\"\"\n\n    content: str\n\n    def __str__(self) -&gt; str:\n        return self.content\n</code></pre>"},{"location":"api/#SilverLingua.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract class for all Large Language Models.</p> <p>This class outlines a standardized lifecycle for interacting with LLMs, aimed at ensuring a consistent process for message trimming, pre-processing, preparing requests for the model, invoking the model, standardizing the response, and post-processing. The lifecycle is as follows:</p> <p>Lifecycle: 1. Pre-processing (_preprocess): Performs any necessary transformations or     adjustments to the messages prior to trimming or preparing them for model input.     (Optional)</p> <ol> <li> <p>Preparing Request (_format_request): Converts the pre-processed messages     into a format suitable for model input.</p> </li> <li> <p>Model Invocation (_call or _acall): Feeds the prepared input to the LLM and     retrieves the raw model output. There should be both synchronous and     asynchronous versions available.</p> </li> <li> <p>Standardizing Response (_standardize_response): Transforms the raw model     output into a consistent response format suitable for further processing or     delivery.</p> </li> <li> <p>Post-processing (_postprocess): Performs any final transformations or     adjustments to the standardized responses, making them ready for delivery.     (Optional)</p> </li> </ol> <p>Subclasses should implement each of the non-optional lifecycle steps in accordance with the specific requirements and behaviors of the target LLM.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>class Model(BaseModel, ABC):\n    \"\"\"\n    Abstract class for all Large Language Models.\n\n    This class outlines a standardized lifecycle for interacting with LLMs,\n    aimed at ensuring a consistent process for message trimming, pre-processing,\n    preparing requests for the model, invoking the model, standardizing the response,\n    and post-processing. The lifecycle is as follows:\n\n    Lifecycle:\n    1. Pre-processing (_preprocess): Performs any necessary transformations or\n        adjustments to the messages prior to trimming or preparing them for model input.\n        (Optional)\n\n    2. Preparing Request (_format_request): Converts the pre-processed messages\n        into a format suitable for model input.\n\n    3. Model Invocation (_call or _acall): Feeds the prepared input to the LLM and\n        retrieves the raw model output. There should be both synchronous and\n        asynchronous versions available.\n\n    4. Standardizing Response (_standardize_response): Transforms the raw model\n        output into a consistent response format suitable for further processing or\n        delivery.\n\n    5. Post-processing (_postprocess): Performs any final transformations or\n        adjustments to the standardized responses, making them ready for delivery.\n        (Optional)\n\n    Subclasses should implement each of the non-optional lifecycle steps in accordance\n    with the specific requirements and behaviors of the target LLM.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    max_response: int = Field(default=0)\n    api_key: str\n    name: str\n    #\n    role: Type[ChatRole]\n    type: ModelType\n    llm: Callable\n    llm_async: Callable\n    can_stream: bool\n    tokenizer: Tokenizer\n\n    @property\n    @abstractmethod\n    def max_tokens(self) -&gt; int:\n        \"\"\"\n        The maximum number of tokens that can be fed to the model at once.\n        \"\"\"\n        pass\n\n    def _process_input(self, messages: Messages) -&gt; Idearium:\n        if isinstance(messages, str):\n            notions = [Notion(content=messages, role=self.role.HUMAN)]\n        elif isinstance(messages, Notion):\n            notions = [messages]\n        elif isinstance(messages, Idearium):\n            return messages  # Already an Idearium, no need to convert\n        elif isinstance(messages, list):\n            notions = [\n                (\n                    Notion(content=msg, role=self.role.HUMAN)\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        else:\n            raise ValueError(\"Invalid input type for messages\")\n\n        return Idearium(self.tokenizer, self.max_tokens, notions)\n\n    def _convert_role(self, role: ChatRole) -&gt; str:\n        \"\"\"\n        Converts the standard ChatRole to the model-specific role.\n        \"\"\"\n        return str(self.role[role.name].value)\n\n    def _preprocess(self, messages: List[Notion]) -&gt; List[Notion]:\n        \"\"\"\n        Preprocesses the List of `Notions`, applying any effects necessary\n        before being prepped for input into an API.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        return [\n            Notion(msg.content, self._convert_role(msg.chat_role), msg.persistent)\n            for msg in messages\n        ]\n\n    @abstractmethod\n    def _format_request(\n        self, messages: List[Notion], *args, **kwargs\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Formats the List of `Notions` into a format suitable for model input.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _standardize_response(\n        self, response: Union[object, str, List[any]], *args, **kwargs\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Standardizes the raw response from the model into a List of Notions.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _postprocess(self, response: List[Notion], *args, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Postprocesses the response from the model, applying any final effects\n        before being returned.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _retry_call(\n        self,\n        input: Union[str, object, List[any]],\n        e: Exception,\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Retry logic for API calls used by `_common_call_logic`.\n        \"\"\"\n        pass\n\n    def _common_call_logic(\n        self,\n        input: Union[str, object, List[any]],\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        if input is None:\n            raise ValueError(\"No input provided.\")\n\n        try:\n            out = api_call(messages=input)\n            return out\n        except Exception as e:\n            logger.error(f\"Error calling LLM API: {e}\")\n            if retries &gt;= 3:\n                raise e\n\n            return self._retry_call(input, e, api_call, retries=retries)\n\n    def _call(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the raw response.\n\n        Should behave exactly as `_acall` does, but synchronously.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n\n        def api_call(**kwargs_):\n            return self.llm(**kwargs_, **kwargs)\n\n        return self._common_call_logic(input, api_call, retries)\n\n    async def _acall(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the\n        raw response asynchronously.\n\n        Should behave exactly as `_call` does, but asynchronously.\n\n        This is a lifecycle method that is called by the `agenerate` method.\n        \"\"\"\n\n        async def api_call(**kwargs_):\n            return await self.llm_async(**kwargs_, **kwargs)\n\n        result = self._common_call_logic(input, api_call, retries)\n        if asyncio.iscoroutine(result):\n            return await result\n        return result\n\n    def _common_generate_logic(\n        self,\n        messages: Messages,\n        is_async=False,\n        **kwargs,\n    ):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        call_method = self._acall if is_async else self._call\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n\n        if is_async:\n\n            async def call():\n                response = await call_method(input, **kwargs)\n                output = self._standardize_response(response)\n                return self._postprocess(output)\n\n            return call()\n        else:\n            response = call_method(input, **kwargs)\n            output = self._standardize_response(response)\n            return self._postprocess(output)\n\n    @abstractmethod\n    def generate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def agenerate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response\n        asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating async responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    def _common_stream_logic(self, messages: Messages):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        if not self.can_stream:\n            raise ValueError(\n                \"This model does not support streaming. \"\n                + \"Please use the `generate` method instead.\"\n            )\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n        return input\n\n    @abstractmethod\n    def stream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def astream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time, asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#SilverLingua.Model-attributes","title":"Attributes","text":""},{"location":"api/#SilverLingua.Model.max_tokens","title":"<code>max_tokens: int</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>The maximum number of tokens that can be fed to the model at once.</p>"},{"location":"api/#SilverLingua.Model-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.Model.agenerate","title":"<code>agenerate(messages, *args, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Calls the model with the given messages and returns the response asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating async responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response\n    asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating async responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#SilverLingua.Model.astream","title":"<code>astream(messages, *args, **kwargs)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Streams the model with the given messages and returns the response, one token at a time, asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def astream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time, asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#SilverLingua.Model.generate","title":"<code>generate(messages, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Calls the model with the given messages and returns the response.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#SilverLingua.Model.stream","title":"<code>stream(messages, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Streams the model with the given messages and returns the response, one token at a time.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef stream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#SilverLingua.Module","title":"<code>Module</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A module that can be loaded into SilverLingua.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the module.</p> <code>description</code> <code>str</code> <p>A description of the module.</p> <code>version</code> <code>str</code> <p>The version of the module.</p> <code>tools</code> <code>List[Tool]</code> <p>The tools in the module.</p> <code>chat_roles</code> <code>List[type[ChatRole]]</code> <p>The chat roles in the module.</p> <code>react_roles</code> <code>List[type[ReactRole]]</code> <p>The react roles in the module.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>class Module(BaseModel):\n    \"\"\"\n    A module that can be loaded into SilverLingua.\n\n    Attributes:\n        name: The name of the module.\n        description: A description of the module.\n        version: The version of the module.\n        tools: The tools in the module.\n        chat_roles: The chat roles in the module.\n        react_roles: The react roles in the module.\n    \"\"\"\n\n    name: str\n    description: str\n    version: str\n    tools: List[Tool]\n    chat_roles: List[type[ChatRole]]\n    react_roles: List[type[ReactRole]]\n\n    @field_validator(\"chat_roles\", mode=\"plain\")\n    def check_chat_roles(cls, v):\n        for role in v:\n            if not issubclass(role, Enum):\n                raise TypeError(\"Expected an enum\")\n            else:\n                for member in role:\n                    if (\n                        not ChatRole[member.name]\n                        or member.value != ChatRole[member.name].value\n                    ):\n                        raise TypeError(\"members must match ChatRole members.\")\n        return v\n\n    @field_validator(\"react_roles\", mode=\"plain\")\n    def check_react_roles(cls, v):\n        for role in v:\n            if not issubclass(role, Enum):\n                raise TypeError(\"Expected an enum\")\n            else:\n                for member in role:\n                    if (\n                        not ReactRole[member.name]\n                        or member.value != ReactRole[member.name].value\n                    ):\n                        raise TypeError(\"members must match ReactRole members.\")\n        return v\n\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        version: str,\n        tools: List[Tool],\n        chat_roles: List[type[ChatRole]],\n        react_roles: List[type[ReactRole]],\n        **kwargs,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            version=version,\n            tools=tools,\n            chat_roles=chat_roles,\n            react_roles=react_roles,\n            **kwargs,\n        )\n\n        Config.register_module(self)\n</code></pre>"},{"location":"api/#SilverLingua.Notion","title":"<code>Notion</code>","text":"<p>               Bases: <code>Memory</code></p> <p>A memory that stores the role associated with its content. The role is usually a <code>ChatRole</code> or a <code>ReactRole</code>. (See <code>atoms/roles</code>)</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the notion.</p> <code>content</code> <code>str</code> <p>The content of the memory.</p> <code>persistent</code> <code>bool</code> <p>Whether the notion should be stored in long-term memory.</p> Source code in <code>src\\SilverLingua\\core\\molecules\\notion.py</code> <pre><code>class Notion(Memory):\n    \"\"\"\n    A memory that stores the role associated with its content.\n    The role is usually a `ChatRole` or a `ReactRole`.\n    (See `atoms/roles`)\n\n    Attributes:\n        role: The role of the notion.\n        content: The content of the memory.\n        persistent: Whether the notion should be stored in long-term memory.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n    role: str\n    persistent: bool = False\n\n    @field_validator(\"role\", mode=\"before\")\n    @classmethod\n    def validate_role(cls, v: Union[ChatRole, ReactRole, str]):\n        if isinstance(v, (ChatRole, ReactRole)):\n            return v.value.value\n        elif isinstance(v, str):\n            return v\n        raise ValueError(f\"Expected a ChatRole, ReactRole, or a string, got {type(v)}\")\n\n    def __str__(self) -&gt; str:\n        return f\"{self.role}: {self.content}\"\n\n    def __init__(\n        self,\n        content: str,\n        role: Union[ChatRole, ReactRole, str],\n        persistent: bool = False,\n    ):\n        super().__init__(content=content, role=role, persistent=persistent)\n\n    @property\n    def chat_role(self) -&gt; ChatRole:\n        \"\"\"\n        Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_chat_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ChatRole.AI\n        return r\n\n    @property\n    def react_role(self) -&gt; ReactRole:\n        \"\"\"\n        Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_react_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ReactRole.THOUGHT\n        return r\n</code></pre>"},{"location":"api/#SilverLingua.Notion-attributes","title":"Attributes","text":""},{"location":"api/#SilverLingua.Notion.chat_role","title":"<code>chat_role: ChatRole</code>  <code>property</code>","text":"<p>Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)</p> <p>(See <code>config</code>)</p>"},{"location":"api/#SilverLingua.Notion.react_role","title":"<code>react_role: ReactRole</code>  <code>property</code>","text":"<p>Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)</p> <p>(See <code>config</code>)</p>"},{"location":"api/#SilverLingua.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre>"},{"location":"api/#SilverLingua.Tool-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.Tool.__init__","title":"<code>__init__(function)</code>","text":"<p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre>"},{"location":"api/#SilverLingua.Tool.use_function_call","title":"<code>use_function_call(function_call)</code>","text":"<p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre>"},{"location":"api/#SilverLingua.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre>"},{"location":"api/#SilverLingua.ToolCall-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.ToolCall.concat","title":"<code>concat(other)</code>","text":"<p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre>"},{"location":"api/#SilverLingua.ToolCallResponse","title":"<code>ToolCallResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre>"},{"location":"api/#SilverLingua.ToolCalls","title":"<code>ToolCalls</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre>"},{"location":"api/#SilverLingua.ToolCalls-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.ToolCalls.concat","title":"<code>concat(other)</code>","text":"<p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre>"},{"location":"api/#SilverLingua-modules","title":"Modules","text":""},{"location":"api/#SilverLingua.anthropic","title":"<code>anthropic</code>","text":"<p>The Anthropic module provides implementations of SilverLingua's core components using the Anthropic API.</p> <p>This module includes: - AnthropicChatAgent: An agent that uses Anthropic's chat completion API - AnthropicModel: A model that uses Anthropic's API - AnthropicChatRole: Role definitions for Anthropic's chat format</p>"},{"location":"api/#SilverLingua.anthropic-classes","title":"Classes","text":""},{"location":"api/#SilverLingua.config","title":"<code>config</code>","text":""},{"location":"api/#SilverLingua.config-classes","title":"Classes","text":""},{"location":"api/#SilverLingua.config.Config","title":"<code>Config</code>","text":"Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>class Config:\n    modules: List[Module] = []\n    chat_roles: List[type[ChatRole]] = [ChatRole]\n    react_roles: List[type[ReactRole]] = [ReactRole]\n    tools: List[Tool] = []\n\n    @classmethod\n    def get_chat_role(self, role: str) -&gt; Optional[ChatRole]:\n        \"\"\"\n        Attempts to get the standardized ChatRole enum from 'role'.\n        If not, returns None.\n\n        This is usually used internally for maintaining\n        consistency in Notions across different LLM backends.\n        \"\"\"\n        for enum_class in self.chat_roles:\n            for enum_member in enum_class:\n                if str(enum_member.value).lower() == str(role).lower():\n                    return ChatRole[enum_member.name]\n        return None\n\n    @classmethod\n    def get_react_role(self, role: str) -&gt; Optional[ReactRole]:\n        \"\"\"\n        Attempts to get the standardized ReactRole enum from 'role'.\n        If not, returns None.\n\n        This is usually used internally for maintaining\n        consistency in Notions across different LLM backends.\n        \"\"\"\n        for enum_class in self.react_roles:\n            for enum_member in enum_class:\n                if enum_member.value == role:\n                    return ReactRole[enum_member.name]\n        return None\n\n    @classmethod\n    def get_tool(self, name: str) -&gt; Optional[Tool]:\n        \"\"\"\n        Attempts to get the tool with the given name.\n        If not, returns None.\n        \"\"\"\n        for tool in self.tools:\n            if tool.name == name:\n                return tool\n        return None\n\n    @classmethod\n    def add_tool(self, tool: Tool) -&gt; None:\n        \"\"\"\n        Adds a tool to the config.\n        \"\"\"\n        self.tools.append(tool)\n\n    @classmethod\n    def add_chat_role(self, role: Type[Enum]) -&gt; None:\n        \"\"\"\n        Adds a chat role to the config.\n        \"\"\"\n        if not issubclass(role, Enum):\n            raise TypeError(\"Expected an enum\")\n        self.chat_roles.append(role)\n\n    @classmethod\n    def add_react_role(self, role: Type[Enum]) -&gt; None:\n        \"\"\"\n        Adds a react role to the config.\n        \"\"\"\n        if not issubclass(role, Enum):\n            raise TypeError(\"Expected an enum\")\n        self.react_roles.append(role)\n\n    @classmethod\n    def register_module(self, module: Module) -&gt; None:\n        \"\"\"\n        Registers a module.\n        \"\"\"\n        self.modules.append(module)\n        for tool in module.tools:\n            self.add_tool(tool)\n        for chat_role in module.chat_roles:\n            self.add_chat_role(chat_role)\n        for react_role in module.react_roles:\n            self.add_react_role(react_role)\n\n        logger.debug(\n            f'Registered module {module.name}@{module.version}: \"{module.description}\"'\n        )\n</code></pre>"},{"location":"api/#SilverLingua.config.Config-functions","title":"Functions","text":"<code>add_chat_role(role)</code> <code>classmethod</code> <p>Adds a chat role to the config.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef add_chat_role(self, role: Type[Enum]) -&gt; None:\n    \"\"\"\n    Adds a chat role to the config.\n    \"\"\"\n    if not issubclass(role, Enum):\n        raise TypeError(\"Expected an enum\")\n    self.chat_roles.append(role)\n</code></pre> <code>add_react_role(role)</code> <code>classmethod</code> <p>Adds a react role to the config.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef add_react_role(self, role: Type[Enum]) -&gt; None:\n    \"\"\"\n    Adds a react role to the config.\n    \"\"\"\n    if not issubclass(role, Enum):\n        raise TypeError(\"Expected an enum\")\n    self.react_roles.append(role)\n</code></pre> <code>add_tool(tool)</code> <code>classmethod</code> <p>Adds a tool to the config.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef add_tool(self, tool: Tool) -&gt; None:\n    \"\"\"\n    Adds a tool to the config.\n    \"\"\"\n    self.tools.append(tool)\n</code></pre> <code>get_chat_role(role)</code> <code>classmethod</code> <p>Attempts to get the standardized ChatRole enum from 'role'. If not, returns None.</p> <p>This is usually used internally for maintaining consistency in Notions across different LLM backends.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef get_chat_role(self, role: str) -&gt; Optional[ChatRole]:\n    \"\"\"\n    Attempts to get the standardized ChatRole enum from 'role'.\n    If not, returns None.\n\n    This is usually used internally for maintaining\n    consistency in Notions across different LLM backends.\n    \"\"\"\n    for enum_class in self.chat_roles:\n        for enum_member in enum_class:\n            if str(enum_member.value).lower() == str(role).lower():\n                return ChatRole[enum_member.name]\n    return None\n</code></pre> <code>get_react_role(role)</code> <code>classmethod</code> <p>Attempts to get the standardized ReactRole enum from 'role'. If not, returns None.</p> <p>This is usually used internally for maintaining consistency in Notions across different LLM backends.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef get_react_role(self, role: str) -&gt; Optional[ReactRole]:\n    \"\"\"\n    Attempts to get the standardized ReactRole enum from 'role'.\n    If not, returns None.\n\n    This is usually used internally for maintaining\n    consistency in Notions across different LLM backends.\n    \"\"\"\n    for enum_class in self.react_roles:\n        for enum_member in enum_class:\n            if enum_member.value == role:\n                return ReactRole[enum_member.name]\n    return None\n</code></pre> <code>get_tool(name)</code> <code>classmethod</code> <p>Attempts to get the tool with the given name. If not, returns None.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef get_tool(self, name: str) -&gt; Optional[Tool]:\n    \"\"\"\n    Attempts to get the tool with the given name.\n    If not, returns None.\n    \"\"\"\n    for tool in self.tools:\n        if tool.name == name:\n            return tool\n    return None\n</code></pre> <code>register_module(module)</code> <code>classmethod</code> <p>Registers a module.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>@classmethod\ndef register_module(self, module: Module) -&gt; None:\n    \"\"\"\n    Registers a module.\n    \"\"\"\n    self.modules.append(module)\n    for tool in module.tools:\n        self.add_tool(tool)\n    for chat_role in module.chat_roles:\n        self.add_chat_role(chat_role)\n    for react_role in module.react_roles:\n        self.add_react_role(react_role)\n\n    logger.debug(\n        f'Registered module {module.name}@{module.version}: \"{module.description}\"'\n    )\n</code></pre>"},{"location":"api/#SilverLingua.config.Module","title":"<code>Module</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A module that can be loaded into SilverLingua.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the module.</p> <code>description</code> <code>str</code> <p>A description of the module.</p> <code>version</code> <code>str</code> <p>The version of the module.</p> <code>tools</code> <code>List[Tool]</code> <p>The tools in the module.</p> <code>chat_roles</code> <code>List[type[ChatRole]]</code> <p>The chat roles in the module.</p> <code>react_roles</code> <code>List[type[ReactRole]]</code> <p>The react roles in the module.</p> Source code in <code>src\\SilverLingua\\config.py</code> <pre><code>class Module(BaseModel):\n    \"\"\"\n    A module that can be loaded into SilverLingua.\n\n    Attributes:\n        name: The name of the module.\n        description: A description of the module.\n        version: The version of the module.\n        tools: The tools in the module.\n        chat_roles: The chat roles in the module.\n        react_roles: The react roles in the module.\n    \"\"\"\n\n    name: str\n    description: str\n    version: str\n    tools: List[Tool]\n    chat_roles: List[type[ChatRole]]\n    react_roles: List[type[ReactRole]]\n\n    @field_validator(\"chat_roles\", mode=\"plain\")\n    def check_chat_roles(cls, v):\n        for role in v:\n            if not issubclass(role, Enum):\n                raise TypeError(\"Expected an enum\")\n            else:\n                for member in role:\n                    if (\n                        not ChatRole[member.name]\n                        or member.value != ChatRole[member.name].value\n                    ):\n                        raise TypeError(\"members must match ChatRole members.\")\n        return v\n\n    @field_validator(\"react_roles\", mode=\"plain\")\n    def check_react_roles(cls, v):\n        for role in v:\n            if not issubclass(role, Enum):\n                raise TypeError(\"Expected an enum\")\n            else:\n                for member in role:\n                    if (\n                        not ReactRole[member.name]\n                        or member.value != ReactRole[member.name].value\n                    ):\n                        raise TypeError(\"members must match ReactRole members.\")\n        return v\n\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        version: str,\n        tools: List[Tool],\n        chat_roles: List[type[ChatRole]],\n        react_roles: List[type[ReactRole]],\n        **kwargs,\n    ):\n        super().__init__(\n            name=name,\n            description=description,\n            version=version,\n            tools=tools,\n            chat_roles=chat_roles,\n            react_roles=react_roles,\n            **kwargs,\n        )\n\n        Config.register_module(self)\n</code></pre>"},{"location":"api/#SilverLingua.core","title":"<code>core</code>","text":"<p>SilverLingua Core</p>"},{"location":"api/#SilverLingua.core-modules","title":"Modules","text":""},{"location":"api/#SilverLingua.core.atoms","title":"<code>atoms</code>","text":""},{"location":"api/#SilverLingua.core.atoms-classes","title":"Classes","text":"<code>FunctionJSONSchema</code> <p>               Bases: <code>BaseModel</code></p> <p>A function according to JSON schema standards.</p> <p>This is also passed in to OpenAI ChatCompletion API functions list so the AI understands how to call a function.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function</p> <code>description</code> <code>str</code> <p>A description of the function</p> <code>parameters</code> <code>Parameters</code> <p>A dictionary of parameters and their types (Optional)</p> <p>Example:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class FunctionJSONSchema(BaseModel):\n    \"\"\"\n    A function according to JSON schema standards.\n\n    This is also passed in to OpenAI ChatCompletion API\n    functions list so the AI understands how to call a function.\n\n    Attributes:\n        name: The name of the function\n        description: A description of the function\n        parameters: A dictionary of parameters and their types (Optional)\n\n    Example:\n\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: Parameters\n</code></pre> <code>Memory</code> <p>               Bases: <code>BaseModel</code></p> <p>A Memory is the smallest unit of storage information, and is the base class for all other storage information.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\memory.py</code> <pre><code>class Memory(BaseModel):\n    \"\"\"\n    A Memory is the smallest unit of storage information, and\n    is the base class for all other storage information.\n    \"\"\"\n\n    content: str\n\n    def __str__(self) -&gt; str:\n        return self.content\n</code></pre> <code>Tokenizer</code> <p>               Bases: <code>BaseModel</code></p> <p>A tokenizer that can encode and decode strings.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tokenizer.py</code> <pre><code>class Tokenizer(BaseModel):\n    \"\"\"\n    A tokenizer that can encode and decode strings.\n    \"\"\"\n\n    encode: Callable[[str], List[int]]\n    decode: Callable[[List[int]], str]\n</code></pre> <code>Tool</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre> Functions <code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre> <code>ToolCall</code> <p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre> <code>ToolCallResponse</code> <p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre> <code>ToolCalls</code> <p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre>"},{"location":"api/#SilverLingua.core.atoms-functions","title":"Functions","text":"<code>create_chat_role(name, SYSTEM, HUMAN, AI, TOOL_CALL, TOOL_RESPONSE)</code> <p>Create a new ChatRole enum with only the values of the RoleMembers changed.</p> <p>This will ensure that the parent of each member is ChatRole, which means that the members will be equal to the members of ChatRole.</p> Example <pre><code>OpenAIChatRole = create_chat_role(\n    \"OpenAIChatRole\",\n    SYSTEM=\"system\",\n    HUMAN=\"user\",\n    AI=\"assistant\",\n    TOOL_CALL=\"function_call\",\n    TOOL_RESPONSE=\"function\",\n)\n\nassert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\role\\chat.py</code> <pre><code>def create_chat_role(\n    name: str, SYSTEM: str, HUMAN: str, AI: str, TOOL_CALL: str, TOOL_RESPONSE: str\n) -&gt; Type[ChatRole]:\n    \"\"\"\n    Create a new ChatRole enum with only the values of the RoleMembers changed.\n\n    This will ensure that the parent of each member is ChatRole, which means\n    that the members will be equal to the members of ChatRole.\n\n    Example:\n        ```python\n        OpenAIChatRole = create_chat_role(\n            \"OpenAIChatRole\",\n            SYSTEM=\"system\",\n            HUMAN=\"user\",\n            AI=\"assistant\",\n            TOOL_CALL=\"function_call\",\n            TOOL_RESPONSE=\"function\",\n        )\n\n        assert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n        ```\n    \"\"\"\n    return Enum(\n        name,\n        {\n            \"SYSTEM\": RoleMember(\"SYSTEM\", SYSTEM, ChatRole),\n            \"HUMAN\": RoleMember(\"HUMAN\", HUMAN, ChatRole),\n            \"AI\": RoleMember(\"AI\", AI, ChatRole),\n            \"TOOL_CALL\": RoleMember(\"TOOL_CALL\", TOOL_CALL, ChatRole),\n            \"TOOL_RESPONSE\": RoleMember(\"TOOL_RESPONSE\", TOOL_RESPONSE, ChatRole),\n        },\n    )\n</code></pre>"},{"location":"api/#SilverLingua.core.atoms-modules","title":"Modules","text":"<code>memory</code> Classes <code>Memory</code> <p>               Bases: <code>BaseModel</code></p> <p>A Memory is the smallest unit of storage information, and is the base class for all other storage information.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\memory.py</code> <pre><code>class Memory(BaseModel):\n    \"\"\"\n    A Memory is the smallest unit of storage information, and\n    is the base class for all other storage information.\n    \"\"\"\n\n    content: str\n\n    def __str__(self) -&gt; str:\n        return self.content\n</code></pre> <code>prompt</code> Functions <code>RolePrompt(role, text)</code> <p>{{ role }}: {{ text }}</p> Source code in <code>src\\SilverLingua\\core\\atoms\\prompt.py</code> <pre><code>@prompt\ndef RolePrompt(role: str, text: str):  # type: ignore\n    \"\"\"{{ role }}: {{ text }}\"\"\"\n</code></pre> <code>prompt(func)</code> <p>A decorator to render a function's docstring as a Jinja2 template. Uses the function arguments as variables for the template.</p> <p>Note: Be deliberate about new lines in your docstrings - they may make meaningful changes in an AI's output. For instance, separating long sentences with a newline for human readability may cause issues.</p> <p>Example Usage:</p> <pre><code>@prompt\ndef fruit_prompt(fruits: list) -&gt; None:\n    \"\"\"\n    You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\n    LIST OF FRUIT:\n    {% for fruit in fruits %}{{ fruit }}\n    {% endfor %}\n    \"\"\"\n\nprint(fruit_prompt([\"apple\", \"orange\"]))\n</code></pre> <p>Expected Output:</p> <pre><code>You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\nLIST OF FRUIT:\napple\norange\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\prompt.py</code> <pre><code>def prompt(func: Callable) -&gt; Callable[..., str]:\n    \"\"\"\n    A decorator to render a function's docstring as a Jinja2 template.\n    Uses the function arguments as variables for the template.\n\n    Note: Be deliberate about new lines in your docstrings - they\n    may make meaningful changes in an AI's output. For instance,\n    separating long sentences with a newline for human\n    readability may cause issues.\n\n    Example Usage:\n    ```python\n    @prompt\n    def fruit_prompt(fruits: list) -&gt; None:\n        \\\"\"\"\n        You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\n        LIST OF FRUIT:\n        {% for fruit in fruits %}{{ fruit }}\n        {% endfor %}\n        \\\"\"\"\n\n    print(fruit_prompt([\"apple\", \"orange\"]))\n    ```\n\n    Expected Output:\n    ```\n    You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\n    LIST OF FRUIT:\n    apple\n    orange\n    ```\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs) -&gt; str:\n        docstring = func.__doc__ or \"\"\n        template = Template(docstring, undefined=StrictUndefined)\n\n        # Get function signature and bind arguments\n        sig = signature(func)\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n\n        # Render the template with bound arguments\n        rendered = template.render(**bound_args.arguments)\n\n        # Strip each line and remove leading/trailing whitespaces\n        stripped_lines = [line.lstrip() for line in rendered.splitlines()]\n        return \"\\n\".join(stripped_lines).strip()\n\n    return wrapper\n</code></pre> <code>role</code> Functions <code>create_chat_role(name, SYSTEM, HUMAN, AI, TOOL_CALL, TOOL_RESPONSE)</code> <p>Create a new ChatRole enum with only the values of the RoleMembers changed.</p> <p>This will ensure that the parent of each member is ChatRole, which means that the members will be equal to the members of ChatRole.</p> Example <pre><code>OpenAIChatRole = create_chat_role(\n    \"OpenAIChatRole\",\n    SYSTEM=\"system\",\n    HUMAN=\"user\",\n    AI=\"assistant\",\n    TOOL_CALL=\"function_call\",\n    TOOL_RESPONSE=\"function\",\n)\n\nassert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\role\\chat.py</code> <pre><code>def create_chat_role(\n    name: str, SYSTEM: str, HUMAN: str, AI: str, TOOL_CALL: str, TOOL_RESPONSE: str\n) -&gt; Type[ChatRole]:\n    \"\"\"\n    Create a new ChatRole enum with only the values of the RoleMembers changed.\n\n    This will ensure that the parent of each member is ChatRole, which means\n    that the members will be equal to the members of ChatRole.\n\n    Example:\n        ```python\n        OpenAIChatRole = create_chat_role(\n            \"OpenAIChatRole\",\n            SYSTEM=\"system\",\n            HUMAN=\"user\",\n            AI=\"assistant\",\n            TOOL_CALL=\"function_call\",\n            TOOL_RESPONSE=\"function\",\n        )\n\n        assert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n        ```\n    \"\"\"\n    return Enum(\n        name,\n        {\n            \"SYSTEM\": RoleMember(\"SYSTEM\", SYSTEM, ChatRole),\n            \"HUMAN\": RoleMember(\"HUMAN\", HUMAN, ChatRole),\n            \"AI\": RoleMember(\"AI\", AI, ChatRole),\n            \"TOOL_CALL\": RoleMember(\"TOOL_CALL\", TOOL_CALL, ChatRole),\n            \"TOOL_RESPONSE\": RoleMember(\"TOOL_RESPONSE\", TOOL_RESPONSE, ChatRole),\n        },\n    )\n</code></pre> Modules <code>chat</code> Functions <code>create_chat_role(name, SYSTEM, HUMAN, AI, TOOL_CALL, TOOL_RESPONSE)</code> <p>Create a new ChatRole enum with only the values of the RoleMembers changed.</p> <p>This will ensure that the parent of each member is ChatRole, which means that the members will be equal to the members of ChatRole.</p> Example <pre><code>OpenAIChatRole = create_chat_role(\n    \"OpenAIChatRole\",\n    SYSTEM=\"system\",\n    HUMAN=\"user\",\n    AI=\"assistant\",\n    TOOL_CALL=\"function_call\",\n    TOOL_RESPONSE=\"function\",\n)\n\nassert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\role\\chat.py</code> <pre><code>def create_chat_role(\n    name: str, SYSTEM: str, HUMAN: str, AI: str, TOOL_CALL: str, TOOL_RESPONSE: str\n) -&gt; Type[ChatRole]:\n    \"\"\"\n    Create a new ChatRole enum with only the values of the RoleMembers changed.\n\n    This will ensure that the parent of each member is ChatRole, which means\n    that the members will be equal to the members of ChatRole.\n\n    Example:\n        ```python\n        OpenAIChatRole = create_chat_role(\n            \"OpenAIChatRole\",\n            SYSTEM=\"system\",\n            HUMAN=\"user\",\n            AI=\"assistant\",\n            TOOL_CALL=\"function_call\",\n            TOOL_RESPONSE=\"function\",\n        )\n\n        assert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n        ```\n    \"\"\"\n    return Enum(\n        name,\n        {\n            \"SYSTEM\": RoleMember(\"SYSTEM\", SYSTEM, ChatRole),\n            \"HUMAN\": RoleMember(\"HUMAN\", HUMAN, ChatRole),\n            \"AI\": RoleMember(\"AI\", AI, ChatRole),\n            \"TOOL_CALL\": RoleMember(\"TOOL_CALL\", TOOL_CALL, ChatRole),\n            \"TOOL_RESPONSE\": RoleMember(\"TOOL_RESPONSE\", TOOL_RESPONSE, ChatRole),\n        },\n    )\n</code></pre> <code>tokenizer</code> Classes <code>Tokenizer</code> <p>               Bases: <code>BaseModel</code></p> <p>A tokenizer that can encode and decode strings.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tokenizer.py</code> <pre><code>class Tokenizer(BaseModel):\n    \"\"\"\n    A tokenizer that can encode and decode strings.\n    \"\"\"\n\n    encode: Callable[[str], List[int]]\n    decode: Callable[[List[int]], str]\n</code></pre> <code>tool</code> Classes <code>FunctionJSONSchema</code> <p>               Bases: <code>BaseModel</code></p> <p>A function according to JSON schema standards.</p> <p>This is also passed in to OpenAI ChatCompletion API functions list so the AI understands how to call a function.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function</p> <code>description</code> <code>str</code> <p>A description of the function</p> <code>parameters</code> <code>Parameters</code> <p>A dictionary of parameters and their types (Optional)</p> <p>Example:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class FunctionJSONSchema(BaseModel):\n    \"\"\"\n    A function according to JSON schema standards.\n\n    This is also passed in to OpenAI ChatCompletion API\n    functions list so the AI understands how to call a function.\n\n    Attributes:\n        name: The name of the function\n        description: A description of the function\n        parameters: A dictionary of parameters and their types (Optional)\n\n    Example:\n\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: Parameters\n</code></pre> <code>Tool</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre> Functions <code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre> <code>ToolCall</code> <p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre> <code>ToolCallResponse</code> <p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre> <code>ToolCalls</code> <p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre> Modules <code>decorator</code> Classes <code>ToolWrapper</code> <p>A wrapper class that makes a function behave like a Tool instance.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\decorator.py</code> <pre><code>class ToolWrapper:\n    \"\"\"A wrapper class that makes a function behave like a Tool instance.\"\"\"\n\n    def __init__(self, func: Callable):\n        self._tool = Tool(function=func)\n        update_wrapper(self, func)\n\n        # Copy commonly accessed attributes\n        self.function = self._tool.function\n        self.description = self._tool.description\n        self.name = self._tool.name\n        self.use_function_call = self._tool.use_function_call\n\n    def __call__(self, *args, **kwargs):\n        return self._tool(*args, **kwargs)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        return getattr(self._tool, name)\n\n    def __str__(self) -&gt; str:\n        return self._tool.model_dump_json()\n</code></pre> Functions <code>tool(func)</code> <p>A decorator that converts a function into a Tool. This allows for a more concise way to create tools compared to using Tool(function).</p> <p>Example Usage:</p> <pre><code>@tool\ndef add_numbers(x: int, y: int) -&gt; int:\n    '''Add two numbers together.'''\n    return x + y\n\n# The function is now a Tool instance\nresult = add_numbers(2, 3)  # Returns \"5\" (as a JSON string)\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\decorator.py</code> <pre><code>def tool(func: Callable) -&gt; ToolWrapper:\n    \"\"\"\n    A decorator that converts a function into a Tool.\n    This allows for a more concise way to create tools compared to using Tool(function).\n\n    Example Usage:\n    ```python\n    @tool\n    def add_numbers(x: int, y: int) -&gt; int:\n        '''Add two numbers together.'''\n        return x + y\n\n    # The function is now a Tool instance\n    result = add_numbers(2, 3)  # Returns \"5\" (as a JSON string)\n    ```\n    \"\"\"\n    return ToolWrapper(func)\n</code></pre> <code>tool</code> Classes <code>Tool</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre> Functions <code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre> Functions <code>util</code> Classes <code>FunctionJSONSchema</code> <p>               Bases: <code>BaseModel</code></p> <p>A function according to JSON schema standards.</p> <p>This is also passed in to OpenAI ChatCompletion API functions list so the AI understands how to call a function.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function</p> <code>description</code> <code>str</code> <p>A description of the function</p> <code>parameters</code> <code>Parameters</code> <p>A dictionary of parameters and their types (Optional)</p> <p>Example:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class FunctionJSONSchema(BaseModel):\n    \"\"\"\n    A function according to JSON schema standards.\n\n    This is also passed in to OpenAI ChatCompletion API\n    functions list so the AI understands how to call a function.\n\n    Attributes:\n        name: The name of the function\n        description: A description of the function\n        parameters: A dictionary of parameters and their types (Optional)\n\n    Example:\n\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: Parameters\n</code></pre> <code>Parameter</code> <p>               Bases: <code>BaseModel</code></p> <p>The parameter of a function according to JSON schema standards. (Used by OpenAI function calling)</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class Parameter(BaseModel):\n    \"\"\"\n    The parameter of a function according to JSON schema standards.\n    (Used by OpenAI function calling)\n    \"\"\"\n\n    type: str\n    description: Optional[str] = None\n    enum: Optional[list[str]] = None\n</code></pre> <code>Parameters</code> <p>               Bases: <code>BaseModel</code></p> <p>The parameters property of a function according to JSON schema standards. (Used by OpenAI function calling)</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class Parameters(BaseModel):\n    \"\"\"\n    The parameters property of a function according to\n    JSON schema standards. (Used by OpenAI function calling)\n    \"\"\"\n\n    type: str\n    properties: Dict[str, Parameter] = {}\n    required: Optional[List[str]] = None\n</code></pre> <code>ToolCall</code> <p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre> <code>ToolCallResponse</code> <p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre> <code>ToolCalls</code> <p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre> Functions <code>generate_function_json(func)</code> <p>Generates a FunctionJSONSchema from a python function.</p> <p>Example:</p> <pre><code>def roll_dice(sides: int = 20,\n              dice: int = 1,\n              modifier: int = 0,\n              advantage: bool = False,\n              disadvantage: bool = False):\n    \"\"\"\n    Rolls a number of dice with a given number of sides, optionally with a modifier\n    and/or advantage/disadvantage.\n    Returns `{result: int, rolls: int[]}`\n\n    Args:\n        sides: The number of sides on each die (default 20)\n        dice: The number of dice to roll (default 1)\n        modifier: The modifier to add to the roll total (default 0)\n        advantage: Whether to roll with advantage (default False)\n        disadvantage: Whether to roll with disadvantage (default False)\n    \"\"\"\n    ...\n</code></pre> <p>Usage:</p> <pre><code>result = generate_function_json(roll_dice)\nprint(result)\n</code></pre> <p>Expected Output:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def generate_function_json(func: Callable[..., Any]) -&gt; FunctionJSONSchema:\n    \"\"\"\n    Generates a FunctionJSONSchema from a python function.\n\n    Example:\n    ```python\n    def roll_dice(sides: int = 20,\n                  dice: int = 1,\n                  modifier: int = 0,\n                  advantage: bool = False,\n                  disadvantage: bool = False):\n        \\\"\"\"\n        Rolls a number of dice with a given number of sides, optionally with a modifier\n        and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\n\n        Args:\n            sides: The number of sides on each die (default 20)\n            dice: The number of dice to roll (default 1)\n            modifier: The modifier to add to the roll total (default 0)\n            advantage: Whether to roll with advantage (default False)\n            disadvantage: Whether to roll with disadvantage (default False)\n        \\\"\"\"\n        ...\n    ```\n\n    Usage:\n    ```python\n    result = generate_function_json(roll_dice)\n    print(result)\n    ```\n\n    Expected Output:\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n    sig = inspect.signature(func)\n    doc = inspect.getdoc(func)\n\n    description = \"\"\n    args_docs = {}\n    if doc:\n        doc_lines = doc.split(\"\\n\")\n        description = doc_lines[0]  # Capture the first line as part of the description.\n        args_lines = doc_lines[1:]\n\n        capturing_description = True\n        start_capturing = False\n        for line in args_lines:\n            if line.strip().lower() in [\"args:\", \"arguments:\"]:\n                start_capturing = True\n                capturing_description = False\n                continue\n\n            if capturing_description:\n                description += \"\\n\" + line\n            elif start_capturing:\n                match = re.match(r\"^\\s+(?P&lt;name&gt;\\w+):\\s(?P&lt;desc&gt;.*)\", line)\n                if match:\n                    args_docs[match.group(\"name\")] = match.group(\"desc\")\n\n    properties = {}\n    required = []\n    for name, param in sig.parameters.items():\n        param_type = param.annotation if param.annotation is not inspect._empty else Any\n        type_schema = python_type_to_json_schema_type(param_type)\n\n        parameter_info = {\"description\": args_docs.get(name, \"\")}\n\n        # Handle different types of type_schema\n        if isinstance(type_schema, str):\n            parameter_info[\"type\"] = type_schema\n        elif isinstance(type_schema, dict):\n            parameter_info.update(type_schema)\n\n        properties[name] = Parameter(**parameter_info)\n\n        if param.default is param.empty:\n            required.append(name)\n\n    parameters_model = Parameters(\n        type=\"object\", properties=properties, required=required or None\n    )\n\n    return FunctionJSONSchema(\n        name=func.__name__, description=description.strip(), parameters=parameters_model\n    )\n</code></pre> <code>python_type_to_json_schema_type(python_type)</code> <p>Maps Python types to JSON Schema types.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type[Any]</code> <p>The Python type.</p> required <p>Returns:</p> Type Description <code>Union[str, Dict]</code> <p>Union[str, Dict]: The corresponding JSON Schema type or schema.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def python_type_to_json_schema_type(python_type: Type[Any]) -&gt; Union[str, Dict]:\n    \"\"\"\n    Maps Python types to JSON Schema types.\n\n    Args:\n      python_type: The Python type.\n\n    Returns:\n      Union[str, Dict]: The corresponding JSON Schema type or schema.\n    \"\"\"\n    simple_type_mapping = {\n        int: \"integer\",\n        float: \"number\",\n        str: \"string\",\n        bool: \"boolean\",\n        type(None): \"null\",\n    }\n\n    if python_type in simple_type_mapping:\n        return simple_type_mapping[python_type]\n\n    if hasattr(python_type, \"__origin__\"):\n        origin = python_type.__origin__  # type: ignore\n\n        if origin is Union:\n            types = python_type.__args__  # type: ignore\n            if type(None) in types:\n                # This is equivalent to Optional[T]\n                types = [t for t in types if t is not type(None)]\n                if len(types) == 1:\n                    return python_type_to_json_schema_type(types[0])\n\n        if origin is list:\n            item_type = (\n                python_type.__args__[0] if python_type.__args__ else Any\n            )  # type: ignore\n            return {\n                \"type\": \"array\",\n                \"items\": {\"type\": python_type_to_json_schema_type(item_type)},\n            }\n        elif origin is dict:\n            key_type = (\n                python_type.__args__[0] if python_type.__args__ else Any\n            )  # type: ignore\n            value_type = (\n                python_type.__args__[1] if python_type.__args__ else Any\n            )  # type: ignore\n            if key_type is not str:\n                raise ValueError(\n                    \"Dictionary key type must be str for conversion to JSON schema\"\n                )\n            return {\n                \"type\": \"object\",\n                \"additionalProperties\": python_type_to_json_schema_type(value_type),\n            }\n\n    if hasattr(python_type, \"__annotations__\"):\n        properties = {}\n        for k, v in python_type.__annotations__.items():\n            type_schema = python_type_to_json_schema_type(v)\n            if isinstance(type_schema, str):\n                properties[k] = {\"type\": type_schema}\n            elif isinstance(type_schema, dict):\n                properties[k] = type_schema\n        required = [\n            k\n            for k in properties\n            if k not in python_type.__optional_keys__  # type: ignore\n        ]\n        return {\n            \"type\": \"object\",\n            \"properties\": properties,\n            **({\"required\": required} if required else {}),\n        }\n\n    print(f\"Unknown type encountered: {python_type}\")\n    return \"unknown\"\n</code></pre>"},{"location":"api/#SilverLingua.core.molecules","title":"<code>molecules</code>","text":""},{"location":"api/#SilverLingua.core.molecules-classes","title":"Classes","text":"<code>Link</code> <p>               Bases: <code>Memory</code></p> <p>A memory that can have a parent and children Links, forming a hierarchical structure of interconnected memories.</p> <p>The content can be either a Notion or a Memory. (You can still use the content as a string via str(link.content).</p> Source code in <code>src\\SilverLingua\\core\\molecules\\link.py</code> <pre><code>class Link(Memory):\n    \"\"\"\n    A memory that can have a parent and children Links,\n    forming a hierarchical structure of interconnected memories.\n\n    The content can be either a Notion or a Memory.\n    (You can still use the content as a string via str(link.content).\n    \"\"\"\n\n    content: Union[Notion, Memory]\n    parent: Optional[\"Link\"] = None\n    children: List[\"Link\"] = Field(default_factory=list)\n\n    def add_child(self, child: \"Link\") -&gt; None:\n        self.children.append(child)\n        child.parent = self\n\n    def remove_child(self, child: \"Link\") -&gt; None:\n        self.children.remove(child)\n        child.parent = None\n\n    @property\n    def path(self) -&gt; List[\"Link\"]:\n        \"\"\"\n        Returns the path from the root to this Link.\n        \"\"\"\n        path = [self]\n        while path[-1].parent is not None:\n            path.append(path[-1].parent)\n        return path\n\n    @property\n    def root(self) -&gt; \"Link\":\n        \"\"\"\n        Returns the root Link of this Link.\n        \"\"\"\n        return self.path[-1]\n\n    @property\n    def depth(self) -&gt; int:\n        \"\"\"\n        Returns 1 based depth of this Link.\n        \"\"\"\n        return len(self.path)\n\n    @property\n    def is_root(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a root Link.\n        \"\"\"\n        return self.parent is None\n\n    @property\n    def is_leaf(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a leaf Link.\n        \"\"\"\n        return len(self.children) == 0\n\n    @property\n    def is_branch(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a branch Link.\n        \"\"\"\n        return not self.is_leaf\n\n    @property\n    def path_string(self) -&gt; str:\n        \"\"\"\n        Returns the path from the root to this Link as a string.\n\n        Example:\n        \"root&gt;child&gt;grandchild\"\n        \"\"\"\n        path_str = f\"{self.content}\"\n        if not self.is_root:\n            path_str = f\"{self.parent.path_string}&gt;{path_str}\"\n        return path_str\n</code></pre> Attributes <code>depth: int</code> <code>property</code> <p>Returns 1 based depth of this Link.</p> <code>is_branch: bool</code> <code>property</code> <p>Returns whether this Link is a branch Link.</p> <code>is_leaf: bool</code> <code>property</code> <p>Returns whether this Link is a leaf Link.</p> <code>is_root: bool</code> <code>property</code> <p>Returns whether this Link is a root Link.</p> <code>path: List[Link]</code> <code>property</code> <p>Returns the path from the root to this Link.</p> <code>path_string: str</code> <code>property</code> <p>Returns the path from the root to this Link as a string.</p> <p>Example: \"root&gt;child&gt;grandchild\"</p> <code>root: Link</code> <code>property</code> <p>Returns the root Link of this Link.</p> <code>Notion</code> <p>               Bases: <code>Memory</code></p> <p>A memory that stores the role associated with its content. The role is usually a <code>ChatRole</code> or a <code>ReactRole</code>. (See <code>atoms/roles</code>)</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the notion.</p> <code>content</code> <code>str</code> <p>The content of the memory.</p> <code>persistent</code> <code>bool</code> <p>Whether the notion should be stored in long-term memory.</p> Source code in <code>src\\SilverLingua\\core\\molecules\\notion.py</code> <pre><code>class Notion(Memory):\n    \"\"\"\n    A memory that stores the role associated with its content.\n    The role is usually a `ChatRole` or a `ReactRole`.\n    (See `atoms/roles`)\n\n    Attributes:\n        role: The role of the notion.\n        content: The content of the memory.\n        persistent: Whether the notion should be stored in long-term memory.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n    role: str\n    persistent: bool = False\n\n    @field_validator(\"role\", mode=\"before\")\n    @classmethod\n    def validate_role(cls, v: Union[ChatRole, ReactRole, str]):\n        if isinstance(v, (ChatRole, ReactRole)):\n            return v.value.value\n        elif isinstance(v, str):\n            return v\n        raise ValueError(f\"Expected a ChatRole, ReactRole, or a string, got {type(v)}\")\n\n    def __str__(self) -&gt; str:\n        return f\"{self.role}: {self.content}\"\n\n    def __init__(\n        self,\n        content: str,\n        role: Union[ChatRole, ReactRole, str],\n        persistent: bool = False,\n    ):\n        super().__init__(content=content, role=role, persistent=persistent)\n\n    @property\n    def chat_role(self) -&gt; ChatRole:\n        \"\"\"\n        Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_chat_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ChatRole.AI\n        return r\n\n    @property\n    def react_role(self) -&gt; ReactRole:\n        \"\"\"\n        Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_react_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ReactRole.THOUGHT\n        return r\n</code></pre> Attributes <code>chat_role: ChatRole</code> <code>property</code> <p>Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)</p> <p>(See <code>config</code>)</p> <code>react_role: ReactRole</code> <code>property</code> <p>Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)</p> <p>(See <code>config</code>)</p>"},{"location":"api/#SilverLingua.core.molecules-modules","title":"Modules","text":"<code>link</code> Classes <code>Link</code> <p>               Bases: <code>Memory</code></p> <p>A memory that can have a parent and children Links, forming a hierarchical structure of interconnected memories.</p> <p>The content can be either a Notion or a Memory. (You can still use the content as a string via str(link.content).</p> Source code in <code>src\\SilverLingua\\core\\molecules\\link.py</code> <pre><code>class Link(Memory):\n    \"\"\"\n    A memory that can have a parent and children Links,\n    forming a hierarchical structure of interconnected memories.\n\n    The content can be either a Notion or a Memory.\n    (You can still use the content as a string via str(link.content).\n    \"\"\"\n\n    content: Union[Notion, Memory]\n    parent: Optional[\"Link\"] = None\n    children: List[\"Link\"] = Field(default_factory=list)\n\n    def add_child(self, child: \"Link\") -&gt; None:\n        self.children.append(child)\n        child.parent = self\n\n    def remove_child(self, child: \"Link\") -&gt; None:\n        self.children.remove(child)\n        child.parent = None\n\n    @property\n    def path(self) -&gt; List[\"Link\"]:\n        \"\"\"\n        Returns the path from the root to this Link.\n        \"\"\"\n        path = [self]\n        while path[-1].parent is not None:\n            path.append(path[-1].parent)\n        return path\n\n    @property\n    def root(self) -&gt; \"Link\":\n        \"\"\"\n        Returns the root Link of this Link.\n        \"\"\"\n        return self.path[-1]\n\n    @property\n    def depth(self) -&gt; int:\n        \"\"\"\n        Returns 1 based depth of this Link.\n        \"\"\"\n        return len(self.path)\n\n    @property\n    def is_root(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a root Link.\n        \"\"\"\n        return self.parent is None\n\n    @property\n    def is_leaf(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a leaf Link.\n        \"\"\"\n        return len(self.children) == 0\n\n    @property\n    def is_branch(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a branch Link.\n        \"\"\"\n        return not self.is_leaf\n\n    @property\n    def path_string(self) -&gt; str:\n        \"\"\"\n        Returns the path from the root to this Link as a string.\n\n        Example:\n        \"root&gt;child&gt;grandchild\"\n        \"\"\"\n        path_str = f\"{self.content}\"\n        if not self.is_root:\n            path_str = f\"{self.parent.path_string}&gt;{path_str}\"\n        return path_str\n</code></pre> Attributes <code>depth: int</code> <code>property</code> <p>Returns 1 based depth of this Link.</p> <code>is_branch: bool</code> <code>property</code> <p>Returns whether this Link is a branch Link.</p> <code>is_leaf: bool</code> <code>property</code> <p>Returns whether this Link is a leaf Link.</p> <code>is_root: bool</code> <code>property</code> <p>Returns whether this Link is a root Link.</p> <code>path: List[Link]</code> <code>property</code> <p>Returns the path from the root to this Link.</p> <code>path_string: str</code> <code>property</code> <p>Returns the path from the root to this Link as a string.</p> <p>Example: \"root&gt;child&gt;grandchild\"</p> <code>root: Link</code> <code>property</code> <p>Returns the root Link of this Link.</p> <code>notion</code> Classes <code>Notion</code> <p>               Bases: <code>Memory</code></p> <p>A memory that stores the role associated with its content. The role is usually a <code>ChatRole</code> or a <code>ReactRole</code>. (See <code>atoms/roles</code>)</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the notion.</p> <code>content</code> <code>str</code> <p>The content of the memory.</p> <code>persistent</code> <code>bool</code> <p>Whether the notion should be stored in long-term memory.</p> Source code in <code>src\\SilverLingua\\core\\molecules\\notion.py</code> <pre><code>class Notion(Memory):\n    \"\"\"\n    A memory that stores the role associated with its content.\n    The role is usually a `ChatRole` or a `ReactRole`.\n    (See `atoms/roles`)\n\n    Attributes:\n        role: The role of the notion.\n        content: The content of the memory.\n        persistent: Whether the notion should be stored in long-term memory.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n    role: str\n    persistent: bool = False\n\n    @field_validator(\"role\", mode=\"before\")\n    @classmethod\n    def validate_role(cls, v: Union[ChatRole, ReactRole, str]):\n        if isinstance(v, (ChatRole, ReactRole)):\n            return v.value.value\n        elif isinstance(v, str):\n            return v\n        raise ValueError(f\"Expected a ChatRole, ReactRole, or a string, got {type(v)}\")\n\n    def __str__(self) -&gt; str:\n        return f\"{self.role}: {self.content}\"\n\n    def __init__(\n        self,\n        content: str,\n        role: Union[ChatRole, ReactRole, str],\n        persistent: bool = False,\n    ):\n        super().__init__(content=content, role=role, persistent=persistent)\n\n    @property\n    def chat_role(self) -&gt; ChatRole:\n        \"\"\"\n        Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_chat_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ChatRole.AI\n        return r\n\n    @property\n    def react_role(self) -&gt; ReactRole:\n        \"\"\"\n        Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_react_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ReactRole.THOUGHT\n        return r\n</code></pre> Attributes <code>chat_role: ChatRole</code> <code>property</code> <p>Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)</p> <p>(See <code>config</code>)</p> <code>react_role: ReactRole</code> <code>property</code> <p>Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)</p> <p>(See <code>config</code>)</p>"},{"location":"api/#SilverLingua.core.organisms","title":"<code>organisms</code>","text":""},{"location":"api/#SilverLingua.core.organisms-classes","title":"Classes","text":"<code>Idearium</code> <p>               Bases: <code>BaseModel</code></p> <p>A collection of <code>Notions</code> that is automatically trimmed to fit within a maximum number of tokens.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>class Idearium(BaseModel):\n    \"\"\"\n    A collection of `Notions` that is automatically trimmed to fit within a maximum\n    number of tokens.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    tokenizer: Tokenizer\n    max_tokens: int\n    notions: List[Notion] = Field(default_factory=list)\n    tokenized_notions: List[List[int]] = Field(default_factory=list)\n    persistent_indices: set = Field(default_factory=set)\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        max_tokens: int,\n        notions: List[Notion] = None,\n        **kwargs,\n    ):\n        # Initialize with empty notions if None\n        notions = notions or []\n\n        # Initialize tokenized_notions\n        tokenized_notions = [tokenizer.encode(notion.content) for notion in notions]\n\n        # Call parent init with all values\n        super().__init__(\n            tokenizer=tokenizer,\n            max_tokens=max_tokens,\n            notions=notions,\n            tokenized_notions=tokenized_notions,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_notions(cls, values):\n        notions = values.notions\n        for notion in notions:\n            cls.validate_notion(notion, values.max_tokens, values.tokenizer)\n        return values\n\n    @classmethod\n    def validate_notion(cls, notion: Notion, max_tokens: int, tokenizer: Tokenizer):\n        if len(notion.content) == 0:\n            raise ValueError(\"Notion content cannot be empty.\")\n\n        tokenized_notion = tokenizer.encode(notion.content)\n        if len(tokenized_notion) &gt; max_tokens:\n            raise ValueError(\"Notion exceeds maximum token length\")\n\n        return tokenized_notion\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"The total number of tokens in the Idearium.\"\"\"\n        return sum(len(notion) for notion in self.tokenized_notions)\n\n    @property\n    def _non_persistent_indices(self) -&gt; set:\n        \"\"\"The indices of non-persistent notions.\"\"\"\n        return set(range(len(self.notions))) - self.persistent_indices\n\n    def index(self, notion: Notion) -&gt; int:\n        \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n        return self.notions.index(notion)\n\n    def append(self, notion: Notion):\n        \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n        logger.debug(f\"Appending notion: {notion.content!r}\")\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        if self.notions:\n            logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n        if (\n            self.notions\n            and self.notions[-1].role == notion.role\n            and self.notions[-1].persistent == notion.persistent\n        ):\n            combined_content = self.notions[-1].content + notion.content\n            combined_notion = Notion(\n                content=combined_content,\n                role=notion.role,\n                persistent=notion.persistent,\n            )\n            self.replace(len(self.notions) - 1, combined_notion)\n            logger.debug(\n                f\"After replace, about to return combined content: {combined_content!r}\"\n            )\n            return\n\n        logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n        self.notions.append(notion)\n        self.tokenized_notions.append(tokenized_notion)\n\n        if notion.persistent:\n            # Modify the set in place instead of reassigning\n            self.persistent_indices.add(len(self.notions) - 1)\n\n        self._trim()\n\n    def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n        \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n        if isinstance(notions, Idearium):\n            notions = notions.notions\n\n        for notion in notions:\n            self.append(notion)\n\n    def insert(self, index: int, notion: Notion):\n        \"\"\"Inserts the given notion at the given index.\"\"\"\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        self.notions.insert(index, notion)\n        self.tokenized_notions.insert(index, tokenized_notion)\n\n        # Update persistent_indices in place\n        new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n        if notion.persistent:\n            self.persistent_indices.add(index)\n\n        self._trim()\n\n    def remove(self, notion: Notion):\n        \"\"\"Removes the first occurrence of the given notion.\"\"\"\n        index = self.index(notion)\n        self.pop(index)\n\n    def pop(self, index: int) -&gt; Notion:\n        \"\"\"Removes and returns the notion at the given index.\"\"\"\n        ret = self.notions.pop(index)\n        self.tokenized_notions.pop(index)\n\n        # Update persistent_indices\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.discard(index)\n        new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n\n        return ret\n\n    def replace(self, index: int, notion: Notion):\n        \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n        self.notions[index] = notion\n        self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n        # Update persistent_indices based on the replaced notion\n        if notion.persistent:\n            self.persistent_indices.add(index)\n        else:\n            self.persistent_indices.discard(index)\n\n        self._trim()\n\n    def copy(self) -&gt; \"Idearium\":\n        \"\"\"Returns a copy of the Idearium.\"\"\"\n        return Idearium(\n            tokenizer=self.tokenizer,\n            max_tokens=self.max_tokens,\n            notions=self.notions.copy(),\n            tokenized_notions=self.tokenized_notions.copy(),\n            persistent_indices=self.persistent_indices.copy(),\n        )\n\n    def _trim(self):\n        \"\"\"\n        Trims the Idearium to fit within the maximum number of tokens, called\n        after every modification.\n\n        This is the primary point of extension for Idearium subclasses, as it\n        allows for custom trimming behavior.\n        \"\"\"\n        while self.total_tokens &gt; self.max_tokens:\n            non_persistent_indices = self._non_persistent_indices\n\n            # Check if there's only one non-persistent user message\n            if len(non_persistent_indices) == 1:\n                single_index = next(iter(non_persistent_indices))\n                tokenized_notion = self.tokenized_notions[single_index]\n\n                # Trim the only non-persistent notion to fit within the token limit\n                tokenized_notion = tokenized_notion[\n                    : self.max_tokens - (self.total_tokens - len(tokenized_notion))\n                ]\n                trimmed_content = self.tokenizer.decode(tokenized_notion)\n                trimmed_notion = Notion(\n                    content=trimmed_content,\n                    role=self.notions[single_index].role,\n                    persistent=self.notions[single_index].persistent,\n                )\n                self.replace(single_index, trimmed_notion)\n                return\n\n            # Attempt to remove the first non-persistent notion\n            for i in non_persistent_indices:\n                self.pop(i)\n                break\n            else:\n                # If all notions are persistent and\n                # the max token length is still exceeded\n                raise ValueError(\n                    \"Persistent notions exceed max_tokens.\"\n                    + \" Reduce the content or increase max_tokens.\"\n                )\n\n    def __len__(self) -&gt; int:\n        return len(self.notions)\n\n    def __getitem__(self, index: int) -&gt; Notion:\n        return self.notions[index]\n\n    def __setitem__(self, index: int, notion: Notion):\n        self.replace(index, notion)\n\n    def __delitem__(self, index: int):\n        self.pop(index)\n\n    def __iter__(self) -&gt; Iterator[Notion]:\n        return iter(self.notions)\n\n    def __contains__(self, notion: Notion) -&gt; bool:\n        return notion in self.notions\n\n    def __str__(self) -&gt; str:\n        return str(self.notions)\n\n    def __repr__(self) -&gt; str:\n        return repr(self.notions)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Idearium):\n            return NotImplemented\n        return self.notions == other.notions\n</code></pre> Attributes <code>total_tokens: int</code> <code>property</code> <p>The total number of tokens in the Idearium.</p> Functions <code>append(notion)</code> <p>Appends the given notion to the end of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def append(self, notion: Notion):\n    \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n    logger.debug(f\"Appending notion: {notion.content!r}\")\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    if self.notions:\n        logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n    if (\n        self.notions\n        and self.notions[-1].role == notion.role\n        and self.notions[-1].persistent == notion.persistent\n    ):\n        combined_content = self.notions[-1].content + notion.content\n        combined_notion = Notion(\n            content=combined_content,\n            role=notion.role,\n            persistent=notion.persistent,\n        )\n        self.replace(len(self.notions) - 1, combined_notion)\n        logger.debug(\n            f\"After replace, about to return combined content: {combined_content!r}\"\n        )\n        return\n\n    logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n    self.notions.append(notion)\n    self.tokenized_notions.append(tokenized_notion)\n\n    if notion.persistent:\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.add(len(self.notions) - 1)\n\n    self._trim()\n</code></pre> <code>copy()</code> <p>Returns a copy of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def copy(self) -&gt; \"Idearium\":\n    \"\"\"Returns a copy of the Idearium.\"\"\"\n    return Idearium(\n        tokenizer=self.tokenizer,\n        max_tokens=self.max_tokens,\n        notions=self.notions.copy(),\n        tokenized_notions=self.tokenized_notions.copy(),\n        persistent_indices=self.persistent_indices.copy(),\n    )\n</code></pre> <code>extend(notions)</code> <p>Extends the Idearium with the given list of notions.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n    \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n    if isinstance(notions, Idearium):\n        notions = notions.notions\n\n    for notion in notions:\n        self.append(notion)\n</code></pre> <code>index(notion)</code> <p>Returns the index of the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def index(self, notion: Notion) -&gt; int:\n    \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n    return self.notions.index(notion)\n</code></pre> <code>insert(index, notion)</code> <p>Inserts the given notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def insert(self, index: int, notion: Notion):\n    \"\"\"Inserts the given notion at the given index.\"\"\"\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    self.notions.insert(index, notion)\n    self.tokenized_notions.insert(index, tokenized_notion)\n\n    # Update persistent_indices in place\n    new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n    if notion.persistent:\n        self.persistent_indices.add(index)\n\n    self._trim()\n</code></pre> <code>pop(index)</code> <p>Removes and returns the notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def pop(self, index: int) -&gt; Notion:\n    \"\"\"Removes and returns the notion at the given index.\"\"\"\n    ret = self.notions.pop(index)\n    self.tokenized_notions.pop(index)\n\n    # Update persistent_indices\n    # Modify the set in place instead of reassigning\n    self.persistent_indices.discard(index)\n    new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n\n    return ret\n</code></pre> <code>remove(notion)</code> <p>Removes the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def remove(self, notion: Notion):\n    \"\"\"Removes the first occurrence of the given notion.\"\"\"\n    index = self.index(notion)\n    self.pop(index)\n</code></pre> <code>replace(index, notion)</code> <p>Replaces the notion at the given index with the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def replace(self, index: int, notion: Notion):\n    \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n    self.notions[index] = notion\n    self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n    # Update persistent_indices based on the replaced notion\n    if notion.persistent:\n        self.persistent_indices.add(index)\n    else:\n        self.persistent_indices.discard(index)\n\n    self._trim()\n</code></pre>"},{"location":"api/#SilverLingua.core.organisms-modules","title":"Modules","text":"<code>idearium</code> Classes <code>Idearium</code> <p>               Bases: <code>BaseModel</code></p> <p>A collection of <code>Notions</code> that is automatically trimmed to fit within a maximum number of tokens.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>class Idearium(BaseModel):\n    \"\"\"\n    A collection of `Notions` that is automatically trimmed to fit within a maximum\n    number of tokens.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    tokenizer: Tokenizer\n    max_tokens: int\n    notions: List[Notion] = Field(default_factory=list)\n    tokenized_notions: List[List[int]] = Field(default_factory=list)\n    persistent_indices: set = Field(default_factory=set)\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        max_tokens: int,\n        notions: List[Notion] = None,\n        **kwargs,\n    ):\n        # Initialize with empty notions if None\n        notions = notions or []\n\n        # Initialize tokenized_notions\n        tokenized_notions = [tokenizer.encode(notion.content) for notion in notions]\n\n        # Call parent init with all values\n        super().__init__(\n            tokenizer=tokenizer,\n            max_tokens=max_tokens,\n            notions=notions,\n            tokenized_notions=tokenized_notions,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_notions(cls, values):\n        notions = values.notions\n        for notion in notions:\n            cls.validate_notion(notion, values.max_tokens, values.tokenizer)\n        return values\n\n    @classmethod\n    def validate_notion(cls, notion: Notion, max_tokens: int, tokenizer: Tokenizer):\n        if len(notion.content) == 0:\n            raise ValueError(\"Notion content cannot be empty.\")\n\n        tokenized_notion = tokenizer.encode(notion.content)\n        if len(tokenized_notion) &gt; max_tokens:\n            raise ValueError(\"Notion exceeds maximum token length\")\n\n        return tokenized_notion\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"The total number of tokens in the Idearium.\"\"\"\n        return sum(len(notion) for notion in self.tokenized_notions)\n\n    @property\n    def _non_persistent_indices(self) -&gt; set:\n        \"\"\"The indices of non-persistent notions.\"\"\"\n        return set(range(len(self.notions))) - self.persistent_indices\n\n    def index(self, notion: Notion) -&gt; int:\n        \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n        return self.notions.index(notion)\n\n    def append(self, notion: Notion):\n        \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n        logger.debug(f\"Appending notion: {notion.content!r}\")\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        if self.notions:\n            logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n        if (\n            self.notions\n            and self.notions[-1].role == notion.role\n            and self.notions[-1].persistent == notion.persistent\n        ):\n            combined_content = self.notions[-1].content + notion.content\n            combined_notion = Notion(\n                content=combined_content,\n                role=notion.role,\n                persistent=notion.persistent,\n            )\n            self.replace(len(self.notions) - 1, combined_notion)\n            logger.debug(\n                f\"After replace, about to return combined content: {combined_content!r}\"\n            )\n            return\n\n        logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n        self.notions.append(notion)\n        self.tokenized_notions.append(tokenized_notion)\n\n        if notion.persistent:\n            # Modify the set in place instead of reassigning\n            self.persistent_indices.add(len(self.notions) - 1)\n\n        self._trim()\n\n    def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n        \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n        if isinstance(notions, Idearium):\n            notions = notions.notions\n\n        for notion in notions:\n            self.append(notion)\n\n    def insert(self, index: int, notion: Notion):\n        \"\"\"Inserts the given notion at the given index.\"\"\"\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        self.notions.insert(index, notion)\n        self.tokenized_notions.insert(index, tokenized_notion)\n\n        # Update persistent_indices in place\n        new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n        if notion.persistent:\n            self.persistent_indices.add(index)\n\n        self._trim()\n\n    def remove(self, notion: Notion):\n        \"\"\"Removes the first occurrence of the given notion.\"\"\"\n        index = self.index(notion)\n        self.pop(index)\n\n    def pop(self, index: int) -&gt; Notion:\n        \"\"\"Removes and returns the notion at the given index.\"\"\"\n        ret = self.notions.pop(index)\n        self.tokenized_notions.pop(index)\n\n        # Update persistent_indices\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.discard(index)\n        new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n\n        return ret\n\n    def replace(self, index: int, notion: Notion):\n        \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n        self.notions[index] = notion\n        self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n        # Update persistent_indices based on the replaced notion\n        if notion.persistent:\n            self.persistent_indices.add(index)\n        else:\n            self.persistent_indices.discard(index)\n\n        self._trim()\n\n    def copy(self) -&gt; \"Idearium\":\n        \"\"\"Returns a copy of the Idearium.\"\"\"\n        return Idearium(\n            tokenizer=self.tokenizer,\n            max_tokens=self.max_tokens,\n            notions=self.notions.copy(),\n            tokenized_notions=self.tokenized_notions.copy(),\n            persistent_indices=self.persistent_indices.copy(),\n        )\n\n    def _trim(self):\n        \"\"\"\n        Trims the Idearium to fit within the maximum number of tokens, called\n        after every modification.\n\n        This is the primary point of extension for Idearium subclasses, as it\n        allows for custom trimming behavior.\n        \"\"\"\n        while self.total_tokens &gt; self.max_tokens:\n            non_persistent_indices = self._non_persistent_indices\n\n            # Check if there's only one non-persistent user message\n            if len(non_persistent_indices) == 1:\n                single_index = next(iter(non_persistent_indices))\n                tokenized_notion = self.tokenized_notions[single_index]\n\n                # Trim the only non-persistent notion to fit within the token limit\n                tokenized_notion = tokenized_notion[\n                    : self.max_tokens - (self.total_tokens - len(tokenized_notion))\n                ]\n                trimmed_content = self.tokenizer.decode(tokenized_notion)\n                trimmed_notion = Notion(\n                    content=trimmed_content,\n                    role=self.notions[single_index].role,\n                    persistent=self.notions[single_index].persistent,\n                )\n                self.replace(single_index, trimmed_notion)\n                return\n\n            # Attempt to remove the first non-persistent notion\n            for i in non_persistent_indices:\n                self.pop(i)\n                break\n            else:\n                # If all notions are persistent and\n                # the max token length is still exceeded\n                raise ValueError(\n                    \"Persistent notions exceed max_tokens.\"\n                    + \" Reduce the content or increase max_tokens.\"\n                )\n\n    def __len__(self) -&gt; int:\n        return len(self.notions)\n\n    def __getitem__(self, index: int) -&gt; Notion:\n        return self.notions[index]\n\n    def __setitem__(self, index: int, notion: Notion):\n        self.replace(index, notion)\n\n    def __delitem__(self, index: int):\n        self.pop(index)\n\n    def __iter__(self) -&gt; Iterator[Notion]:\n        return iter(self.notions)\n\n    def __contains__(self, notion: Notion) -&gt; bool:\n        return notion in self.notions\n\n    def __str__(self) -&gt; str:\n        return str(self.notions)\n\n    def __repr__(self) -&gt; str:\n        return repr(self.notions)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Idearium):\n            return NotImplemented\n        return self.notions == other.notions\n</code></pre> Attributes <code>total_tokens: int</code> <code>property</code> <p>The total number of tokens in the Idearium.</p> Functions <code>append(notion)</code> <p>Appends the given notion to the end of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def append(self, notion: Notion):\n    \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n    logger.debug(f\"Appending notion: {notion.content!r}\")\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    if self.notions:\n        logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n    if (\n        self.notions\n        and self.notions[-1].role == notion.role\n        and self.notions[-1].persistent == notion.persistent\n    ):\n        combined_content = self.notions[-1].content + notion.content\n        combined_notion = Notion(\n            content=combined_content,\n            role=notion.role,\n            persistent=notion.persistent,\n        )\n        self.replace(len(self.notions) - 1, combined_notion)\n        logger.debug(\n            f\"After replace, about to return combined content: {combined_content!r}\"\n        )\n        return\n\n    logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n    self.notions.append(notion)\n    self.tokenized_notions.append(tokenized_notion)\n\n    if notion.persistent:\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.add(len(self.notions) - 1)\n\n    self._trim()\n</code></pre> <code>copy()</code> <p>Returns a copy of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def copy(self) -&gt; \"Idearium\":\n    \"\"\"Returns a copy of the Idearium.\"\"\"\n    return Idearium(\n        tokenizer=self.tokenizer,\n        max_tokens=self.max_tokens,\n        notions=self.notions.copy(),\n        tokenized_notions=self.tokenized_notions.copy(),\n        persistent_indices=self.persistent_indices.copy(),\n    )\n</code></pre> <code>extend(notions)</code> <p>Extends the Idearium with the given list of notions.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n    \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n    if isinstance(notions, Idearium):\n        notions = notions.notions\n\n    for notion in notions:\n        self.append(notion)\n</code></pre> <code>index(notion)</code> <p>Returns the index of the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def index(self, notion: Notion) -&gt; int:\n    \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n    return self.notions.index(notion)\n</code></pre> <code>insert(index, notion)</code> <p>Inserts the given notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def insert(self, index: int, notion: Notion):\n    \"\"\"Inserts the given notion at the given index.\"\"\"\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    self.notions.insert(index, notion)\n    self.tokenized_notions.insert(index, tokenized_notion)\n\n    # Update persistent_indices in place\n    new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n    if notion.persistent:\n        self.persistent_indices.add(index)\n\n    self._trim()\n</code></pre> <code>pop(index)</code> <p>Removes and returns the notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def pop(self, index: int) -&gt; Notion:\n    \"\"\"Removes and returns the notion at the given index.\"\"\"\n    ret = self.notions.pop(index)\n    self.tokenized_notions.pop(index)\n\n    # Update persistent_indices\n    # Modify the set in place instead of reassigning\n    self.persistent_indices.discard(index)\n    new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n\n    return ret\n</code></pre> <code>remove(notion)</code> <p>Removes the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def remove(self, notion: Notion):\n    \"\"\"Removes the first occurrence of the given notion.\"\"\"\n    index = self.index(notion)\n    self.pop(index)\n</code></pre> <code>replace(index, notion)</code> <p>Replaces the notion at the given index with the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def replace(self, index: int, notion: Notion):\n    \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n    self.notions[index] = notion\n    self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n    # Update persistent_indices based on the replaced notion\n    if notion.persistent:\n        self.persistent_indices.add(index)\n    else:\n        self.persistent_indices.discard(index)\n\n    self._trim()\n</code></pre>"},{"location":"api/#SilverLingua.core.templates","title":"<code>templates</code>","text":"<p>Core templates module.</p>"},{"location":"api/#SilverLingua.core.templates-classes","title":"Classes","text":"<code>Agent</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper around a model that utilizes an Idearium and a set of Tools.</p> <p>This is a base class not meant to be used directly. It is meant to be subclassed by specific model implementations.</p> <p>However, there is limited boilerplate. The only thing that needs to be redefined in subclasses is the <code>_bind_tools</code> method.</p> <p>Additionally, the <code>_use_tools</code> method is a common method to redefine.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>class Agent(BaseModel):\n    \"\"\"\n    A wrapper around a model that utilizes an Idearium and a set of Tools.\n\n    This is a base class not meant to be used directly. It is meant to be\n    subclassed by specific model implementations.\n\n    However, there is limited boilerplate. The only thing that needs to be\n    redefined in subclasses is the `_bind_tools` method.\n\n    Additionally, the `_use_tools` method is a common method to redefine.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    model: Model\n    idearium: Idearium\n    \"\"\"\n    The Idearium used by the agent.\n    \"\"\"\n    tools: List[Tool]\n    \"\"\"\n    The tools used by the agent.\n\n    WARNING: Do not modify this list directly. Use `add_tool`, `add_tools`,\n    and `remove_tool` instead.\n    \"\"\"\n    auto_append_response: bool = True\n    \"\"\"\n    Whether to automatically append the response to the idearium after\n    generating a response.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        idearium: Optional[Idearium] = None,\n        tools: Optional[List[Tool]] = None,\n        auto_append_response: bool = True,\n    ):\n        \"\"\"\n        Initializes the agent.\n\n        Args:\n            model (Model): The model to use.\n            idearium (Idearium, optional): The idearium to use.\n                If None, a new one will be created.\n            tools (List[Tool], optional): The tools to use.\n        \"\"\"\n        super().__init__(\n            model=model,\n            idearium=idearium\n            or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n            tools=tools or [],\n            auto_append_response=auto_append_response,\n        )\n\n    def model_post_init(self, __content):\n        self._bind_tools()\n\n    @property\n    def model(self) -&gt; Model:\n        \"\"\"\n        The model used by the agent.\n        \"\"\"\n        return self.model\n\n    @property\n    def role(self) -&gt; ChatRole:\n        \"\"\"\n        The ChatRole object for the model.\n        \"\"\"\n        return self.model.role\n\n    def _find_tool(self, name: str) -&gt; Tool | None:\n        \"\"\"\n        Finds a tool by name.\n        \"\"\"\n        for t in self.tools:\n            if t.name == name:\n                return t\n        return None\n\n    def _use_tools(self, tool_calls: ToolCalls) -&gt; List[Notion]:\n        \"\"\"\n        Uses Tools based on the given ToolCalls, returning Notions\n        containing ToolCallResponses.\n\n        Args:\n            tool_calls (ToolCalls): The ToolCalls to use.\n\n        Returns:\n            List[Notion]: The Notions containing ToolCallResponses.\n                Each Notion will have a role of ChatRole.TOOL_RESPONSE.\n        \"\"\"\n        responses: List[Notion] = []\n        for tool_call in tool_calls.list:\n            tool = self._find_tool(tool_call.function.name)\n            if tool is not None:\n                tc_function_response = {}\n                with contextlib.suppress(json.JSONDecodeError):\n                    tc_function_response = json.loads(tool_call.function.arguments)\n\n                tc_response = ToolCallResponse.from_tool_call(\n                    tool_call=tool_call, response=tool(**tc_function_response)\n                )\n                responses.append(\n                    Notion(\n                        content=tc_response.model_dump_json(exclude_none=True),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n            else:\n                responses.append(\n                    Notion(\n                        content=json.dumps(\n                            {\n                                \"tool_call_id\": tool_call.id,\n                                \"content\": \"Tool not found\",\n                                \"name\": \"error\",\n                            }\n                        ),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n        return responses\n\n    def _bind_tools(self) -&gt; None:\n        \"\"\"\n        Called at the end of __init__ to bind the tools to the model.\n\n        This MUST be redefined in subclasses to dictate how\n        the tools are bound to the model.\n\n        Example:\n        ```python\n        # From OpenAIChatAgent\n        def _bind_tools(self) -&gt; None:\n            m_tools: List[ChatCompletionToolParam] = [\n                {\"type\": \"function\", \"function\": tool.description}\n                for tool in self.tools\n            ]\n\n            if len(m_tools) &gt; 0:\n                self.model.tools = m_tools\n        ```\n        \"\"\"\n        pass\n\n    def add_tool(self, tool: Tool) -&gt; None:\n        \"\"\"\n        Adds a tool to the agent.\n        \"\"\"\n        self.tools.append(tool)\n        self._bind_tools()\n\n    def add_tools(self, tools: List[Tool]) -&gt; None:\n        \"\"\"\n        Adds a list of tools to the agent.\n        \"\"\"\n        self.tools.extend(tools)\n        self._bind_tools()\n\n    def remove_tool(self, name: str) -&gt; None:\n        \"\"\"\n        Removes a tool from the agent.\n        \"\"\"\n        for i, tool in enumerate(self.tools):\n            if tool.name == name:\n                self.tools.pop(i)\n                break\n        self._bind_tools()\n\n    def _process_messages(self, messages: Messages) -&gt; List[Notion]:\n        \"\"\"Convert various message types into a list of Notions.\"\"\"\n        if isinstance(messages, str):\n            return [Notion(content=messages, role=str(self.role.HUMAN.value))]\n        elif isinstance(messages, Notion):\n            return [messages]\n        elif isinstance(messages, Idearium):\n            return messages.notions\n        elif isinstance(messages, list):\n            return [\n                (\n                    Notion(content=msg, role=str(self.role.HUMAN.value))\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        raise ValueError(f\"Unsupported message type: {type(messages)}\")\n\n    def _process_generation(\n        self, responses: List[Notion], is_async=False\n    ) -&gt; List[Notion]:\n        \"\"\"Wrapper around shared logic between generate and agenerate.\"\"\"\n        response = responses[0]\n        # logger.debug(f\"Response: {response}\")\n        if response.chat_role == ChatRole.TOOL_CALL:\n            # logger.debug(\"Tool call detected\")\n            # Add the tool call to the idearium\n            self.idearium.append(response)\n            # Call generate again with the tool response\n            tool_calls = ToolCalls.model_validate_json(\n                '{\"list\": ' + response.content + \"}\"\n            )\n            tool_response = self._use_tools(tool_calls)\n            # logger.debug(f\"Tool response: {tool_response}\")\n            if is_async:\n                return self.agenerate(tool_response)\n            else:\n                return self.generate(tool_response)\n        else:\n            return responses\n\n    def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Generates a response to the given messages by calling the\n        underlying model's generate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = self.model.generate(self.idearium, **kwargs)\n        result = self._process_generation(responses)\n\n        if self.auto_append_response:\n            self.idearium.extend(result)\n\n        return result\n\n    async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Asynchronously generates a response to the given messages by calling the\n        underlying model's agenerate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = await self.model.agenerate(self.idearium, **kwargs)\n        result = self._process_generation(responses, True)\n        r = await result if asyncio.iscoroutine(result) else result\n\n        if self.auto_append_response:\n            self.idearium.extend(r)\n\n        return r\n\n    def _process_tool_calls(self, tool_calls: ToolCalls):\n        \"\"\"\n        Processes tool calls and returns the tool response.\n\n        Args:\n            tool_calls (ToolCalls): The tool calls to process.\n\n        Returns:\n            Optional[ToolCalls]: The tool response. If None, no tool calls were found.\n        \"\"\"\n        for i, tool_call in enumerate(tool_calls.list):\n            if not tool_call.id.startswith(\"call_\"):\n                # Something went wrong and this tool call is not valid\n                tool_calls.list.pop(i)\n                logger.error(\n                    \"Invalid tool call: \"\n                    + f\"{tool_call.model_dump_json(exclude_none=True)}\"\n                )\n\n        tc_dump = tool_calls.model_dump(exclude_none=True)\n        if tc_dump.get(\"list\"):\n            logger.debug(f\"Tool calls: {tc_dump}\")\n\n            # Create a new notion from the tool calls\n            tc_notion = Notion(\n                content=json.dumps(tc_dump.get(\"list\")),\n                role=str(ChatRole.TOOL_CALL.value),\n            )\n\n            # Add the tool call to the idearium\n            self.idearium.append(tc_notion)\n            # Call stream again with the tool response\n            tool_response = self._use_tools(tool_calls)\n            return tool_response\n        else:\n            logger.error(\"No tool calls found\")\n            return None\n\n    def stream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Streams a response to the given prompt by calling the\n        underlying model's stream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.stream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in stream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                for r in self.stream(tool_response):\n                    yield r\n\n    async def astream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Asynchronously streams a response to the given prompt by calling the\n        underlying model's astream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.astream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        async for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in astream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                async for r in self.astream(tool_response):\n                    yield r\n</code></pre> Attributes <code>auto_append_response: bool = True</code> <code>class-attribute</code> <code>instance-attribute</code> <p>Whether to automatically append the response to the idearium after generating a response.</p> <code>idearium: Idearium</code> <code>instance-attribute</code> <p>The Idearium used by the agent.</p> <code>model: Model</code> <code>property</code> <p>The model used by the agent.</p> <code>role: ChatRole</code> <code>property</code> <p>The ChatRole object for the model.</p> <code>tools: List[Tool]</code> <code>instance-attribute</code> <p>The tools used by the agent.</p> <p>WARNING: Do not modify this list directly. Use <code>add_tool</code>, <code>add_tools</code>, and <code>remove_tool</code> instead.</p> Functions <code>__init__(model, idearium=None, tools=None, auto_append_response=True)</code> <p>Initializes the agent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use.</p> required <code>idearium</code> <code>Idearium</code> <p>The idearium to use. If None, a new one will be created.</p> <code>None</code> <code>tools</code> <code>List[Tool]</code> <p>The tools to use.</p> <code>None</code> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def __init__(\n    self,\n    model: Model,\n    idearium: Optional[Idearium] = None,\n    tools: Optional[List[Tool]] = None,\n    auto_append_response: bool = True,\n):\n    \"\"\"\n    Initializes the agent.\n\n    Args:\n        model (Model): The model to use.\n        idearium (Idearium, optional): The idearium to use.\n            If None, a new one will be created.\n        tools (List[Tool], optional): The tools to use.\n    \"\"\"\n    super().__init__(\n        model=model,\n        idearium=idearium\n        or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n        tools=tools or [],\n        auto_append_response=auto_append_response,\n    )\n</code></pre> <code>add_tool(tool)</code> <p>Adds a tool to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tool(self, tool: Tool) -&gt; None:\n    \"\"\"\n    Adds a tool to the agent.\n    \"\"\"\n    self.tools.append(tool)\n    self._bind_tools()\n</code></pre> <code>add_tools(tools)</code> <p>Adds a list of tools to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tools(self, tools: List[Tool]) -&gt; None:\n    \"\"\"\n    Adds a list of tools to the agent.\n    \"\"\"\n    self.tools.extend(tools)\n    self._bind_tools()\n</code></pre> <code>agenerate(messages, **kwargs)</code> <code>async</code> <p>Asynchronously generates a response to the given messages by calling the underlying model's agenerate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Asynchronously generates a response to the given messages by calling the\n    underlying model's agenerate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = await self.model.agenerate(self.idearium, **kwargs)\n    result = self._process_generation(responses, True)\n    r = await result if asyncio.iscoroutine(result) else result\n\n    if self.auto_append_response:\n        self.idearium.extend(r)\n\n    return r\n</code></pre> <code>astream(messages, **kwargs)</code> <code>async</code> <p>Asynchronously streams a response to the given prompt by calling the underlying model's astream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def astream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Asynchronously streams a response to the given prompt by calling the\n    underlying model's astream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.astream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    async for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in astream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            async for r in self.astream(tool_response):\n                yield r\n</code></pre> <code>generate(messages, **kwargs)</code> <p>Generates a response to the given messages by calling the underlying model's generate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Generates a response to the given messages by calling the\n    underlying model's generate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = self.model.generate(self.idearium, **kwargs)\n    result = self._process_generation(responses)\n\n    if self.auto_append_response:\n        self.idearium.extend(result)\n\n    return result\n</code></pre> <code>remove_tool(name)</code> <p>Removes a tool from the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def remove_tool(self, name: str) -&gt; None:\n    \"\"\"\n    Removes a tool from the agent.\n    \"\"\"\n    for i, tool in enumerate(self.tools):\n        if tool.name == name:\n            self.tools.pop(i)\n            break\n    self._bind_tools()\n</code></pre> <code>stream(messages, **kwargs)</code> <p>Streams a response to the given prompt by calling the underlying model's stream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def stream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Streams a response to the given prompt by calling the\n    underlying model's stream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.stream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in stream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            for r in self.stream(tool_response):\n                yield r\n</code></pre> <code>Model</code> <p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract class for all Large Language Models.</p> <p>This class outlines a standardized lifecycle for interacting with LLMs, aimed at ensuring a consistent process for message trimming, pre-processing, preparing requests for the model, invoking the model, standardizing the response, and post-processing. The lifecycle is as follows:</p> <p>Lifecycle: 1. Pre-processing (_preprocess): Performs any necessary transformations or     adjustments to the messages prior to trimming or preparing them for model input.     (Optional)</p> <ol> <li> <p>Preparing Request (_format_request): Converts the pre-processed messages     into a format suitable for model input.</p> </li> <li> <p>Model Invocation (_call or _acall): Feeds the prepared input to the LLM and     retrieves the raw model output. There should be both synchronous and     asynchronous versions available.</p> </li> <li> <p>Standardizing Response (_standardize_response): Transforms the raw model     output into a consistent response format suitable for further processing or     delivery.</p> </li> <li> <p>Post-processing (_postprocess): Performs any final transformations or     adjustments to the standardized responses, making them ready for delivery.     (Optional)</p> </li> </ol> <p>Subclasses should implement each of the non-optional lifecycle steps in accordance with the specific requirements and behaviors of the target LLM.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>class Model(BaseModel, ABC):\n    \"\"\"\n    Abstract class for all Large Language Models.\n\n    This class outlines a standardized lifecycle for interacting with LLMs,\n    aimed at ensuring a consistent process for message trimming, pre-processing,\n    preparing requests for the model, invoking the model, standardizing the response,\n    and post-processing. The lifecycle is as follows:\n\n    Lifecycle:\n    1. Pre-processing (_preprocess): Performs any necessary transformations or\n        adjustments to the messages prior to trimming or preparing them for model input.\n        (Optional)\n\n    2. Preparing Request (_format_request): Converts the pre-processed messages\n        into a format suitable for model input.\n\n    3. Model Invocation (_call or _acall): Feeds the prepared input to the LLM and\n        retrieves the raw model output. There should be both synchronous and\n        asynchronous versions available.\n\n    4. Standardizing Response (_standardize_response): Transforms the raw model\n        output into a consistent response format suitable for further processing or\n        delivery.\n\n    5. Post-processing (_postprocess): Performs any final transformations or\n        adjustments to the standardized responses, making them ready for delivery.\n        (Optional)\n\n    Subclasses should implement each of the non-optional lifecycle steps in accordance\n    with the specific requirements and behaviors of the target LLM.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    max_response: int = Field(default=0)\n    api_key: str\n    name: str\n    #\n    role: Type[ChatRole]\n    type: ModelType\n    llm: Callable\n    llm_async: Callable\n    can_stream: bool\n    tokenizer: Tokenizer\n\n    @property\n    @abstractmethod\n    def max_tokens(self) -&gt; int:\n        \"\"\"\n        The maximum number of tokens that can be fed to the model at once.\n        \"\"\"\n        pass\n\n    def _process_input(self, messages: Messages) -&gt; Idearium:\n        if isinstance(messages, str):\n            notions = [Notion(content=messages, role=self.role.HUMAN)]\n        elif isinstance(messages, Notion):\n            notions = [messages]\n        elif isinstance(messages, Idearium):\n            return messages  # Already an Idearium, no need to convert\n        elif isinstance(messages, list):\n            notions = [\n                (\n                    Notion(content=msg, role=self.role.HUMAN)\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        else:\n            raise ValueError(\"Invalid input type for messages\")\n\n        return Idearium(self.tokenizer, self.max_tokens, notions)\n\n    def _convert_role(self, role: ChatRole) -&gt; str:\n        \"\"\"\n        Converts the standard ChatRole to the model-specific role.\n        \"\"\"\n        return str(self.role[role.name].value)\n\n    def _preprocess(self, messages: List[Notion]) -&gt; List[Notion]:\n        \"\"\"\n        Preprocesses the List of `Notions`, applying any effects necessary\n        before being prepped for input into an API.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        return [\n            Notion(msg.content, self._convert_role(msg.chat_role), msg.persistent)\n            for msg in messages\n        ]\n\n    @abstractmethod\n    def _format_request(\n        self, messages: List[Notion], *args, **kwargs\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Formats the List of `Notions` into a format suitable for model input.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _standardize_response(\n        self, response: Union[object, str, List[any]], *args, **kwargs\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Standardizes the raw response from the model into a List of Notions.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _postprocess(self, response: List[Notion], *args, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Postprocesses the response from the model, applying any final effects\n        before being returned.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _retry_call(\n        self,\n        input: Union[str, object, List[any]],\n        e: Exception,\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Retry logic for API calls used by `_common_call_logic`.\n        \"\"\"\n        pass\n\n    def _common_call_logic(\n        self,\n        input: Union[str, object, List[any]],\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        if input is None:\n            raise ValueError(\"No input provided.\")\n\n        try:\n            out = api_call(messages=input)\n            return out\n        except Exception as e:\n            logger.error(f\"Error calling LLM API: {e}\")\n            if retries &gt;= 3:\n                raise e\n\n            return self._retry_call(input, e, api_call, retries=retries)\n\n    def _call(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the raw response.\n\n        Should behave exactly as `_acall` does, but synchronously.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n\n        def api_call(**kwargs_):\n            return self.llm(**kwargs_, **kwargs)\n\n        return self._common_call_logic(input, api_call, retries)\n\n    async def _acall(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the\n        raw response asynchronously.\n\n        Should behave exactly as `_call` does, but asynchronously.\n\n        This is a lifecycle method that is called by the `agenerate` method.\n        \"\"\"\n\n        async def api_call(**kwargs_):\n            return await self.llm_async(**kwargs_, **kwargs)\n\n        result = self._common_call_logic(input, api_call, retries)\n        if asyncio.iscoroutine(result):\n            return await result\n        return result\n\n    def _common_generate_logic(\n        self,\n        messages: Messages,\n        is_async=False,\n        **kwargs,\n    ):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        call_method = self._acall if is_async else self._call\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n\n        if is_async:\n\n            async def call():\n                response = await call_method(input, **kwargs)\n                output = self._standardize_response(response)\n                return self._postprocess(output)\n\n            return call()\n        else:\n            response = call_method(input, **kwargs)\n            output = self._standardize_response(response)\n            return self._postprocess(output)\n\n    @abstractmethod\n    def generate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def agenerate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response\n        asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating async responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    def _common_stream_logic(self, messages: Messages):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        if not self.can_stream:\n            raise ValueError(\n                \"This model does not support streaming. \"\n                + \"Please use the `generate` method instead.\"\n            )\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n        return input\n\n    @abstractmethod\n    def stream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def astream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time, asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n</code></pre> Attributes <code>max_tokens: int</code> <code>abstractmethod</code> <code>property</code> <p>The maximum number of tokens that can be fed to the model at once.</p> Functions <code>agenerate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Calls the model with the given messages and returns the response asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating async responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response\n    asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating async responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>astream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Streams the model with the given messages and returns the response, one token at a time, asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def astream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time, asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre> <code>generate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Calls the model with the given messages and returns the response.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>stream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Streams the model with the given messages and returns the response, one token at a time.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef stream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#SilverLingua.core.templates-modules","title":"Modules","text":"<code>agent</code> Attributes Classes <code>Agent</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper around a model that utilizes an Idearium and a set of Tools.</p> <p>This is a base class not meant to be used directly. It is meant to be subclassed by specific model implementations.</p> <p>However, there is limited boilerplate. The only thing that needs to be redefined in subclasses is the <code>_bind_tools</code> method.</p> <p>Additionally, the <code>_use_tools</code> method is a common method to redefine.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>class Agent(BaseModel):\n    \"\"\"\n    A wrapper around a model that utilizes an Idearium and a set of Tools.\n\n    This is a base class not meant to be used directly. It is meant to be\n    subclassed by specific model implementations.\n\n    However, there is limited boilerplate. The only thing that needs to be\n    redefined in subclasses is the `_bind_tools` method.\n\n    Additionally, the `_use_tools` method is a common method to redefine.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    model: Model\n    idearium: Idearium\n    \"\"\"\n    The Idearium used by the agent.\n    \"\"\"\n    tools: List[Tool]\n    \"\"\"\n    The tools used by the agent.\n\n    WARNING: Do not modify this list directly. Use `add_tool`, `add_tools`,\n    and `remove_tool` instead.\n    \"\"\"\n    auto_append_response: bool = True\n    \"\"\"\n    Whether to automatically append the response to the idearium after\n    generating a response.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        idearium: Optional[Idearium] = None,\n        tools: Optional[List[Tool]] = None,\n        auto_append_response: bool = True,\n    ):\n        \"\"\"\n        Initializes the agent.\n\n        Args:\n            model (Model): The model to use.\n            idearium (Idearium, optional): The idearium to use.\n                If None, a new one will be created.\n            tools (List[Tool], optional): The tools to use.\n        \"\"\"\n        super().__init__(\n            model=model,\n            idearium=idearium\n            or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n            tools=tools or [],\n            auto_append_response=auto_append_response,\n        )\n\n    def model_post_init(self, __content):\n        self._bind_tools()\n\n    @property\n    def model(self) -&gt; Model:\n        \"\"\"\n        The model used by the agent.\n        \"\"\"\n        return self.model\n\n    @property\n    def role(self) -&gt; ChatRole:\n        \"\"\"\n        The ChatRole object for the model.\n        \"\"\"\n        return self.model.role\n\n    def _find_tool(self, name: str) -&gt; Tool | None:\n        \"\"\"\n        Finds a tool by name.\n        \"\"\"\n        for t in self.tools:\n            if t.name == name:\n                return t\n        return None\n\n    def _use_tools(self, tool_calls: ToolCalls) -&gt; List[Notion]:\n        \"\"\"\n        Uses Tools based on the given ToolCalls, returning Notions\n        containing ToolCallResponses.\n\n        Args:\n            tool_calls (ToolCalls): The ToolCalls to use.\n\n        Returns:\n            List[Notion]: The Notions containing ToolCallResponses.\n                Each Notion will have a role of ChatRole.TOOL_RESPONSE.\n        \"\"\"\n        responses: List[Notion] = []\n        for tool_call in tool_calls.list:\n            tool = self._find_tool(tool_call.function.name)\n            if tool is not None:\n                tc_function_response = {}\n                with contextlib.suppress(json.JSONDecodeError):\n                    tc_function_response = json.loads(tool_call.function.arguments)\n\n                tc_response = ToolCallResponse.from_tool_call(\n                    tool_call=tool_call, response=tool(**tc_function_response)\n                )\n                responses.append(\n                    Notion(\n                        content=tc_response.model_dump_json(exclude_none=True),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n            else:\n                responses.append(\n                    Notion(\n                        content=json.dumps(\n                            {\n                                \"tool_call_id\": tool_call.id,\n                                \"content\": \"Tool not found\",\n                                \"name\": \"error\",\n                            }\n                        ),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n        return responses\n\n    def _bind_tools(self) -&gt; None:\n        \"\"\"\n        Called at the end of __init__ to bind the tools to the model.\n\n        This MUST be redefined in subclasses to dictate how\n        the tools are bound to the model.\n\n        Example:\n        ```python\n        # From OpenAIChatAgent\n        def _bind_tools(self) -&gt; None:\n            m_tools: List[ChatCompletionToolParam] = [\n                {\"type\": \"function\", \"function\": tool.description}\n                for tool in self.tools\n            ]\n\n            if len(m_tools) &gt; 0:\n                self.model.tools = m_tools\n        ```\n        \"\"\"\n        pass\n\n    def add_tool(self, tool: Tool) -&gt; None:\n        \"\"\"\n        Adds a tool to the agent.\n        \"\"\"\n        self.tools.append(tool)\n        self._bind_tools()\n\n    def add_tools(self, tools: List[Tool]) -&gt; None:\n        \"\"\"\n        Adds a list of tools to the agent.\n        \"\"\"\n        self.tools.extend(tools)\n        self._bind_tools()\n\n    def remove_tool(self, name: str) -&gt; None:\n        \"\"\"\n        Removes a tool from the agent.\n        \"\"\"\n        for i, tool in enumerate(self.tools):\n            if tool.name == name:\n                self.tools.pop(i)\n                break\n        self._bind_tools()\n\n    def _process_messages(self, messages: Messages) -&gt; List[Notion]:\n        \"\"\"Convert various message types into a list of Notions.\"\"\"\n        if isinstance(messages, str):\n            return [Notion(content=messages, role=str(self.role.HUMAN.value))]\n        elif isinstance(messages, Notion):\n            return [messages]\n        elif isinstance(messages, Idearium):\n            return messages.notions\n        elif isinstance(messages, list):\n            return [\n                (\n                    Notion(content=msg, role=str(self.role.HUMAN.value))\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        raise ValueError(f\"Unsupported message type: {type(messages)}\")\n\n    def _process_generation(\n        self, responses: List[Notion], is_async=False\n    ) -&gt; List[Notion]:\n        \"\"\"Wrapper around shared logic between generate and agenerate.\"\"\"\n        response = responses[0]\n        # logger.debug(f\"Response: {response}\")\n        if response.chat_role == ChatRole.TOOL_CALL:\n            # logger.debug(\"Tool call detected\")\n            # Add the tool call to the idearium\n            self.idearium.append(response)\n            # Call generate again with the tool response\n            tool_calls = ToolCalls.model_validate_json(\n                '{\"list\": ' + response.content + \"}\"\n            )\n            tool_response = self._use_tools(tool_calls)\n            # logger.debug(f\"Tool response: {tool_response}\")\n            if is_async:\n                return self.agenerate(tool_response)\n            else:\n                return self.generate(tool_response)\n        else:\n            return responses\n\n    def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Generates a response to the given messages by calling the\n        underlying model's generate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = self.model.generate(self.idearium, **kwargs)\n        result = self._process_generation(responses)\n\n        if self.auto_append_response:\n            self.idearium.extend(result)\n\n        return result\n\n    async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Asynchronously generates a response to the given messages by calling the\n        underlying model's agenerate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = await self.model.agenerate(self.idearium, **kwargs)\n        result = self._process_generation(responses, True)\n        r = await result if asyncio.iscoroutine(result) else result\n\n        if self.auto_append_response:\n            self.idearium.extend(r)\n\n        return r\n\n    def _process_tool_calls(self, tool_calls: ToolCalls):\n        \"\"\"\n        Processes tool calls and returns the tool response.\n\n        Args:\n            tool_calls (ToolCalls): The tool calls to process.\n\n        Returns:\n            Optional[ToolCalls]: The tool response. If None, no tool calls were found.\n        \"\"\"\n        for i, tool_call in enumerate(tool_calls.list):\n            if not tool_call.id.startswith(\"call_\"):\n                # Something went wrong and this tool call is not valid\n                tool_calls.list.pop(i)\n                logger.error(\n                    \"Invalid tool call: \"\n                    + f\"{tool_call.model_dump_json(exclude_none=True)}\"\n                )\n\n        tc_dump = tool_calls.model_dump(exclude_none=True)\n        if tc_dump.get(\"list\"):\n            logger.debug(f\"Tool calls: {tc_dump}\")\n\n            # Create a new notion from the tool calls\n            tc_notion = Notion(\n                content=json.dumps(tc_dump.get(\"list\")),\n                role=str(ChatRole.TOOL_CALL.value),\n            )\n\n            # Add the tool call to the idearium\n            self.idearium.append(tc_notion)\n            # Call stream again with the tool response\n            tool_response = self._use_tools(tool_calls)\n            return tool_response\n        else:\n            logger.error(\"No tool calls found\")\n            return None\n\n    def stream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Streams a response to the given prompt by calling the\n        underlying model's stream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.stream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in stream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                for r in self.stream(tool_response):\n                    yield r\n\n    async def astream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Asynchronously streams a response to the given prompt by calling the\n        underlying model's astream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.astream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        async for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in astream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                async for r in self.astream(tool_response):\n                    yield r\n</code></pre> Attributes <code>auto_append_response: bool = True</code> <code>class-attribute</code> <code>instance-attribute</code> <p>Whether to automatically append the response to the idearium after generating a response.</p> <code>idearium: Idearium</code> <code>instance-attribute</code> <p>The Idearium used by the agent.</p> <code>model: Model</code> <code>property</code> <p>The model used by the agent.</p> <code>role: ChatRole</code> <code>property</code> <p>The ChatRole object for the model.</p> <code>tools: List[Tool]</code> <code>instance-attribute</code> <p>The tools used by the agent.</p> <p>WARNING: Do not modify this list directly. Use <code>add_tool</code>, <code>add_tools</code>, and <code>remove_tool</code> instead.</p> Functions <code>__init__(model, idearium=None, tools=None, auto_append_response=True)</code> <p>Initializes the agent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use.</p> required <code>idearium</code> <code>Idearium</code> <p>The idearium to use. If None, a new one will be created.</p> <code>None</code> <code>tools</code> <code>List[Tool]</code> <p>The tools to use.</p> <code>None</code> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def __init__(\n    self,\n    model: Model,\n    idearium: Optional[Idearium] = None,\n    tools: Optional[List[Tool]] = None,\n    auto_append_response: bool = True,\n):\n    \"\"\"\n    Initializes the agent.\n\n    Args:\n        model (Model): The model to use.\n        idearium (Idearium, optional): The idearium to use.\n            If None, a new one will be created.\n        tools (List[Tool], optional): The tools to use.\n    \"\"\"\n    super().__init__(\n        model=model,\n        idearium=idearium\n        or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n        tools=tools or [],\n        auto_append_response=auto_append_response,\n    )\n</code></pre> <code>add_tool(tool)</code> <p>Adds a tool to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tool(self, tool: Tool) -&gt; None:\n    \"\"\"\n    Adds a tool to the agent.\n    \"\"\"\n    self.tools.append(tool)\n    self._bind_tools()\n</code></pre> <code>add_tools(tools)</code> <p>Adds a list of tools to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tools(self, tools: List[Tool]) -&gt; None:\n    \"\"\"\n    Adds a list of tools to the agent.\n    \"\"\"\n    self.tools.extend(tools)\n    self._bind_tools()\n</code></pre> <code>agenerate(messages, **kwargs)</code> <code>async</code> <p>Asynchronously generates a response to the given messages by calling the underlying model's agenerate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Asynchronously generates a response to the given messages by calling the\n    underlying model's agenerate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = await self.model.agenerate(self.idearium, **kwargs)\n    result = self._process_generation(responses, True)\n    r = await result if asyncio.iscoroutine(result) else result\n\n    if self.auto_append_response:\n        self.idearium.extend(r)\n\n    return r\n</code></pre> <code>astream(messages, **kwargs)</code> <code>async</code> <p>Asynchronously streams a response to the given prompt by calling the underlying model's astream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def astream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Asynchronously streams a response to the given prompt by calling the\n    underlying model's astream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.astream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    async for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in astream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            async for r in self.astream(tool_response):\n                yield r\n</code></pre> <code>generate(messages, **kwargs)</code> <p>Generates a response to the given messages by calling the underlying model's generate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Generates a response to the given messages by calling the\n    underlying model's generate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = self.model.generate(self.idearium, **kwargs)\n    result = self._process_generation(responses)\n\n    if self.auto_append_response:\n        self.idearium.extend(result)\n\n    return result\n</code></pre> <code>remove_tool(name)</code> <p>Removes a tool from the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def remove_tool(self, name: str) -&gt; None:\n    \"\"\"\n    Removes a tool from the agent.\n    \"\"\"\n    for i, tool in enumerate(self.tools):\n        if tool.name == name:\n            self.tools.pop(i)\n            break\n    self._bind_tools()\n</code></pre> <code>stream(messages, **kwargs)</code> <p>Streams a response to the given prompt by calling the underlying model's stream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def stream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Streams a response to the given prompt by calling the\n    underlying model's stream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.stream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in stream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            for r in self.stream(tool_response):\n                yield r\n</code></pre> <code>model</code> Attributes <code>Messages = Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> <code>module-attribute</code> <p>A type alias for the various types of messages that can be passed to a model.</p> Classes <code>Model</code> <p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract class for all Large Language Models.</p> <p>This class outlines a standardized lifecycle for interacting with LLMs, aimed at ensuring a consistent process for message trimming, pre-processing, preparing requests for the model, invoking the model, standardizing the response, and post-processing. The lifecycle is as follows:</p> <p>Lifecycle: 1. Pre-processing (_preprocess): Performs any necessary transformations or     adjustments to the messages prior to trimming or preparing them for model input.     (Optional)</p> <ol> <li> <p>Preparing Request (_format_request): Converts the pre-processed messages     into a format suitable for model input.</p> </li> <li> <p>Model Invocation (_call or _acall): Feeds the prepared input to the LLM and     retrieves the raw model output. There should be both synchronous and     asynchronous versions available.</p> </li> <li> <p>Standardizing Response (_standardize_response): Transforms the raw model     output into a consistent response format suitable for further processing or     delivery.</p> </li> <li> <p>Post-processing (_postprocess): Performs any final transformations or     adjustments to the standardized responses, making them ready for delivery.     (Optional)</p> </li> </ol> <p>Subclasses should implement each of the non-optional lifecycle steps in accordance with the specific requirements and behaviors of the target LLM.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>class Model(BaseModel, ABC):\n    \"\"\"\n    Abstract class for all Large Language Models.\n\n    This class outlines a standardized lifecycle for interacting with LLMs,\n    aimed at ensuring a consistent process for message trimming, pre-processing,\n    preparing requests for the model, invoking the model, standardizing the response,\n    and post-processing. The lifecycle is as follows:\n\n    Lifecycle:\n    1. Pre-processing (_preprocess): Performs any necessary transformations or\n        adjustments to the messages prior to trimming or preparing them for model input.\n        (Optional)\n\n    2. Preparing Request (_format_request): Converts the pre-processed messages\n        into a format suitable for model input.\n\n    3. Model Invocation (_call or _acall): Feeds the prepared input to the LLM and\n        retrieves the raw model output. There should be both synchronous and\n        asynchronous versions available.\n\n    4. Standardizing Response (_standardize_response): Transforms the raw model\n        output into a consistent response format suitable for further processing or\n        delivery.\n\n    5. Post-processing (_postprocess): Performs any final transformations or\n        adjustments to the standardized responses, making them ready for delivery.\n        (Optional)\n\n    Subclasses should implement each of the non-optional lifecycle steps in accordance\n    with the specific requirements and behaviors of the target LLM.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    max_response: int = Field(default=0)\n    api_key: str\n    name: str\n    #\n    role: Type[ChatRole]\n    type: ModelType\n    llm: Callable\n    llm_async: Callable\n    can_stream: bool\n    tokenizer: Tokenizer\n\n    @property\n    @abstractmethod\n    def max_tokens(self) -&gt; int:\n        \"\"\"\n        The maximum number of tokens that can be fed to the model at once.\n        \"\"\"\n        pass\n\n    def _process_input(self, messages: Messages) -&gt; Idearium:\n        if isinstance(messages, str):\n            notions = [Notion(content=messages, role=self.role.HUMAN)]\n        elif isinstance(messages, Notion):\n            notions = [messages]\n        elif isinstance(messages, Idearium):\n            return messages  # Already an Idearium, no need to convert\n        elif isinstance(messages, list):\n            notions = [\n                (\n                    Notion(content=msg, role=self.role.HUMAN)\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        else:\n            raise ValueError(\"Invalid input type for messages\")\n\n        return Idearium(self.tokenizer, self.max_tokens, notions)\n\n    def _convert_role(self, role: ChatRole) -&gt; str:\n        \"\"\"\n        Converts the standard ChatRole to the model-specific role.\n        \"\"\"\n        return str(self.role[role.name].value)\n\n    def _preprocess(self, messages: List[Notion]) -&gt; List[Notion]:\n        \"\"\"\n        Preprocesses the List of `Notions`, applying any effects necessary\n        before being prepped for input into an API.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        return [\n            Notion(msg.content, self._convert_role(msg.chat_role), msg.persistent)\n            for msg in messages\n        ]\n\n    @abstractmethod\n    def _format_request(\n        self, messages: List[Notion], *args, **kwargs\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Formats the List of `Notions` into a format suitable for model input.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _standardize_response(\n        self, response: Union[object, str, List[any]], *args, **kwargs\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Standardizes the raw response from the model into a List of Notions.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _postprocess(self, response: List[Notion], *args, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Postprocesses the response from the model, applying any final effects\n        before being returned.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _retry_call(\n        self,\n        input: Union[str, object, List[any]],\n        e: Exception,\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Retry logic for API calls used by `_common_call_logic`.\n        \"\"\"\n        pass\n\n    def _common_call_logic(\n        self,\n        input: Union[str, object, List[any]],\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        if input is None:\n            raise ValueError(\"No input provided.\")\n\n        try:\n            out = api_call(messages=input)\n            return out\n        except Exception as e:\n            logger.error(f\"Error calling LLM API: {e}\")\n            if retries &gt;= 3:\n                raise e\n\n            return self._retry_call(input, e, api_call, retries=retries)\n\n    def _call(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the raw response.\n\n        Should behave exactly as `_acall` does, but synchronously.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n\n        def api_call(**kwargs_):\n            return self.llm(**kwargs_, **kwargs)\n\n        return self._common_call_logic(input, api_call, retries)\n\n    async def _acall(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the\n        raw response asynchronously.\n\n        Should behave exactly as `_call` does, but asynchronously.\n\n        This is a lifecycle method that is called by the `agenerate` method.\n        \"\"\"\n\n        async def api_call(**kwargs_):\n            return await self.llm_async(**kwargs_, **kwargs)\n\n        result = self._common_call_logic(input, api_call, retries)\n        if asyncio.iscoroutine(result):\n            return await result\n        return result\n\n    def _common_generate_logic(\n        self,\n        messages: Messages,\n        is_async=False,\n        **kwargs,\n    ):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        call_method = self._acall if is_async else self._call\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n\n        if is_async:\n\n            async def call():\n                response = await call_method(input, **kwargs)\n                output = self._standardize_response(response)\n                return self._postprocess(output)\n\n            return call()\n        else:\n            response = call_method(input, **kwargs)\n            output = self._standardize_response(response)\n            return self._postprocess(output)\n\n    @abstractmethod\n    def generate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def agenerate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response\n        asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating async responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    def _common_stream_logic(self, messages: Messages):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        if not self.can_stream:\n            raise ValueError(\n                \"This model does not support streaming. \"\n                + \"Please use the `generate` method instead.\"\n            )\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n        return input\n\n    @abstractmethod\n    def stream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def astream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time, asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n</code></pre> Attributes <code>max_tokens: int</code> <code>abstractmethod</code> <code>property</code> <p>The maximum number of tokens that can be fed to the model at once.</p> Functions <code>agenerate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Calls the model with the given messages and returns the response asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating async responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response\n    asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating async responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>astream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Streams the model with the given messages and returns the response, one token at a time, asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def astream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time, asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre> <code>generate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Calls the model with the given messages and returns the response.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>stream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Streams the model with the given messages and returns the response, one token at a time.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef stream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#SilverLingua.openai","title":"<code>openai</code>","text":"<p>The OpenAI module provides implementations of SilverLingua's core components using the OpenAI API.</p> <p>This module includes: - OpenAIChatAgent: An agent that uses OpenAI's chat completion API - OpenAIModel: A model that uses OpenAI's API - OpenAIChatRole: Role definitions for OpenAI's chat format</p>"},{"location":"api/#SilverLingua.openai-classes","title":"Classes","text":""},{"location":"api/#SilverLingua.search_tools","title":"<code>search_tools</code>","text":""},{"location":"api/#SilverLingua.search_tools-classes","title":"Classes","text":""},{"location":"api/#SilverLingua.search_tools-modules","title":"Modules","text":""},{"location":"api/#SilverLingua.search_tools.google_search","title":"<code>google_search</code>","text":""},{"location":"api/#SilverLingua.search_tools.google_search-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.search_tools.google_search-modules","title":"Modules","text":"<code>serper</code> Classes Functions <code>google_search(query, type='search')</code> <p>Searches Google for the given query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to search for.</p> required <code>type</code> <code>str</code> <p>Can be one of \"search\", \"images\", \"maps\", \"news\", \"shopping\", \"scholar\". Defaults to \"search\".</p> <code>'search'</code> Source code in <code>src\\SilverLingua\\search_tools\\google_search\\serper.py</code> <pre><code>@Tool\ndef google_search(query: str, type: str = \"search\"):\n    \"\"\"\n    Searches Google for the given query.\n\n    Args:\n        query: The query to search for.\n        type: Can be one of \"search\", \"images\", \"maps\", \"news\", \"shopping\", \"scholar\". Defaults to \"search\".\n    \"\"\"\n    payload = {\n        \"q\": query,\n        \"location\": \"Dalles, Texas, United States\",\n    }\n\n    headers = {\n        \"X-API-KEY\": os.getenv(\"SERPER_API_KEY\"),\n        \"Content-Type\": \"application/json\",\n    }\n\n    response = requests.post(url + type, headers=headers, data=json.dumps(payload))\n\n    return response.text\n</code></pre>"},{"location":"api/#SilverLingua.tool","title":"<code>tool</code>","text":""},{"location":"api/#SilverLingua.tool-classes","title":"Classes","text":""},{"location":"api/#SilverLingua.tool.FunctionJSONSchema","title":"<code>FunctionJSONSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A function according to JSON schema standards.</p> <p>This is also passed in to OpenAI ChatCompletion API functions list so the AI understands how to call a function.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function</p> <code>description</code> <code>str</code> <p>A description of the function</p> <code>parameters</code> <code>Parameters</code> <p>A dictionary of parameters and their types (Optional)</p> <p>Example:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class FunctionJSONSchema(BaseModel):\n    \"\"\"\n    A function according to JSON schema standards.\n\n    This is also passed in to OpenAI ChatCompletion API\n    functions list so the AI understands how to call a function.\n\n    Attributes:\n        name: The name of the function\n        description: A description of the function\n        parameters: A dictionary of parameters and their types (Optional)\n\n    Example:\n\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: Parameters\n</code></pre>"},{"location":"api/#SilverLingua.tool.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre>"},{"location":"api/#SilverLingua.tool.Tool-functions","title":"Functions","text":"<code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre>"},{"location":"api/#SilverLingua.tool.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre>"},{"location":"api/#SilverLingua.tool.ToolCall-functions","title":"Functions","text":"<code>concat(other)</code> <p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre>"},{"location":"api/#SilverLingua.tool.ToolCallResponse","title":"<code>ToolCallResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre>"},{"location":"api/#SilverLingua.tool.ToolCalls","title":"<code>ToolCalls</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre>"},{"location":"api/#SilverLingua.tool.ToolCalls-functions","title":"Functions","text":"<code>concat(other)</code> <p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre>"},{"location":"api/#SilverLingua.tool-modules","title":"Modules","text":""},{"location":"api/#SilverLingua.tool.tool","title":"<code>tool</code>","text":""},{"location":"api/#SilverLingua.tool.tool-classes","title":"Classes","text":"<code>Tool</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre> Functions <code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre>"},{"location":"api/#SilverLingua.tool.tool-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.util","title":"<code>util</code>","text":""},{"location":"api/#SilverLingua.util-functions","title":"Functions","text":""},{"location":"api/#SilverLingua.util.timeit","title":"<code>timeit(func)</code>","text":"<p>Decorator to time the execution of a function and log the time taken.</p> Usage <p>@timeit def my_function():     pass</p> <p>The time taken for 'my_function' will be logged.</p> Source code in <code>src\\SilverLingua\\util.py</code> <pre><code>def timeit(func):\n    \"\"\"\n    Decorator to time the execution of a function and log the time taken.\n\n    Usage:\n        @timeit\n        def my_function():\n            pass\n\n    The time taken for 'my_function' will be logged.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # Dynamically grab the logger based on the module where `func` is defined\n        logger = logging.getLogger(func.__module__)\n\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n\n        logger.debug(\n            f\"[{func.__name__}] finished. Time taken: {end - start:.4f} seconds\"\n        )\n\n        return result\n\n    return wrapper\n</code></pre>"},{"location":"api/core/","title":"Core API","text":""},{"location":"api/core/#core-components","title":"Core Components","text":"<p>options: members: true showroot_heading: true show_source: true show_submodules: true filters: [\"!^\"] docstring_style: google docstring_section_style: list show_if_no_docstring: true show_signature_annotations: true separate_signature: true unwrap_annotated: true merge_init_into_class: true</p>"},{"location":"api/core/#SilverLingua.core","title":"<code>SilverLingua.core</code>","text":"<p>SilverLingua Core</p>"},{"location":"api/core/#SilverLingua.core-modules","title":"Modules","text":""},{"location":"api/core/#SilverLingua.core.atoms","title":"<code>atoms</code>","text":""},{"location":"api/core/#SilverLingua.core.atoms-classes","title":"Classes","text":""},{"location":"api/core/#SilverLingua.core.atoms.FunctionJSONSchema","title":"<code>FunctionJSONSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A function according to JSON schema standards.</p> <p>This is also passed in to OpenAI ChatCompletion API functions list so the AI understands how to call a function.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function</p> <code>description</code> <code>str</code> <p>A description of the function</p> <code>parameters</code> <code>Parameters</code> <p>A dictionary of parameters and their types (Optional)</p> <p>Example:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class FunctionJSONSchema(BaseModel):\n    \"\"\"\n    A function according to JSON schema standards.\n\n    This is also passed in to OpenAI ChatCompletion API\n    functions list so the AI understands how to call a function.\n\n    Attributes:\n        name: The name of the function\n        description: A description of the function\n        parameters: A dictionary of parameters and their types (Optional)\n\n    Example:\n\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: Parameters\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.Memory","title":"<code>Memory</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A Memory is the smallest unit of storage information, and is the base class for all other storage information.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\memory.py</code> <pre><code>class Memory(BaseModel):\n    \"\"\"\n    A Memory is the smallest unit of storage information, and\n    is the base class for all other storage information.\n    \"\"\"\n\n    content: str\n\n    def __str__(self) -&gt; str:\n        return self.content\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A tokenizer that can encode and decode strings.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tokenizer.py</code> <pre><code>class Tokenizer(BaseModel):\n    \"\"\"\n    A tokenizer that can encode and decode strings.\n    \"\"\"\n\n    encode: Callable[[str], List[int]]\n    decode: Callable[[List[int]], str]\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.Tool-functions","title":"Functions","text":"<code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.ToolCall","title":"<code>ToolCall</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.ToolCall-functions","title":"Functions","text":"<code>concat(other)</code> <p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.ToolCallResponse","title":"<code>ToolCallResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.ToolCalls","title":"<code>ToolCalls</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.ToolCalls-functions","title":"Functions","text":"<code>concat(other)</code> <p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms-functions","title":"Functions","text":""},{"location":"api/core/#SilverLingua.core.atoms.create_chat_role","title":"<code>create_chat_role(name, SYSTEM, HUMAN, AI, TOOL_CALL, TOOL_RESPONSE)</code>","text":"<p>Create a new ChatRole enum with only the values of the RoleMembers changed.</p> <p>This will ensure that the parent of each member is ChatRole, which means that the members will be equal to the members of ChatRole.</p> Example <pre><code>OpenAIChatRole = create_chat_role(\n    \"OpenAIChatRole\",\n    SYSTEM=\"system\",\n    HUMAN=\"user\",\n    AI=\"assistant\",\n    TOOL_CALL=\"function_call\",\n    TOOL_RESPONSE=\"function\",\n)\n\nassert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\role\\chat.py</code> <pre><code>def create_chat_role(\n    name: str, SYSTEM: str, HUMAN: str, AI: str, TOOL_CALL: str, TOOL_RESPONSE: str\n) -&gt; Type[ChatRole]:\n    \"\"\"\n    Create a new ChatRole enum with only the values of the RoleMembers changed.\n\n    This will ensure that the parent of each member is ChatRole, which means\n    that the members will be equal to the members of ChatRole.\n\n    Example:\n        ```python\n        OpenAIChatRole = create_chat_role(\n            \"OpenAIChatRole\",\n            SYSTEM=\"system\",\n            HUMAN=\"user\",\n            AI=\"assistant\",\n            TOOL_CALL=\"function_call\",\n            TOOL_RESPONSE=\"function\",\n        )\n\n        assert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n        ```\n    \"\"\"\n    return Enum(\n        name,\n        {\n            \"SYSTEM\": RoleMember(\"SYSTEM\", SYSTEM, ChatRole),\n            \"HUMAN\": RoleMember(\"HUMAN\", HUMAN, ChatRole),\n            \"AI\": RoleMember(\"AI\", AI, ChatRole),\n            \"TOOL_CALL\": RoleMember(\"TOOL_CALL\", TOOL_CALL, ChatRole),\n            \"TOOL_RESPONSE\": RoleMember(\"TOOL_RESPONSE\", TOOL_RESPONSE, ChatRole),\n        },\n    )\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms-modules","title":"Modules","text":""},{"location":"api/core/#SilverLingua.core.atoms.memory","title":"<code>memory</code>","text":""},{"location":"api/core/#SilverLingua.core.atoms.memory-classes","title":"Classes","text":"<code>Memory</code> <p>               Bases: <code>BaseModel</code></p> <p>A Memory is the smallest unit of storage information, and is the base class for all other storage information.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\memory.py</code> <pre><code>class Memory(BaseModel):\n    \"\"\"\n    A Memory is the smallest unit of storage information, and\n    is the base class for all other storage information.\n    \"\"\"\n\n    content: str\n\n    def __str__(self) -&gt; str:\n        return self.content\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.prompt","title":"<code>prompt</code>","text":""},{"location":"api/core/#SilverLingua.core.atoms.prompt-functions","title":"Functions","text":"<code>RolePrompt(role, text)</code> <p>{{ role }}: {{ text }}</p> Source code in <code>src\\SilverLingua\\core\\atoms\\prompt.py</code> <pre><code>@prompt\ndef RolePrompt(role: str, text: str):  # type: ignore\n    \"\"\"{{ role }}: {{ text }}\"\"\"\n</code></pre> <code>prompt(func)</code> <p>A decorator to render a function's docstring as a Jinja2 template. Uses the function arguments as variables for the template.</p> <p>Note: Be deliberate about new lines in your docstrings - they may make meaningful changes in an AI's output. For instance, separating long sentences with a newline for human readability may cause issues.</p> <p>Example Usage:</p> <pre><code>@prompt\ndef fruit_prompt(fruits: list) -&gt; None:\n    \"\"\"\n    You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\n    LIST OF FRUIT:\n    {% for fruit in fruits %}{{ fruit }}\n    {% endfor %}\n    \"\"\"\n\nprint(fruit_prompt([\"apple\", \"orange\"]))\n</code></pre> <p>Expected Output:</p> <pre><code>You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\nLIST OF FRUIT:\napple\norange\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\prompt.py</code> <pre><code>def prompt(func: Callable) -&gt; Callable[..., str]:\n    \"\"\"\n    A decorator to render a function's docstring as a Jinja2 template.\n    Uses the function arguments as variables for the template.\n\n    Note: Be deliberate about new lines in your docstrings - they\n    may make meaningful changes in an AI's output. For instance,\n    separating long sentences with a newline for human\n    readability may cause issues.\n\n    Example Usage:\n    ```python\n    @prompt\n    def fruit_prompt(fruits: list) -&gt; None:\n        \\\"\"\"\n        You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\n        LIST OF FRUIT:\n        {% for fruit in fruits %}{{ fruit }}\n        {% endfor %}\n        \\\"\"\"\n\n    print(fruit_prompt([\"apple\", \"orange\"]))\n    ```\n\n    Expected Output:\n    ```\n    You are a helpful assistant that takes a list of fruit and gives information about their nutrition.\n\n    LIST OF FRUIT:\n    apple\n    orange\n    ```\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs) -&gt; str:\n        docstring = func.__doc__ or \"\"\n        template = Template(docstring, undefined=StrictUndefined)\n\n        # Get function signature and bind arguments\n        sig = signature(func)\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n\n        # Render the template with bound arguments\n        rendered = template.render(**bound_args.arguments)\n\n        # Strip each line and remove leading/trailing whitespaces\n        stripped_lines = [line.lstrip() for line in rendered.splitlines()]\n        return \"\\n\".join(stripped_lines).strip()\n\n    return wrapper\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.role","title":"<code>role</code>","text":""},{"location":"api/core/#SilverLingua.core.atoms.role-functions","title":"Functions","text":"<code>create_chat_role(name, SYSTEM, HUMAN, AI, TOOL_CALL, TOOL_RESPONSE)</code> <p>Create a new ChatRole enum with only the values of the RoleMembers changed.</p> <p>This will ensure that the parent of each member is ChatRole, which means that the members will be equal to the members of ChatRole.</p> Example <pre><code>OpenAIChatRole = create_chat_role(\n    \"OpenAIChatRole\",\n    SYSTEM=\"system\",\n    HUMAN=\"user\",\n    AI=\"assistant\",\n    TOOL_CALL=\"function_call\",\n    TOOL_RESPONSE=\"function\",\n)\n\nassert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\role\\chat.py</code> <pre><code>def create_chat_role(\n    name: str, SYSTEM: str, HUMAN: str, AI: str, TOOL_CALL: str, TOOL_RESPONSE: str\n) -&gt; Type[ChatRole]:\n    \"\"\"\n    Create a new ChatRole enum with only the values of the RoleMembers changed.\n\n    This will ensure that the parent of each member is ChatRole, which means\n    that the members will be equal to the members of ChatRole.\n\n    Example:\n        ```python\n        OpenAIChatRole = create_chat_role(\n            \"OpenAIChatRole\",\n            SYSTEM=\"system\",\n            HUMAN=\"user\",\n            AI=\"assistant\",\n            TOOL_CALL=\"function_call\",\n            TOOL_RESPONSE=\"function\",\n        )\n\n        assert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n        ```\n    \"\"\"\n    return Enum(\n        name,\n        {\n            \"SYSTEM\": RoleMember(\"SYSTEM\", SYSTEM, ChatRole),\n            \"HUMAN\": RoleMember(\"HUMAN\", HUMAN, ChatRole),\n            \"AI\": RoleMember(\"AI\", AI, ChatRole),\n            \"TOOL_CALL\": RoleMember(\"TOOL_CALL\", TOOL_CALL, ChatRole),\n            \"TOOL_RESPONSE\": RoleMember(\"TOOL_RESPONSE\", TOOL_RESPONSE, ChatRole),\n        },\n    )\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.role-modules","title":"Modules","text":"<code>chat</code> Functions <code>create_chat_role(name, SYSTEM, HUMAN, AI, TOOL_CALL, TOOL_RESPONSE)</code> <p>Create a new ChatRole enum with only the values of the RoleMembers changed.</p> <p>This will ensure that the parent of each member is ChatRole, which means that the members will be equal to the members of ChatRole.</p> Example <pre><code>OpenAIChatRole = create_chat_role(\n    \"OpenAIChatRole\",\n    SYSTEM=\"system\",\n    HUMAN=\"user\",\n    AI=\"assistant\",\n    TOOL_CALL=\"function_call\",\n    TOOL_RESPONSE=\"function\",\n)\n\nassert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\role\\chat.py</code> <pre><code>def create_chat_role(\n    name: str, SYSTEM: str, HUMAN: str, AI: str, TOOL_CALL: str, TOOL_RESPONSE: str\n) -&gt; Type[ChatRole]:\n    \"\"\"\n    Create a new ChatRole enum with only the values of the RoleMembers changed.\n\n    This will ensure that the parent of each member is ChatRole, which means\n    that the members will be equal to the members of ChatRole.\n\n    Example:\n        ```python\n        OpenAIChatRole = create_chat_role(\n            \"OpenAIChatRole\",\n            SYSTEM=\"system\",\n            HUMAN=\"user\",\n            AI=\"assistant\",\n            TOOL_CALL=\"function_call\",\n            TOOL_RESPONSE=\"function\",\n        )\n\n        assert OpenAIChatRole.SYSTEM == ChatRole.SYSTEM # True\n        ```\n    \"\"\"\n    return Enum(\n        name,\n        {\n            \"SYSTEM\": RoleMember(\"SYSTEM\", SYSTEM, ChatRole),\n            \"HUMAN\": RoleMember(\"HUMAN\", HUMAN, ChatRole),\n            \"AI\": RoleMember(\"AI\", AI, ChatRole),\n            \"TOOL_CALL\": RoleMember(\"TOOL_CALL\", TOOL_CALL, ChatRole),\n            \"TOOL_RESPONSE\": RoleMember(\"TOOL_RESPONSE\", TOOL_RESPONSE, ChatRole),\n        },\n    )\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.tokenizer","title":"<code>tokenizer</code>","text":""},{"location":"api/core/#SilverLingua.core.atoms.tokenizer-classes","title":"Classes","text":"<code>Tokenizer</code> <p>               Bases: <code>BaseModel</code></p> <p>A tokenizer that can encode and decode strings.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tokenizer.py</code> <pre><code>class Tokenizer(BaseModel):\n    \"\"\"\n    A tokenizer that can encode and decode strings.\n    \"\"\"\n\n    encode: Callable[[str], List[int]]\n    decode: Callable[[List[int]], str]\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.tool","title":"<code>tool</code>","text":""},{"location":"api/core/#SilverLingua.core.atoms.tool-classes","title":"Classes","text":"<code>FunctionJSONSchema</code> <p>               Bases: <code>BaseModel</code></p> <p>A function according to JSON schema standards.</p> <p>This is also passed in to OpenAI ChatCompletion API functions list so the AI understands how to call a function.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function</p> <code>description</code> <code>str</code> <p>A description of the function</p> <code>parameters</code> <code>Parameters</code> <p>A dictionary of parameters and their types (Optional)</p> <p>Example:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class FunctionJSONSchema(BaseModel):\n    \"\"\"\n    A function according to JSON schema standards.\n\n    This is also passed in to OpenAI ChatCompletion API\n    functions list so the AI understands how to call a function.\n\n    Attributes:\n        name: The name of the function\n        description: A description of the function\n        parameters: A dictionary of parameters and their types (Optional)\n\n    Example:\n\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: Parameters\n</code></pre> <code>Tool</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre> Functions <code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre> <code>ToolCall</code> <p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre> <code>ToolCallResponse</code> <p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre> <code>ToolCalls</code> <p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre>"},{"location":"api/core/#SilverLingua.core.atoms.tool-modules","title":"Modules","text":"<code>decorator</code> Classes <code>ToolWrapper</code> <p>A wrapper class that makes a function behave like a Tool instance.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\decorator.py</code> <pre><code>class ToolWrapper:\n    \"\"\"A wrapper class that makes a function behave like a Tool instance.\"\"\"\n\n    def __init__(self, func: Callable):\n        self._tool = Tool(function=func)\n        update_wrapper(self, func)\n\n        # Copy commonly accessed attributes\n        self.function = self._tool.function\n        self.description = self._tool.description\n        self.name = self._tool.name\n        self.use_function_call = self._tool.use_function_call\n\n    def __call__(self, *args, **kwargs):\n        return self._tool(*args, **kwargs)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        return getattr(self._tool, name)\n\n    def __str__(self) -&gt; str:\n        return self._tool.model_dump_json()\n</code></pre> Functions <code>tool(func)</code> <p>A decorator that converts a function into a Tool. This allows for a more concise way to create tools compared to using Tool(function).</p> <p>Example Usage:</p> <pre><code>@tool\ndef add_numbers(x: int, y: int) -&gt; int:\n    '''Add two numbers together.'''\n    return x + y\n\n# The function is now a Tool instance\nresult = add_numbers(2, 3)  # Returns \"5\" (as a JSON string)\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\decorator.py</code> <pre><code>def tool(func: Callable) -&gt; ToolWrapper:\n    \"\"\"\n    A decorator that converts a function into a Tool.\n    This allows for a more concise way to create tools compared to using Tool(function).\n\n    Example Usage:\n    ```python\n    @tool\n    def add_numbers(x: int, y: int) -&gt; int:\n        '''Add two numbers together.'''\n        return x + y\n\n    # The function is now a Tool instance\n    result = add_numbers(2, 3)  # Returns \"5\" (as a JSON string)\n    ```\n    \"\"\"\n    return ToolWrapper(func)\n</code></pre> <code>tool</code> Classes <code>Tool</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper class for functions that allows them to be both directly callable and serializable to JSON for use with an LLM.</p> <p>Attributes:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The function to be wrapped.</p> <code>description</code> <code>FunctionJSONSchema</code> <p>A TypedDict that describes the function according to JSON schema standards.</p> <code>name</code> <code>str</code> <p>The name of the function, extracted from the FunctionJSONSchema.</p> Example <pre><code>def my_function(x, y):\n    return x + y\n\n# Create a Tool instance\ntool_instance = Tool(my_function)\n\n# Directly call the wrapped function\nresult = tool_instance(1, 2)  # Output will be 3\n\n# Serialize to JSON\nserialized = str(tool_instance)\n</code></pre> <p>Alternatively, you can call the function using a ToolCallFunction object.     ```python     # Create a FunctionCall object     function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})</p> <pre><code># Call the function using the FunctionCall object\nresult = tool_instance(function_call)  # Output will be 3\n```\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A wrapper class for functions that allows them to be both directly callable\n    and serializable to JSON for use with an LLM.\n\n    Attributes:\n        function (Callable): The function to be wrapped.\n        description (FunctionJSONSchema): A TypedDict that describes the function\n            according to JSON schema standards.\n        name (str): The name of the function, extracted from the FunctionJSONSchema.\n\n    Example:\n        ```python\n        def my_function(x, y):\n            return x + y\n\n        # Create a Tool instance\n        tool_instance = Tool(my_function)\n\n        # Directly call the wrapped function\n        result = tool_instance(1, 2)  # Output will be 3\n\n        # Serialize to JSON\n        serialized = str(tool_instance)\n        ```\n\n    Alternatively, you can call the function using a ToolCallFunction object.\n        ```python\n        # Create a FunctionCall object\n        function_call = FunctionCall(\"my_function\", {\"x\": 1, \"y\": 2})\n\n        # Call the function using the FunctionCall object\n        result = tool_instance(function_call)  # Output will be 3\n        ```\n    \"\"\"\n\n    function: Callable = Field(exclude=True)\n    description: FunctionJSONSchema = Field(validate_default=True)\n    name: str = Field(validate_default=True)\n\n    def use_function_call(self, function_call: ToolCallFunction):\n        \"\"\"\n        Uses a FunctionCall to call the function.\n        \"\"\"\n        arguments_dict = function_call.arguments\n        if arguments_dict == \"\":\n            return json.dumps(self.function())\n\n        try:\n            arguments_dict = json.loads(function_call.arguments)\n        except json.JSONDecodeError:\n            raise ValueError(\n                \"ToolCall.arguments must be a JSON string.\\n\"\n                + f\"function_call.arguments: {function_call.arguments}\\n\"\n                + f\"json.loads result: {arguments_dict}\"\n            ) from None\n\n        return json.dumps(self.function(**arguments_dict))\n\n    def __call__(self, *args, **kwargs):\n        if len(args) == 1 and isinstance(args[0], ToolCallFunction):\n            return self.use_function_call(args[0])\n        return json.dumps(self.function(*args, **kwargs))\n\n    def __str__(self) -&gt; str:\n        return self.model_dump_json()\n\n    def __init__(self, function: Callable):\n        \"\"\"\n        Creates a new Tool instance from the given function.\n\n        Args:\n            function (Callable): The function to be turned into a Tool.\n        \"\"\"\n        description = generate_function_json(function)\n        name = description.name\n        super().__init__(function=function, description=description, name=name)\n</code></pre> Functions <code>__init__(function)</code> <p>Creates a new Tool instance from the given function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to be turned into a Tool.</p> required Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def __init__(self, function: Callable):\n    \"\"\"\n    Creates a new Tool instance from the given function.\n\n    Args:\n        function (Callable): The function to be turned into a Tool.\n    \"\"\"\n    description = generate_function_json(function)\n    name = description.name\n    super().__init__(function=function, description=description, name=name)\n</code></pre> <code>use_function_call(function_call)</code> <p>Uses a FunctionCall to call the function.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\tool.py</code> <pre><code>def use_function_call(self, function_call: ToolCallFunction):\n    \"\"\"\n    Uses a FunctionCall to call the function.\n    \"\"\"\n    arguments_dict = function_call.arguments\n    if arguments_dict == \"\":\n        return json.dumps(self.function())\n\n    try:\n        arguments_dict = json.loads(function_call.arguments)\n    except json.JSONDecodeError:\n        raise ValueError(\n            \"ToolCall.arguments must be a JSON string.\\n\"\n            + f\"function_call.arguments: {function_call.arguments}\\n\"\n            + f\"json.loads result: {arguments_dict}\"\n        ) from None\n\n    return json.dumps(self.function(**arguments_dict))\n</code></pre> Functions <code>util</code> Classes <code>FunctionJSONSchema</code> <p>               Bases: <code>BaseModel</code></p> <p>A function according to JSON schema standards.</p> <p>This is also passed in to OpenAI ChatCompletion API functions list so the AI understands how to call a function.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the function</p> <code>description</code> <code>str</code> <p>A description of the function</p> <code>parameters</code> <code>Parameters</code> <p>A dictionary of parameters and their types (Optional)</p> <p>Example:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class FunctionJSONSchema(BaseModel):\n    \"\"\"\n    A function according to JSON schema standards.\n\n    This is also passed in to OpenAI ChatCompletion API\n    functions list so the AI understands how to call a function.\n\n    Attributes:\n        name: The name of the function\n        description: A description of the function\n        parameters: A dictionary of parameters and their types (Optional)\n\n    Example:\n\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n\n    name: str\n    description: str\n    parameters: Parameters\n</code></pre> <code>Parameter</code> <p>               Bases: <code>BaseModel</code></p> <p>The parameter of a function according to JSON schema standards. (Used by OpenAI function calling)</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class Parameter(BaseModel):\n    \"\"\"\n    The parameter of a function according to JSON schema standards.\n    (Used by OpenAI function calling)\n    \"\"\"\n\n    type: str\n    description: Optional[str] = None\n    enum: Optional[list[str]] = None\n</code></pre> <code>Parameters</code> <p>               Bases: <code>BaseModel</code></p> <p>The parameters property of a function according to JSON schema standards. (Used by OpenAI function calling)</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class Parameters(BaseModel):\n    \"\"\"\n    The parameters property of a function according to\n    JSON schema standards. (Used by OpenAI function calling)\n    \"\"\"\n\n    type: str\n    properties: Dict[str, Parameter] = {}\n    required: Optional[List[str]] = None\n</code></pre> <code>ToolCall</code> <p>               Bases: <code>BaseModel</code></p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCall(BaseModel):\n    model_config = ConfigDict(extra=\"allow\", ignored_types=(type(None),))\n    #\n    function: ToolCallFunction\n    id: str = Field(default_factory=lambda: str(uuid4()))\n    index: Optional[int] = None\n\n    @field_validator(\"id\", mode=\"before\")\n    def string_if_none(cls, v):\n        return v if v is not None else str(uuid4())\n\n    def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n        \"\"\"\n        Concatenates two tool calls and returns the result.\n\n        If the IDs are different, prioritize the ID of 'self'.\n        For 'function', merge the 'name' and 'arguments' fields.\n\n        We will prefer the `id` of self over other for 2 reasons:\n        1. We assume that self is the older of the two\n        2. The newer may be stream chunked, in which case\n        the `id` of `other` may have been `None` and generated\n        using UUID, but the older ID likely was generated\n        by an API and thus this newer ID is not the true ID.\n        \"\"\"\n        merged_function = {\n            \"name\": self.function.name or other.function.name,\n            \"arguments\": (self.function.arguments or \"\")\n            + (other.function.arguments or \"\"),\n        }\n        merged_function = ToolCallFunction(**merged_function)\n\n        self_extra = self.__pydantic_extra__\n        other_extra = other.__pydantic_extra__\n\n        index = self.index if self.index is not None else other.index\n\n        # Compare the two extra fields\n        if self_extra != other_extra:\n            if not self_extra or not other_extra:\n                return ToolCall(\n                    id=(self.id) or other.id,\n                    function=merged_function,\n                    index=index,\n                    **(self_extra or {}),\n                    **(other_extra or {}),\n                )\n            # If they are different, merge them\n            merged_extra = {}\n            for key in set(self_extra.keys()) | set(other_extra.keys()):\n                if key in self_extra and key in other_extra:\n                    # If one is None, use the other\n                    # Else, concatenate them\n                    if self_extra[key] == other_extra[key]:\n                        merged_extra[key] = self_extra[key]\n                    else:\n                        if self_extra[key] is None or other_extra[key] is None:\n                            merged_extra[key] = self_extra[key] or other_extra[key]\n                        else:\n                            merged_extra[key] = self_extra[key] + other_extra[key]\n                elif key in self_extra:\n                    merged_extra[key] = self_extra[key]\n                elif key in other_extra:\n                    merged_extra[key] = other_extra[key]\n            #\n            return ToolCall(\n                id=self.id or other.id,\n                function=merged_function,\n                index=index,\n                **merged_extra,\n            )\n\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **(self_extra or {}),\n        )\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls and returns the result.</p> <p>If the IDs are different, prioritize the ID of 'self'. For 'function', merge the 'name' and 'arguments' fields.</p> <p>We will prefer the <code>id</code> of self over other for 2 reasons: 1. We assume that self is the older of the two 2. The newer may be stream chunked, in which case the <code>id</code> of <code>other</code> may have been <code>None</code> and generated using UUID, but the older ID likely was generated by an API and thus this newer ID is not the true ID.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCall\") -&gt; \"ToolCall\":\n    \"\"\"\n    Concatenates two tool calls and returns the result.\n\n    If the IDs are different, prioritize the ID of 'self'.\n    For 'function', merge the 'name' and 'arguments' fields.\n\n    We will prefer the `id` of self over other for 2 reasons:\n    1. We assume that self is the older of the two\n    2. The newer may be stream chunked, in which case\n    the `id` of `other` may have been `None` and generated\n    using UUID, but the older ID likely was generated\n    by an API and thus this newer ID is not the true ID.\n    \"\"\"\n    merged_function = {\n        \"name\": self.function.name or other.function.name,\n        \"arguments\": (self.function.arguments or \"\")\n        + (other.function.arguments or \"\"),\n    }\n    merged_function = ToolCallFunction(**merged_function)\n\n    self_extra = self.__pydantic_extra__\n    other_extra = other.__pydantic_extra__\n\n    index = self.index if self.index is not None else other.index\n\n    # Compare the two extra fields\n    if self_extra != other_extra:\n        if not self_extra or not other_extra:\n            return ToolCall(\n                id=(self.id) or other.id,\n                function=merged_function,\n                index=index,\n                **(self_extra or {}),\n                **(other_extra or {}),\n            )\n        # If they are different, merge them\n        merged_extra = {}\n        for key in set(self_extra.keys()) | set(other_extra.keys()):\n            if key in self_extra and key in other_extra:\n                # If one is None, use the other\n                # Else, concatenate them\n                if self_extra[key] == other_extra[key]:\n                    merged_extra[key] = self_extra[key]\n                else:\n                    if self_extra[key] is None or other_extra[key] is None:\n                        merged_extra[key] = self_extra[key] or other_extra[key]\n                    else:\n                        merged_extra[key] = self_extra[key] + other_extra[key]\n            elif key in self_extra:\n                merged_extra[key] = self_extra[key]\n            elif key in other_extra:\n                merged_extra[key] = other_extra[key]\n        #\n        return ToolCall(\n            id=self.id or other.id,\n            function=merged_function,\n            index=index,\n            **merged_extra,\n        )\n\n    return ToolCall(\n        id=self.id or other.id,\n        function=merged_function,\n        index=index,\n        **(self_extra or {}),\n    )\n</code></pre> <code>ToolCallResponse</code> <p>               Bases: <code>BaseModel</code></p> <p>The response property of a tool call.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCallResponse(BaseModel):\n    \"\"\"\n    The response property of a tool call.\n    \"\"\"\n\n    tool_call_id: str\n    name: str\n    content: str\n\n    @classmethod\n    def from_tool_call(cls, tool_call: \"ToolCall\", response: str) -&gt; \"ToolCallResponse\":\n        return cls(\n            tool_call_id=tool_call.id,\n            name=tool_call.function.name,\n            content=response,\n        )\n</code></pre> <code>ToolCalls</code> <p>               Bases: <code>BaseModel</code></p> <p>A list of tool calls.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>class ToolCalls(BaseModel):\n    \"\"\"\n    A list of tool calls.\n    \"\"\"\n\n    list: List[ToolCall] = Field(default_factory=list, frozen=True)\n\n    def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n        \"\"\"\n        Concatenates two tool calls lists and returns the result.\n        \"\"\"\n        new: List[ToolCall] = self.list.copy()\n        for tool_call in other.list:\n            found = False\n            # Find the tool call with the same ID\n            for i, self_tool_call in enumerate(new):\n                if (\n                    self_tool_call.id == tool_call.id\n                    or self_tool_call.index == tool_call.index\n                ):\n                    new[i] = self_tool_call.concat(tool_call)\n                    found = True\n            if not found:\n                new.append(tool_call)\n        return ToolCalls(list=new)\n</code></pre> Functions <code>concat(other)</code> <p>Concatenates two tool calls lists and returns the result.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def concat(self, other: \"ToolCalls\") -&gt; \"ToolCalls\":\n    \"\"\"\n    Concatenates two tool calls lists and returns the result.\n    \"\"\"\n    new: List[ToolCall] = self.list.copy()\n    for tool_call in other.list:\n        found = False\n        # Find the tool call with the same ID\n        for i, self_tool_call in enumerate(new):\n            if (\n                self_tool_call.id == tool_call.id\n                or self_tool_call.index == tool_call.index\n            ):\n                new[i] = self_tool_call.concat(tool_call)\n                found = True\n        if not found:\n            new.append(tool_call)\n    return ToolCalls(list=new)\n</code></pre> Functions <code>generate_function_json(func)</code> <p>Generates a FunctionJSONSchema from a python function.</p> <p>Example:</p> <pre><code>def roll_dice(sides: int = 20,\n              dice: int = 1,\n              modifier: int = 0,\n              advantage: bool = False,\n              disadvantage: bool = False):\n    \"\"\"\n    Rolls a number of dice with a given number of sides, optionally with a modifier\n    and/or advantage/disadvantage.\n    Returns `{result: int, rolls: int[]}`\n\n    Args:\n        sides: The number of sides on each die (default 20)\n        dice: The number of dice to roll (default 1)\n        modifier: The modifier to add to the roll total (default 0)\n        advantage: Whether to roll with advantage (default False)\n        disadvantage: Whether to roll with disadvantage (default False)\n    \"\"\"\n    ...\n</code></pre> <p>Usage:</p> <pre><code>result = generate_function_json(roll_dice)\nprint(result)\n</code></pre> <p>Expected Output:</p> <pre><code>{\n    \"name\": \"roll_dice\",\n    \"description\": \"Rolls a number of dice with a given number of sides, optionally\n        with a modifier and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"sides\": {\n              \"description\": \"The number of sides on each die\",\n              \"type\": \"integer\"\n            },\n            \"dice\": {\n              \"description\": \"The number of dice to roll (default 1)\",\n              \"type\": \"integer\"\n            },\n            \"modifier\": {\n              \"description\": \"The modifier to add to the roll total (default 0)\",\n              \"type\": \"integer\"\n            },\n            \"advantage\": {\n              \"description\": \"Whether to roll with advantage (default False)\",\n              \"type\": \"boolean\"\n            },\n            \"disadvantage\": {\n              \"description\": \"Whether to roll with disadvantage (default False)\",\n              \"type\": \"boolean\"\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def generate_function_json(func: Callable[..., Any]) -&gt; FunctionJSONSchema:\n    \"\"\"\n    Generates a FunctionJSONSchema from a python function.\n\n    Example:\n    ```python\n    def roll_dice(sides: int = 20,\n                  dice: int = 1,\n                  modifier: int = 0,\n                  advantage: bool = False,\n                  disadvantage: bool = False):\n        \\\"\"\"\n        Rolls a number of dice with a given number of sides, optionally with a modifier\n        and/or advantage/disadvantage.\n        Returns `{result: int, rolls: int[]}`\n\n        Args:\n            sides: The number of sides on each die (default 20)\n            dice: The number of dice to roll (default 1)\n            modifier: The modifier to add to the roll total (default 0)\n            advantage: Whether to roll with advantage (default False)\n            disadvantage: Whether to roll with disadvantage (default False)\n        \\\"\"\"\n        ...\n    ```\n\n    Usage:\n    ```python\n    result = generate_function_json(roll_dice)\n    print(result)\n    ```\n\n    Expected Output:\n    ```json\n    {\n        \"name\": \"roll_dice\",\n        \"description\": \"Rolls a number of dice with a given number of sides, optionally\n            with a modifier and/or advantage/disadvantage.\n            Returns `{result: int, rolls: int[]}`\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"sides\": {\n                  \"description\": \"The number of sides on each die\",\n                  \"type\": \"integer\"\n                },\n                \"dice\": {\n                  \"description\": \"The number of dice to roll (default 1)\",\n                  \"type\": \"integer\"\n                },\n                \"modifier\": {\n                  \"description\": \"The modifier to add to the roll total (default 0)\",\n                  \"type\": \"integer\"\n                },\n                \"advantage\": {\n                  \"description\": \"Whether to roll with advantage (default False)\",\n                  \"type\": \"boolean\"\n                },\n                \"disadvantage\": {\n                  \"description\": \"Whether to roll with disadvantage (default False)\",\n                  \"type\": \"boolean\"\n                }\n            }\n        }\n    }\n    ```\n    \"\"\"\n    sig = inspect.signature(func)\n    doc = inspect.getdoc(func)\n\n    description = \"\"\n    args_docs = {}\n    if doc:\n        doc_lines = doc.split(\"\\n\")\n        description = doc_lines[0]  # Capture the first line as part of the description.\n        args_lines = doc_lines[1:]\n\n        capturing_description = True\n        start_capturing = False\n        for line in args_lines:\n            if line.strip().lower() in [\"args:\", \"arguments:\"]:\n                start_capturing = True\n                capturing_description = False\n                continue\n\n            if capturing_description:\n                description += \"\\n\" + line\n            elif start_capturing:\n                match = re.match(r\"^\\s+(?P&lt;name&gt;\\w+):\\s(?P&lt;desc&gt;.*)\", line)\n                if match:\n                    args_docs[match.group(\"name\")] = match.group(\"desc\")\n\n    properties = {}\n    required = []\n    for name, param in sig.parameters.items():\n        param_type = param.annotation if param.annotation is not inspect._empty else Any\n        type_schema = python_type_to_json_schema_type(param_type)\n\n        parameter_info = {\"description\": args_docs.get(name, \"\")}\n\n        # Handle different types of type_schema\n        if isinstance(type_schema, str):\n            parameter_info[\"type\"] = type_schema\n        elif isinstance(type_schema, dict):\n            parameter_info.update(type_schema)\n\n        properties[name] = Parameter(**parameter_info)\n\n        if param.default is param.empty:\n            required.append(name)\n\n    parameters_model = Parameters(\n        type=\"object\", properties=properties, required=required or None\n    )\n\n    return FunctionJSONSchema(\n        name=func.__name__, description=description.strip(), parameters=parameters_model\n    )\n</code></pre> <code>python_type_to_json_schema_type(python_type)</code> <p>Maps Python types to JSON Schema types.</p> <p>Parameters:</p> Name Type Description Default <code>python_type</code> <code>Type[Any]</code> <p>The Python type.</p> required <p>Returns:</p> Type Description <code>Union[str, Dict]</code> <p>Union[str, Dict]: The corresponding JSON Schema type or schema.</p> Source code in <code>src\\SilverLingua\\core\\atoms\\tool\\util.py</code> <pre><code>def python_type_to_json_schema_type(python_type: Type[Any]) -&gt; Union[str, Dict]:\n    \"\"\"\n    Maps Python types to JSON Schema types.\n\n    Args:\n      python_type: The Python type.\n\n    Returns:\n      Union[str, Dict]: The corresponding JSON Schema type or schema.\n    \"\"\"\n    simple_type_mapping = {\n        int: \"integer\",\n        float: \"number\",\n        str: \"string\",\n        bool: \"boolean\",\n        type(None): \"null\",\n    }\n\n    if python_type in simple_type_mapping:\n        return simple_type_mapping[python_type]\n\n    if hasattr(python_type, \"__origin__\"):\n        origin = python_type.__origin__  # type: ignore\n\n        if origin is Union:\n            types = python_type.__args__  # type: ignore\n            if type(None) in types:\n                # This is equivalent to Optional[T]\n                types = [t for t in types if t is not type(None)]\n                if len(types) == 1:\n                    return python_type_to_json_schema_type(types[0])\n\n        if origin is list:\n            item_type = (\n                python_type.__args__[0] if python_type.__args__ else Any\n            )  # type: ignore\n            return {\n                \"type\": \"array\",\n                \"items\": {\"type\": python_type_to_json_schema_type(item_type)},\n            }\n        elif origin is dict:\n            key_type = (\n                python_type.__args__[0] if python_type.__args__ else Any\n            )  # type: ignore\n            value_type = (\n                python_type.__args__[1] if python_type.__args__ else Any\n            )  # type: ignore\n            if key_type is not str:\n                raise ValueError(\n                    \"Dictionary key type must be str for conversion to JSON schema\"\n                )\n            return {\n                \"type\": \"object\",\n                \"additionalProperties\": python_type_to_json_schema_type(value_type),\n            }\n\n    if hasattr(python_type, \"__annotations__\"):\n        properties = {}\n        for k, v in python_type.__annotations__.items():\n            type_schema = python_type_to_json_schema_type(v)\n            if isinstance(type_schema, str):\n                properties[k] = {\"type\": type_schema}\n            elif isinstance(type_schema, dict):\n                properties[k] = type_schema\n        required = [\n            k\n            for k in properties\n            if k not in python_type.__optional_keys__  # type: ignore\n        ]\n        return {\n            \"type\": \"object\",\n            \"properties\": properties,\n            **({\"required\": required} if required else {}),\n        }\n\n    print(f\"Unknown type encountered: {python_type}\")\n    return \"unknown\"\n</code></pre>"},{"location":"api/core/#SilverLingua.core.molecules","title":"<code>molecules</code>","text":""},{"location":"api/core/#SilverLingua.core.molecules-classes","title":"Classes","text":""},{"location":"api/core/#SilverLingua.core.molecules.Link","title":"<code>Link</code>","text":"<p>               Bases: <code>Memory</code></p> <p>A memory that can have a parent and children Links, forming a hierarchical structure of interconnected memories.</p> <p>The content can be either a Notion or a Memory. (You can still use the content as a string via str(link.content).</p> Source code in <code>src\\SilverLingua\\core\\molecules\\link.py</code> <pre><code>class Link(Memory):\n    \"\"\"\n    A memory that can have a parent and children Links,\n    forming a hierarchical structure of interconnected memories.\n\n    The content can be either a Notion or a Memory.\n    (You can still use the content as a string via str(link.content).\n    \"\"\"\n\n    content: Union[Notion, Memory]\n    parent: Optional[\"Link\"] = None\n    children: List[\"Link\"] = Field(default_factory=list)\n\n    def add_child(self, child: \"Link\") -&gt; None:\n        self.children.append(child)\n        child.parent = self\n\n    def remove_child(self, child: \"Link\") -&gt; None:\n        self.children.remove(child)\n        child.parent = None\n\n    @property\n    def path(self) -&gt; List[\"Link\"]:\n        \"\"\"\n        Returns the path from the root to this Link.\n        \"\"\"\n        path = [self]\n        while path[-1].parent is not None:\n            path.append(path[-1].parent)\n        return path\n\n    @property\n    def root(self) -&gt; \"Link\":\n        \"\"\"\n        Returns the root Link of this Link.\n        \"\"\"\n        return self.path[-1]\n\n    @property\n    def depth(self) -&gt; int:\n        \"\"\"\n        Returns 1 based depth of this Link.\n        \"\"\"\n        return len(self.path)\n\n    @property\n    def is_root(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a root Link.\n        \"\"\"\n        return self.parent is None\n\n    @property\n    def is_leaf(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a leaf Link.\n        \"\"\"\n        return len(self.children) == 0\n\n    @property\n    def is_branch(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a branch Link.\n        \"\"\"\n        return not self.is_leaf\n\n    @property\n    def path_string(self) -&gt; str:\n        \"\"\"\n        Returns the path from the root to this Link as a string.\n\n        Example:\n        \"root&gt;child&gt;grandchild\"\n        \"\"\"\n        path_str = f\"{self.content}\"\n        if not self.is_root:\n            path_str = f\"{self.parent.path_string}&gt;{path_str}\"\n        return path_str\n</code></pre>"},{"location":"api/core/#SilverLingua.core.molecules.Link-attributes","title":"Attributes","text":"<code>depth: int</code> <code>property</code> <p>Returns 1 based depth of this Link.</p> <code>is_branch: bool</code> <code>property</code> <p>Returns whether this Link is a branch Link.</p> <code>is_leaf: bool</code> <code>property</code> <p>Returns whether this Link is a leaf Link.</p> <code>is_root: bool</code> <code>property</code> <p>Returns whether this Link is a root Link.</p> <code>path: List[Link]</code> <code>property</code> <p>Returns the path from the root to this Link.</p> <code>path_string: str</code> <code>property</code> <p>Returns the path from the root to this Link as a string.</p> <p>Example: \"root&gt;child&gt;grandchild\"</p> <code>root: Link</code> <code>property</code> <p>Returns the root Link of this Link.</p>"},{"location":"api/core/#SilverLingua.core.molecules.Notion","title":"<code>Notion</code>","text":"<p>               Bases: <code>Memory</code></p> <p>A memory that stores the role associated with its content. The role is usually a <code>ChatRole</code> or a <code>ReactRole</code>. (See <code>atoms/roles</code>)</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the notion.</p> <code>content</code> <code>str</code> <p>The content of the memory.</p> <code>persistent</code> <code>bool</code> <p>Whether the notion should be stored in long-term memory.</p> Source code in <code>src\\SilverLingua\\core\\molecules\\notion.py</code> <pre><code>class Notion(Memory):\n    \"\"\"\n    A memory that stores the role associated with its content.\n    The role is usually a `ChatRole` or a `ReactRole`.\n    (See `atoms/roles`)\n\n    Attributes:\n        role: The role of the notion.\n        content: The content of the memory.\n        persistent: Whether the notion should be stored in long-term memory.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n    role: str\n    persistent: bool = False\n\n    @field_validator(\"role\", mode=\"before\")\n    @classmethod\n    def validate_role(cls, v: Union[ChatRole, ReactRole, str]):\n        if isinstance(v, (ChatRole, ReactRole)):\n            return v.value.value\n        elif isinstance(v, str):\n            return v\n        raise ValueError(f\"Expected a ChatRole, ReactRole, or a string, got {type(v)}\")\n\n    def __str__(self) -&gt; str:\n        return f\"{self.role}: {self.content}\"\n\n    def __init__(\n        self,\n        content: str,\n        role: Union[ChatRole, ReactRole, str],\n        persistent: bool = False,\n    ):\n        super().__init__(content=content, role=role, persistent=persistent)\n\n    @property\n    def chat_role(self) -&gt; ChatRole:\n        \"\"\"\n        Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_chat_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ChatRole.AI\n        return r\n\n    @property\n    def react_role(self) -&gt; ReactRole:\n        \"\"\"\n        Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_react_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ReactRole.THOUGHT\n        return r\n</code></pre>"},{"location":"api/core/#SilverLingua.core.molecules.Notion-attributes","title":"Attributes","text":"<code>chat_role: ChatRole</code> <code>property</code> <p>Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)</p> <p>(See <code>config</code>)</p> <code>react_role: ReactRole</code> <code>property</code> <p>Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)</p> <p>(See <code>config</code>)</p>"},{"location":"api/core/#SilverLingua.core.molecules-modules","title":"Modules","text":""},{"location":"api/core/#SilverLingua.core.molecules.link","title":"<code>link</code>","text":""},{"location":"api/core/#SilverLingua.core.molecules.link-classes","title":"Classes","text":"<code>Link</code> <p>               Bases: <code>Memory</code></p> <p>A memory that can have a parent and children Links, forming a hierarchical structure of interconnected memories.</p> <p>The content can be either a Notion or a Memory. (You can still use the content as a string via str(link.content).</p> Source code in <code>src\\SilverLingua\\core\\molecules\\link.py</code> <pre><code>class Link(Memory):\n    \"\"\"\n    A memory that can have a parent and children Links,\n    forming a hierarchical structure of interconnected memories.\n\n    The content can be either a Notion or a Memory.\n    (You can still use the content as a string via str(link.content).\n    \"\"\"\n\n    content: Union[Notion, Memory]\n    parent: Optional[\"Link\"] = None\n    children: List[\"Link\"] = Field(default_factory=list)\n\n    def add_child(self, child: \"Link\") -&gt; None:\n        self.children.append(child)\n        child.parent = self\n\n    def remove_child(self, child: \"Link\") -&gt; None:\n        self.children.remove(child)\n        child.parent = None\n\n    @property\n    def path(self) -&gt; List[\"Link\"]:\n        \"\"\"\n        Returns the path from the root to this Link.\n        \"\"\"\n        path = [self]\n        while path[-1].parent is not None:\n            path.append(path[-1].parent)\n        return path\n\n    @property\n    def root(self) -&gt; \"Link\":\n        \"\"\"\n        Returns the root Link of this Link.\n        \"\"\"\n        return self.path[-1]\n\n    @property\n    def depth(self) -&gt; int:\n        \"\"\"\n        Returns 1 based depth of this Link.\n        \"\"\"\n        return len(self.path)\n\n    @property\n    def is_root(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a root Link.\n        \"\"\"\n        return self.parent is None\n\n    @property\n    def is_leaf(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a leaf Link.\n        \"\"\"\n        return len(self.children) == 0\n\n    @property\n    def is_branch(self) -&gt; bool:\n        \"\"\"\n        Returns whether this Link is a branch Link.\n        \"\"\"\n        return not self.is_leaf\n\n    @property\n    def path_string(self) -&gt; str:\n        \"\"\"\n        Returns the path from the root to this Link as a string.\n\n        Example:\n        \"root&gt;child&gt;grandchild\"\n        \"\"\"\n        path_str = f\"{self.content}\"\n        if not self.is_root:\n            path_str = f\"{self.parent.path_string}&gt;{path_str}\"\n        return path_str\n</code></pre> Attributes <code>depth: int</code> <code>property</code> <p>Returns 1 based depth of this Link.</p> <code>is_branch: bool</code> <code>property</code> <p>Returns whether this Link is a branch Link.</p> <code>is_leaf: bool</code> <code>property</code> <p>Returns whether this Link is a leaf Link.</p> <code>is_root: bool</code> <code>property</code> <p>Returns whether this Link is a root Link.</p> <code>path: List[Link]</code> <code>property</code> <p>Returns the path from the root to this Link.</p> <code>path_string: str</code> <code>property</code> <p>Returns the path from the root to this Link as a string.</p> <p>Example: \"root&gt;child&gt;grandchild\"</p> <code>root: Link</code> <code>property</code> <p>Returns the root Link of this Link.</p>"},{"location":"api/core/#SilverLingua.core.molecules.notion","title":"<code>notion</code>","text":""},{"location":"api/core/#SilverLingua.core.molecules.notion-classes","title":"Classes","text":"<code>Notion</code> <p>               Bases: <code>Memory</code></p> <p>A memory that stores the role associated with its content. The role is usually a <code>ChatRole</code> or a <code>ReactRole</code>. (See <code>atoms/roles</code>)</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the notion.</p> <code>content</code> <code>str</code> <p>The content of the memory.</p> <code>persistent</code> <code>bool</code> <p>Whether the notion should be stored in long-term memory.</p> Source code in <code>src\\SilverLingua\\core\\molecules\\notion.py</code> <pre><code>class Notion(Memory):\n    \"\"\"\n    A memory that stores the role associated with its content.\n    The role is usually a `ChatRole` or a `ReactRole`.\n    (See `atoms/roles`)\n\n    Attributes:\n        role: The role of the notion.\n        content: The content of the memory.\n        persistent: Whether the notion should be stored in long-term memory.\n    \"\"\"\n\n    model_config = ConfigDict(from_attributes=True)\n    role: str\n    persistent: bool = False\n\n    @field_validator(\"role\", mode=\"before\")\n    @classmethod\n    def validate_role(cls, v: Union[ChatRole, ReactRole, str]):\n        if isinstance(v, (ChatRole, ReactRole)):\n            return v.value.value\n        elif isinstance(v, str):\n            return v\n        raise ValueError(f\"Expected a ChatRole, ReactRole, or a string, got {type(v)}\")\n\n    def __str__(self) -&gt; str:\n        return f\"{self.role}: {self.content}\"\n\n    def __init__(\n        self,\n        content: str,\n        role: Union[ChatRole, ReactRole, str],\n        persistent: bool = False,\n    ):\n        super().__init__(content=content, role=role, persistent=persistent)\n\n    @property\n    def chat_role(self) -&gt; ChatRole:\n        \"\"\"\n        Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_chat_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ChatRole.AI\n        return r\n\n    @property\n    def react_role(self) -&gt; ReactRole:\n        \"\"\"\n        Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)\n\n        (See `config`)\n        \"\"\"\n        from ...config import Config\n\n        # Check if self.role is a member of Role\n        r = Config.get_react_role(self.role)\n        if r is None:\n            # If not, then the role is AI.\n            # Why? Because it must be an internal role.\n            return ReactRole.THOUGHT\n        return r\n</code></pre> Attributes <code>chat_role: ChatRole</code> <code>property</code> <p>Gets the chat based role Enum (e.g. Role.SYSTEM, Role.HUMAN, etc.)</p> <p>(See <code>config</code>)</p> <code>react_role: ReactRole</code> <code>property</code> <p>Gets the react based role Enum (e.g. Role.THOUGHT, Role.OBSERVATION, etc.)</p> <p>(See <code>config</code>)</p>"},{"location":"api/core/#SilverLingua.core.organisms","title":"<code>organisms</code>","text":""},{"location":"api/core/#SilverLingua.core.organisms-classes","title":"Classes","text":""},{"location":"api/core/#SilverLingua.core.organisms.Idearium","title":"<code>Idearium</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of <code>Notions</code> that is automatically trimmed to fit within a maximum number of tokens.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>class Idearium(BaseModel):\n    \"\"\"\n    A collection of `Notions` that is automatically trimmed to fit within a maximum\n    number of tokens.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    tokenizer: Tokenizer\n    max_tokens: int\n    notions: List[Notion] = Field(default_factory=list)\n    tokenized_notions: List[List[int]] = Field(default_factory=list)\n    persistent_indices: set = Field(default_factory=set)\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        max_tokens: int,\n        notions: List[Notion] = None,\n        **kwargs,\n    ):\n        # Initialize with empty notions if None\n        notions = notions or []\n\n        # Initialize tokenized_notions\n        tokenized_notions = [tokenizer.encode(notion.content) for notion in notions]\n\n        # Call parent init with all values\n        super().__init__(\n            tokenizer=tokenizer,\n            max_tokens=max_tokens,\n            notions=notions,\n            tokenized_notions=tokenized_notions,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_notions(cls, values):\n        notions = values.notions\n        for notion in notions:\n            cls.validate_notion(notion, values.max_tokens, values.tokenizer)\n        return values\n\n    @classmethod\n    def validate_notion(cls, notion: Notion, max_tokens: int, tokenizer: Tokenizer):\n        if len(notion.content) == 0:\n            raise ValueError(\"Notion content cannot be empty.\")\n\n        tokenized_notion = tokenizer.encode(notion.content)\n        if len(tokenized_notion) &gt; max_tokens:\n            raise ValueError(\"Notion exceeds maximum token length\")\n\n        return tokenized_notion\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"The total number of tokens in the Idearium.\"\"\"\n        return sum(len(notion) for notion in self.tokenized_notions)\n\n    @property\n    def _non_persistent_indices(self) -&gt; set:\n        \"\"\"The indices of non-persistent notions.\"\"\"\n        return set(range(len(self.notions))) - self.persistent_indices\n\n    def index(self, notion: Notion) -&gt; int:\n        \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n        return self.notions.index(notion)\n\n    def append(self, notion: Notion):\n        \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n        logger.debug(f\"Appending notion: {notion.content!r}\")\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        if self.notions:\n            logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n        if (\n            self.notions\n            and self.notions[-1].role == notion.role\n            and self.notions[-1].persistent == notion.persistent\n        ):\n            combined_content = self.notions[-1].content + notion.content\n            combined_notion = Notion(\n                content=combined_content,\n                role=notion.role,\n                persistent=notion.persistent,\n            )\n            self.replace(len(self.notions) - 1, combined_notion)\n            logger.debug(\n                f\"After replace, about to return combined content: {combined_content!r}\"\n            )\n            return\n\n        logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n        self.notions.append(notion)\n        self.tokenized_notions.append(tokenized_notion)\n\n        if notion.persistent:\n            # Modify the set in place instead of reassigning\n            self.persistent_indices.add(len(self.notions) - 1)\n\n        self._trim()\n\n    def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n        \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n        if isinstance(notions, Idearium):\n            notions = notions.notions\n\n        for notion in notions:\n            self.append(notion)\n\n    def insert(self, index: int, notion: Notion):\n        \"\"\"Inserts the given notion at the given index.\"\"\"\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        self.notions.insert(index, notion)\n        self.tokenized_notions.insert(index, tokenized_notion)\n\n        # Update persistent_indices in place\n        new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n        if notion.persistent:\n            self.persistent_indices.add(index)\n\n        self._trim()\n\n    def remove(self, notion: Notion):\n        \"\"\"Removes the first occurrence of the given notion.\"\"\"\n        index = self.index(notion)\n        self.pop(index)\n\n    def pop(self, index: int) -&gt; Notion:\n        \"\"\"Removes and returns the notion at the given index.\"\"\"\n        ret = self.notions.pop(index)\n        self.tokenized_notions.pop(index)\n\n        # Update persistent_indices\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.discard(index)\n        new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n\n        return ret\n\n    def replace(self, index: int, notion: Notion):\n        \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n        self.notions[index] = notion\n        self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n        # Update persistent_indices based on the replaced notion\n        if notion.persistent:\n            self.persistent_indices.add(index)\n        else:\n            self.persistent_indices.discard(index)\n\n        self._trim()\n\n    def copy(self) -&gt; \"Idearium\":\n        \"\"\"Returns a copy of the Idearium.\"\"\"\n        return Idearium(\n            tokenizer=self.tokenizer,\n            max_tokens=self.max_tokens,\n            notions=self.notions.copy(),\n            tokenized_notions=self.tokenized_notions.copy(),\n            persistent_indices=self.persistent_indices.copy(),\n        )\n\n    def _trim(self):\n        \"\"\"\n        Trims the Idearium to fit within the maximum number of tokens, called\n        after every modification.\n\n        This is the primary point of extension for Idearium subclasses, as it\n        allows for custom trimming behavior.\n        \"\"\"\n        while self.total_tokens &gt; self.max_tokens:\n            non_persistent_indices = self._non_persistent_indices\n\n            # Check if there's only one non-persistent user message\n            if len(non_persistent_indices) == 1:\n                single_index = next(iter(non_persistent_indices))\n                tokenized_notion = self.tokenized_notions[single_index]\n\n                # Trim the only non-persistent notion to fit within the token limit\n                tokenized_notion = tokenized_notion[\n                    : self.max_tokens - (self.total_tokens - len(tokenized_notion))\n                ]\n                trimmed_content = self.tokenizer.decode(tokenized_notion)\n                trimmed_notion = Notion(\n                    content=trimmed_content,\n                    role=self.notions[single_index].role,\n                    persistent=self.notions[single_index].persistent,\n                )\n                self.replace(single_index, trimmed_notion)\n                return\n\n            # Attempt to remove the first non-persistent notion\n            for i in non_persistent_indices:\n                self.pop(i)\n                break\n            else:\n                # If all notions are persistent and\n                # the max token length is still exceeded\n                raise ValueError(\n                    \"Persistent notions exceed max_tokens.\"\n                    + \" Reduce the content or increase max_tokens.\"\n                )\n\n    def __len__(self) -&gt; int:\n        return len(self.notions)\n\n    def __getitem__(self, index: int) -&gt; Notion:\n        return self.notions[index]\n\n    def __setitem__(self, index: int, notion: Notion):\n        self.replace(index, notion)\n\n    def __delitem__(self, index: int):\n        self.pop(index)\n\n    def __iter__(self) -&gt; Iterator[Notion]:\n        return iter(self.notions)\n\n    def __contains__(self, notion: Notion) -&gt; bool:\n        return notion in self.notions\n\n    def __str__(self) -&gt; str:\n        return str(self.notions)\n\n    def __repr__(self) -&gt; str:\n        return repr(self.notions)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Idearium):\n            return NotImplemented\n        return self.notions == other.notions\n</code></pre>"},{"location":"api/core/#SilverLingua.core.organisms.Idearium-attributes","title":"Attributes","text":"<code>total_tokens: int</code> <code>property</code> <p>The total number of tokens in the Idearium.</p>"},{"location":"api/core/#SilverLingua.core.organisms.Idearium-functions","title":"Functions","text":"<code>append(notion)</code> <p>Appends the given notion to the end of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def append(self, notion: Notion):\n    \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n    logger.debug(f\"Appending notion: {notion.content!r}\")\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    if self.notions:\n        logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n    if (\n        self.notions\n        and self.notions[-1].role == notion.role\n        and self.notions[-1].persistent == notion.persistent\n    ):\n        combined_content = self.notions[-1].content + notion.content\n        combined_notion = Notion(\n            content=combined_content,\n            role=notion.role,\n            persistent=notion.persistent,\n        )\n        self.replace(len(self.notions) - 1, combined_notion)\n        logger.debug(\n            f\"After replace, about to return combined content: {combined_content!r}\"\n        )\n        return\n\n    logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n    self.notions.append(notion)\n    self.tokenized_notions.append(tokenized_notion)\n\n    if notion.persistent:\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.add(len(self.notions) - 1)\n\n    self._trim()\n</code></pre> <code>copy()</code> <p>Returns a copy of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def copy(self) -&gt; \"Idearium\":\n    \"\"\"Returns a copy of the Idearium.\"\"\"\n    return Idearium(\n        tokenizer=self.tokenizer,\n        max_tokens=self.max_tokens,\n        notions=self.notions.copy(),\n        tokenized_notions=self.tokenized_notions.copy(),\n        persistent_indices=self.persistent_indices.copy(),\n    )\n</code></pre> <code>extend(notions)</code> <p>Extends the Idearium with the given list of notions.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n    \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n    if isinstance(notions, Idearium):\n        notions = notions.notions\n\n    for notion in notions:\n        self.append(notion)\n</code></pre> <code>index(notion)</code> <p>Returns the index of the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def index(self, notion: Notion) -&gt; int:\n    \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n    return self.notions.index(notion)\n</code></pre> <code>insert(index, notion)</code> <p>Inserts the given notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def insert(self, index: int, notion: Notion):\n    \"\"\"Inserts the given notion at the given index.\"\"\"\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    self.notions.insert(index, notion)\n    self.tokenized_notions.insert(index, tokenized_notion)\n\n    # Update persistent_indices in place\n    new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n    if notion.persistent:\n        self.persistent_indices.add(index)\n\n    self._trim()\n</code></pre> <code>pop(index)</code> <p>Removes and returns the notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def pop(self, index: int) -&gt; Notion:\n    \"\"\"Removes and returns the notion at the given index.\"\"\"\n    ret = self.notions.pop(index)\n    self.tokenized_notions.pop(index)\n\n    # Update persistent_indices\n    # Modify the set in place instead of reassigning\n    self.persistent_indices.discard(index)\n    new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n\n    return ret\n</code></pre> <code>remove(notion)</code> <p>Removes the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def remove(self, notion: Notion):\n    \"\"\"Removes the first occurrence of the given notion.\"\"\"\n    index = self.index(notion)\n    self.pop(index)\n</code></pre> <code>replace(index, notion)</code> <p>Replaces the notion at the given index with the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def replace(self, index: int, notion: Notion):\n    \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n    self.notions[index] = notion\n    self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n    # Update persistent_indices based on the replaced notion\n    if notion.persistent:\n        self.persistent_indices.add(index)\n    else:\n        self.persistent_indices.discard(index)\n\n    self._trim()\n</code></pre>"},{"location":"api/core/#SilverLingua.core.organisms-modules","title":"Modules","text":""},{"location":"api/core/#SilverLingua.core.organisms.idearium","title":"<code>idearium</code>","text":""},{"location":"api/core/#SilverLingua.core.organisms.idearium-classes","title":"Classes","text":"<code>Idearium</code> <p>               Bases: <code>BaseModel</code></p> <p>A collection of <code>Notions</code> that is automatically trimmed to fit within a maximum number of tokens.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>class Idearium(BaseModel):\n    \"\"\"\n    A collection of `Notions` that is automatically trimmed to fit within a maximum\n    number of tokens.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    tokenizer: Tokenizer\n    max_tokens: int\n    notions: List[Notion] = Field(default_factory=list)\n    tokenized_notions: List[List[int]] = Field(default_factory=list)\n    persistent_indices: set = Field(default_factory=set)\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        max_tokens: int,\n        notions: List[Notion] = None,\n        **kwargs,\n    ):\n        # Initialize with empty notions if None\n        notions = notions or []\n\n        # Initialize tokenized_notions\n        tokenized_notions = [tokenizer.encode(notion.content) for notion in notions]\n\n        # Call parent init with all values\n        super().__init__(\n            tokenizer=tokenizer,\n            max_tokens=max_tokens,\n            notions=notions,\n            tokenized_notions=tokenized_notions,\n            **kwargs,\n        )\n\n    @model_validator(mode=\"after\")\n    def validate_notions(cls, values):\n        notions = values.notions\n        for notion in notions:\n            cls.validate_notion(notion, values.max_tokens, values.tokenizer)\n        return values\n\n    @classmethod\n    def validate_notion(cls, notion: Notion, max_tokens: int, tokenizer: Tokenizer):\n        if len(notion.content) == 0:\n            raise ValueError(\"Notion content cannot be empty.\")\n\n        tokenized_notion = tokenizer.encode(notion.content)\n        if len(tokenized_notion) &gt; max_tokens:\n            raise ValueError(\"Notion exceeds maximum token length\")\n\n        return tokenized_notion\n\n    @property\n    def total_tokens(self) -&gt; int:\n        \"\"\"The total number of tokens in the Idearium.\"\"\"\n        return sum(len(notion) for notion in self.tokenized_notions)\n\n    @property\n    def _non_persistent_indices(self) -&gt; set:\n        \"\"\"The indices of non-persistent notions.\"\"\"\n        return set(range(len(self.notions))) - self.persistent_indices\n\n    def index(self, notion: Notion) -&gt; int:\n        \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n        return self.notions.index(notion)\n\n    def append(self, notion: Notion):\n        \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n        logger.debug(f\"Appending notion: {notion.content!r}\")\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        if self.notions:\n            logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n        if (\n            self.notions\n            and self.notions[-1].role == notion.role\n            and self.notions[-1].persistent == notion.persistent\n        ):\n            combined_content = self.notions[-1].content + notion.content\n            combined_notion = Notion(\n                content=combined_content,\n                role=notion.role,\n                persistent=notion.persistent,\n            )\n            self.replace(len(self.notions) - 1, combined_notion)\n            logger.debug(\n                f\"After replace, about to return combined content: {combined_content!r}\"\n            )\n            return\n\n        logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n        self.notions.append(notion)\n        self.tokenized_notions.append(tokenized_notion)\n\n        if notion.persistent:\n            # Modify the set in place instead of reassigning\n            self.persistent_indices.add(len(self.notions) - 1)\n\n        self._trim()\n\n    def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n        \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n        if isinstance(notions, Idearium):\n            notions = notions.notions\n\n        for notion in notions:\n            self.append(notion)\n\n    def insert(self, index: int, notion: Notion):\n        \"\"\"Inserts the given notion at the given index.\"\"\"\n        tokenized_notion = self.tokenizer.encode(notion.content)\n\n        self.notions.insert(index, notion)\n        self.tokenized_notions.insert(index, tokenized_notion)\n\n        # Update persistent_indices in place\n        new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n        if notion.persistent:\n            self.persistent_indices.add(index)\n\n        self._trim()\n\n    def remove(self, notion: Notion):\n        \"\"\"Removes the first occurrence of the given notion.\"\"\"\n        index = self.index(notion)\n        self.pop(index)\n\n    def pop(self, index: int) -&gt; Notion:\n        \"\"\"Removes and returns the notion at the given index.\"\"\"\n        ret = self.notions.pop(index)\n        self.tokenized_notions.pop(index)\n\n        # Update persistent_indices\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.discard(index)\n        new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n        self.persistent_indices.clear()\n        self.persistent_indices.update(new_indices)\n\n        return ret\n\n    def replace(self, index: int, notion: Notion):\n        \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n        self.notions[index] = notion\n        self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n        # Update persistent_indices based on the replaced notion\n        if notion.persistent:\n            self.persistent_indices.add(index)\n        else:\n            self.persistent_indices.discard(index)\n\n        self._trim()\n\n    def copy(self) -&gt; \"Idearium\":\n        \"\"\"Returns a copy of the Idearium.\"\"\"\n        return Idearium(\n            tokenizer=self.tokenizer,\n            max_tokens=self.max_tokens,\n            notions=self.notions.copy(),\n            tokenized_notions=self.tokenized_notions.copy(),\n            persistent_indices=self.persistent_indices.copy(),\n        )\n\n    def _trim(self):\n        \"\"\"\n        Trims the Idearium to fit within the maximum number of tokens, called\n        after every modification.\n\n        This is the primary point of extension for Idearium subclasses, as it\n        allows for custom trimming behavior.\n        \"\"\"\n        while self.total_tokens &gt; self.max_tokens:\n            non_persistent_indices = self._non_persistent_indices\n\n            # Check if there's only one non-persistent user message\n            if len(non_persistent_indices) == 1:\n                single_index = next(iter(non_persistent_indices))\n                tokenized_notion = self.tokenized_notions[single_index]\n\n                # Trim the only non-persistent notion to fit within the token limit\n                tokenized_notion = tokenized_notion[\n                    : self.max_tokens - (self.total_tokens - len(tokenized_notion))\n                ]\n                trimmed_content = self.tokenizer.decode(tokenized_notion)\n                trimmed_notion = Notion(\n                    content=trimmed_content,\n                    role=self.notions[single_index].role,\n                    persistent=self.notions[single_index].persistent,\n                )\n                self.replace(single_index, trimmed_notion)\n                return\n\n            # Attempt to remove the first non-persistent notion\n            for i in non_persistent_indices:\n                self.pop(i)\n                break\n            else:\n                # If all notions are persistent and\n                # the max token length is still exceeded\n                raise ValueError(\n                    \"Persistent notions exceed max_tokens.\"\n                    + \" Reduce the content or increase max_tokens.\"\n                )\n\n    def __len__(self) -&gt; int:\n        return len(self.notions)\n\n    def __getitem__(self, index: int) -&gt; Notion:\n        return self.notions[index]\n\n    def __setitem__(self, index: int, notion: Notion):\n        self.replace(index, notion)\n\n    def __delitem__(self, index: int):\n        self.pop(index)\n\n    def __iter__(self) -&gt; Iterator[Notion]:\n        return iter(self.notions)\n\n    def __contains__(self, notion: Notion) -&gt; bool:\n        return notion in self.notions\n\n    def __str__(self) -&gt; str:\n        return str(self.notions)\n\n    def __repr__(self) -&gt; str:\n        return repr(self.notions)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, Idearium):\n            return NotImplemented\n        return self.notions == other.notions\n</code></pre> Attributes <code>total_tokens: int</code> <code>property</code> <p>The total number of tokens in the Idearium.</p> Functions <code>append(notion)</code> <p>Appends the given notion to the end of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def append(self, notion: Notion):\n    \"\"\"Appends the given notion to the end of the Idearium.\"\"\"\n    logger.debug(f\"Appending notion: {notion.content!r}\")\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    if self.notions:\n        logger.debug(f\"Current last notion: {self.notions[-1].content!r}\")\n\n    if (\n        self.notions\n        and self.notions[-1].role == notion.role\n        and self.notions[-1].persistent == notion.persistent\n    ):\n        combined_content = self.notions[-1].content + notion.content\n        combined_notion = Notion(\n            content=combined_content,\n            role=notion.role,\n            persistent=notion.persistent,\n        )\n        self.replace(len(self.notions) - 1, combined_notion)\n        logger.debug(\n            f\"After replace, about to return combined content: {combined_content!r}\"\n        )\n        return\n\n    logger.debug(f\"Hitting append path. Appending new notion: {notion.content!r}\")\n    self.notions.append(notion)\n    self.tokenized_notions.append(tokenized_notion)\n\n    if notion.persistent:\n        # Modify the set in place instead of reassigning\n        self.persistent_indices.add(len(self.notions) - 1)\n\n    self._trim()\n</code></pre> <code>copy()</code> <p>Returns a copy of the Idearium.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def copy(self) -&gt; \"Idearium\":\n    \"\"\"Returns a copy of the Idearium.\"\"\"\n    return Idearium(\n        tokenizer=self.tokenizer,\n        max_tokens=self.max_tokens,\n        notions=self.notions.copy(),\n        tokenized_notions=self.tokenized_notions.copy(),\n        persistent_indices=self.persistent_indices.copy(),\n    )\n</code></pre> <code>extend(notions)</code> <p>Extends the Idearium with the given list of notions.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def extend(self, notions: Union[List[Notion], \"Idearium\"]):\n    \"\"\"Extends the Idearium with the given list of notions.\"\"\"\n    if isinstance(notions, Idearium):\n        notions = notions.notions\n\n    for notion in notions:\n        self.append(notion)\n</code></pre> <code>index(notion)</code> <p>Returns the index of the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def index(self, notion: Notion) -&gt; int:\n    \"\"\"Returns the index of the first occurrence of the given notion.\"\"\"\n    return self.notions.index(notion)\n</code></pre> <code>insert(index, notion)</code> <p>Inserts the given notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def insert(self, index: int, notion: Notion):\n    \"\"\"Inserts the given notion at the given index.\"\"\"\n    tokenized_notion = self.tokenizer.encode(notion.content)\n\n    self.notions.insert(index, notion)\n    self.tokenized_notions.insert(index, tokenized_notion)\n\n    # Update persistent_indices in place\n    new_indices = {i + 1 if i &gt;= index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n    if notion.persistent:\n        self.persistent_indices.add(index)\n\n    self._trim()\n</code></pre> <code>pop(index)</code> <p>Removes and returns the notion at the given index.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def pop(self, index: int) -&gt; Notion:\n    \"\"\"Removes and returns the notion at the given index.\"\"\"\n    ret = self.notions.pop(index)\n    self.tokenized_notions.pop(index)\n\n    # Update persistent_indices\n    # Modify the set in place instead of reassigning\n    self.persistent_indices.discard(index)\n    new_indices = {i - 1 if i &gt; index else i for i in self.persistent_indices}\n    self.persistent_indices.clear()\n    self.persistent_indices.update(new_indices)\n\n    return ret\n</code></pre> <code>remove(notion)</code> <p>Removes the first occurrence of the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def remove(self, notion: Notion):\n    \"\"\"Removes the first occurrence of the given notion.\"\"\"\n    index = self.index(notion)\n    self.pop(index)\n</code></pre> <code>replace(index, notion)</code> <p>Replaces the notion at the given index with the given notion.</p> Source code in <code>src\\SilverLingua\\core\\organisms\\idearium.py</code> <pre><code>def replace(self, index: int, notion: Notion):\n    \"\"\"Replaces the notion at the given index with the given notion.\"\"\"\n    self.notions[index] = notion\n    self.tokenized_notions[index] = self.tokenizer.encode(notion.content)\n\n    # Update persistent_indices based on the replaced notion\n    if notion.persistent:\n        self.persistent_indices.add(index)\n    else:\n        self.persistent_indices.discard(index)\n\n    self._trim()\n</code></pre>"},{"location":"api/core/#SilverLingua.core.templates","title":"<code>templates</code>","text":"<p>Core templates module.</p>"},{"location":"api/core/#SilverLingua.core.templates-classes","title":"Classes","text":""},{"location":"api/core/#SilverLingua.core.templates.Agent","title":"<code>Agent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A wrapper around a model that utilizes an Idearium and a set of Tools.</p> <p>This is a base class not meant to be used directly. It is meant to be subclassed by specific model implementations.</p> <p>However, there is limited boilerplate. The only thing that needs to be redefined in subclasses is the <code>_bind_tools</code> method.</p> <p>Additionally, the <code>_use_tools</code> method is a common method to redefine.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>class Agent(BaseModel):\n    \"\"\"\n    A wrapper around a model that utilizes an Idearium and a set of Tools.\n\n    This is a base class not meant to be used directly. It is meant to be\n    subclassed by specific model implementations.\n\n    However, there is limited boilerplate. The only thing that needs to be\n    redefined in subclasses is the `_bind_tools` method.\n\n    Additionally, the `_use_tools` method is a common method to redefine.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    model: Model\n    idearium: Idearium\n    \"\"\"\n    The Idearium used by the agent.\n    \"\"\"\n    tools: List[Tool]\n    \"\"\"\n    The tools used by the agent.\n\n    WARNING: Do not modify this list directly. Use `add_tool`, `add_tools`,\n    and `remove_tool` instead.\n    \"\"\"\n    auto_append_response: bool = True\n    \"\"\"\n    Whether to automatically append the response to the idearium after\n    generating a response.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        idearium: Optional[Idearium] = None,\n        tools: Optional[List[Tool]] = None,\n        auto_append_response: bool = True,\n    ):\n        \"\"\"\n        Initializes the agent.\n\n        Args:\n            model (Model): The model to use.\n            idearium (Idearium, optional): The idearium to use.\n                If None, a new one will be created.\n            tools (List[Tool], optional): The tools to use.\n        \"\"\"\n        super().__init__(\n            model=model,\n            idearium=idearium\n            or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n            tools=tools or [],\n            auto_append_response=auto_append_response,\n        )\n\n    def model_post_init(self, __content):\n        self._bind_tools()\n\n    @property\n    def model(self) -&gt; Model:\n        \"\"\"\n        The model used by the agent.\n        \"\"\"\n        return self.model\n\n    @property\n    def role(self) -&gt; ChatRole:\n        \"\"\"\n        The ChatRole object for the model.\n        \"\"\"\n        return self.model.role\n\n    def _find_tool(self, name: str) -&gt; Tool | None:\n        \"\"\"\n        Finds a tool by name.\n        \"\"\"\n        for t in self.tools:\n            if t.name == name:\n                return t\n        return None\n\n    def _use_tools(self, tool_calls: ToolCalls) -&gt; List[Notion]:\n        \"\"\"\n        Uses Tools based on the given ToolCalls, returning Notions\n        containing ToolCallResponses.\n\n        Args:\n            tool_calls (ToolCalls): The ToolCalls to use.\n\n        Returns:\n            List[Notion]: The Notions containing ToolCallResponses.\n                Each Notion will have a role of ChatRole.TOOL_RESPONSE.\n        \"\"\"\n        responses: List[Notion] = []\n        for tool_call in tool_calls.list:\n            tool = self._find_tool(tool_call.function.name)\n            if tool is not None:\n                tc_function_response = {}\n                with contextlib.suppress(json.JSONDecodeError):\n                    tc_function_response = json.loads(tool_call.function.arguments)\n\n                tc_response = ToolCallResponse.from_tool_call(\n                    tool_call=tool_call, response=tool(**tc_function_response)\n                )\n                responses.append(\n                    Notion(\n                        content=tc_response.model_dump_json(exclude_none=True),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n            else:\n                responses.append(\n                    Notion(\n                        content=json.dumps(\n                            {\n                                \"tool_call_id\": tool_call.id,\n                                \"content\": \"Tool not found\",\n                                \"name\": \"error\",\n                            }\n                        ),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n        return responses\n\n    def _bind_tools(self) -&gt; None:\n        \"\"\"\n        Called at the end of __init__ to bind the tools to the model.\n\n        This MUST be redefined in subclasses to dictate how\n        the tools are bound to the model.\n\n        Example:\n        ```python\n        # From OpenAIChatAgent\n        def _bind_tools(self) -&gt; None:\n            m_tools: List[ChatCompletionToolParam] = [\n                {\"type\": \"function\", \"function\": tool.description}\n                for tool in self.tools\n            ]\n\n            if len(m_tools) &gt; 0:\n                self.model.tools = m_tools\n        ```\n        \"\"\"\n        pass\n\n    def add_tool(self, tool: Tool) -&gt; None:\n        \"\"\"\n        Adds a tool to the agent.\n        \"\"\"\n        self.tools.append(tool)\n        self._bind_tools()\n\n    def add_tools(self, tools: List[Tool]) -&gt; None:\n        \"\"\"\n        Adds a list of tools to the agent.\n        \"\"\"\n        self.tools.extend(tools)\n        self._bind_tools()\n\n    def remove_tool(self, name: str) -&gt; None:\n        \"\"\"\n        Removes a tool from the agent.\n        \"\"\"\n        for i, tool in enumerate(self.tools):\n            if tool.name == name:\n                self.tools.pop(i)\n                break\n        self._bind_tools()\n\n    def _process_messages(self, messages: Messages) -&gt; List[Notion]:\n        \"\"\"Convert various message types into a list of Notions.\"\"\"\n        if isinstance(messages, str):\n            return [Notion(content=messages, role=str(self.role.HUMAN.value))]\n        elif isinstance(messages, Notion):\n            return [messages]\n        elif isinstance(messages, Idearium):\n            return messages.notions\n        elif isinstance(messages, list):\n            return [\n                (\n                    Notion(content=msg, role=str(self.role.HUMAN.value))\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        raise ValueError(f\"Unsupported message type: {type(messages)}\")\n\n    def _process_generation(\n        self, responses: List[Notion], is_async=False\n    ) -&gt; List[Notion]:\n        \"\"\"Wrapper around shared logic between generate and agenerate.\"\"\"\n        response = responses[0]\n        # logger.debug(f\"Response: {response}\")\n        if response.chat_role == ChatRole.TOOL_CALL:\n            # logger.debug(\"Tool call detected\")\n            # Add the tool call to the idearium\n            self.idearium.append(response)\n            # Call generate again with the tool response\n            tool_calls = ToolCalls.model_validate_json(\n                '{\"list\": ' + response.content + \"}\"\n            )\n            tool_response = self._use_tools(tool_calls)\n            # logger.debug(f\"Tool response: {tool_response}\")\n            if is_async:\n                return self.agenerate(tool_response)\n            else:\n                return self.generate(tool_response)\n        else:\n            return responses\n\n    def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Generates a response to the given messages by calling the\n        underlying model's generate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = self.model.generate(self.idearium, **kwargs)\n        result = self._process_generation(responses)\n\n        if self.auto_append_response:\n            self.idearium.extend(result)\n\n        return result\n\n    async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Asynchronously generates a response to the given messages by calling the\n        underlying model's agenerate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = await self.model.agenerate(self.idearium, **kwargs)\n        result = self._process_generation(responses, True)\n        r = await result if asyncio.iscoroutine(result) else result\n\n        if self.auto_append_response:\n            self.idearium.extend(r)\n\n        return r\n\n    def _process_tool_calls(self, tool_calls: ToolCalls):\n        \"\"\"\n        Processes tool calls and returns the tool response.\n\n        Args:\n            tool_calls (ToolCalls): The tool calls to process.\n\n        Returns:\n            Optional[ToolCalls]: The tool response. If None, no tool calls were found.\n        \"\"\"\n        for i, tool_call in enumerate(tool_calls.list):\n            if not tool_call.id.startswith(\"call_\"):\n                # Something went wrong and this tool call is not valid\n                tool_calls.list.pop(i)\n                logger.error(\n                    \"Invalid tool call: \"\n                    + f\"{tool_call.model_dump_json(exclude_none=True)}\"\n                )\n\n        tc_dump = tool_calls.model_dump(exclude_none=True)\n        if tc_dump.get(\"list\"):\n            logger.debug(f\"Tool calls: {tc_dump}\")\n\n            # Create a new notion from the tool calls\n            tc_notion = Notion(\n                content=json.dumps(tc_dump.get(\"list\")),\n                role=str(ChatRole.TOOL_CALL.value),\n            )\n\n            # Add the tool call to the idearium\n            self.idearium.append(tc_notion)\n            # Call stream again with the tool response\n            tool_response = self._use_tools(tool_calls)\n            return tool_response\n        else:\n            logger.error(\"No tool calls found\")\n            return None\n\n    def stream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Streams a response to the given prompt by calling the\n        underlying model's stream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.stream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in stream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                for r in self.stream(tool_response):\n                    yield r\n\n    async def astream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Asynchronously streams a response to the given prompt by calling the\n        underlying model's astream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.astream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        async for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in astream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                async for r in self.astream(tool_response):\n                    yield r\n</code></pre>"},{"location":"api/core/#SilverLingua.core.templates.Agent-attributes","title":"Attributes","text":"<code>auto_append_response: bool = True</code> <code>class-attribute</code> <code>instance-attribute</code> <p>Whether to automatically append the response to the idearium after generating a response.</p> <code>idearium: Idearium</code> <code>instance-attribute</code> <p>The Idearium used by the agent.</p> <code>model: Model</code> <code>property</code> <p>The model used by the agent.</p> <code>role: ChatRole</code> <code>property</code> <p>The ChatRole object for the model.</p> <code>tools: List[Tool]</code> <code>instance-attribute</code> <p>The tools used by the agent.</p> <p>WARNING: Do not modify this list directly. Use <code>add_tool</code>, <code>add_tools</code>, and <code>remove_tool</code> instead.</p>"},{"location":"api/core/#SilverLingua.core.templates.Agent-functions","title":"Functions","text":"<code>__init__(model, idearium=None, tools=None, auto_append_response=True)</code> <p>Initializes the agent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use.</p> required <code>idearium</code> <code>Idearium</code> <p>The idearium to use. If None, a new one will be created.</p> <code>None</code> <code>tools</code> <code>List[Tool]</code> <p>The tools to use.</p> <code>None</code> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def __init__(\n    self,\n    model: Model,\n    idearium: Optional[Idearium] = None,\n    tools: Optional[List[Tool]] = None,\n    auto_append_response: bool = True,\n):\n    \"\"\"\n    Initializes the agent.\n\n    Args:\n        model (Model): The model to use.\n        idearium (Idearium, optional): The idearium to use.\n            If None, a new one will be created.\n        tools (List[Tool], optional): The tools to use.\n    \"\"\"\n    super().__init__(\n        model=model,\n        idearium=idearium\n        or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n        tools=tools or [],\n        auto_append_response=auto_append_response,\n    )\n</code></pre> <code>add_tool(tool)</code> <p>Adds a tool to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tool(self, tool: Tool) -&gt; None:\n    \"\"\"\n    Adds a tool to the agent.\n    \"\"\"\n    self.tools.append(tool)\n    self._bind_tools()\n</code></pre> <code>add_tools(tools)</code> <p>Adds a list of tools to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tools(self, tools: List[Tool]) -&gt; None:\n    \"\"\"\n    Adds a list of tools to the agent.\n    \"\"\"\n    self.tools.extend(tools)\n    self._bind_tools()\n</code></pre> <code>agenerate(messages, **kwargs)</code> <code>async</code> <p>Asynchronously generates a response to the given messages by calling the underlying model's agenerate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Asynchronously generates a response to the given messages by calling the\n    underlying model's agenerate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = await self.model.agenerate(self.idearium, **kwargs)\n    result = self._process_generation(responses, True)\n    r = await result if asyncio.iscoroutine(result) else result\n\n    if self.auto_append_response:\n        self.idearium.extend(r)\n\n    return r\n</code></pre> <code>astream(messages, **kwargs)</code> <code>async</code> <p>Asynchronously streams a response to the given prompt by calling the underlying model's astream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def astream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Asynchronously streams a response to the given prompt by calling the\n    underlying model's astream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.astream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    async for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in astream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            async for r in self.astream(tool_response):\n                yield r\n</code></pre> <code>generate(messages, **kwargs)</code> <p>Generates a response to the given messages by calling the underlying model's generate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Generates a response to the given messages by calling the\n    underlying model's generate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = self.model.generate(self.idearium, **kwargs)\n    result = self._process_generation(responses)\n\n    if self.auto_append_response:\n        self.idearium.extend(result)\n\n    return result\n</code></pre> <code>remove_tool(name)</code> <p>Removes a tool from the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def remove_tool(self, name: str) -&gt; None:\n    \"\"\"\n    Removes a tool from the agent.\n    \"\"\"\n    for i, tool in enumerate(self.tools):\n        if tool.name == name:\n            self.tools.pop(i)\n            break\n    self._bind_tools()\n</code></pre> <code>stream(messages, **kwargs)</code> <p>Streams a response to the given prompt by calling the underlying model's stream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def stream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Streams a response to the given prompt by calling the\n    underlying model's stream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.stream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in stream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            for r in self.stream(tool_response):\n                yield r\n</code></pre>"},{"location":"api/core/#SilverLingua.core.templates.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract class for all Large Language Models.</p> <p>This class outlines a standardized lifecycle for interacting with LLMs, aimed at ensuring a consistent process for message trimming, pre-processing, preparing requests for the model, invoking the model, standardizing the response, and post-processing. The lifecycle is as follows:</p> <p>Lifecycle: 1. Pre-processing (_preprocess): Performs any necessary transformations or     adjustments to the messages prior to trimming or preparing them for model input.     (Optional)</p> <ol> <li> <p>Preparing Request (_format_request): Converts the pre-processed messages     into a format suitable for model input.</p> </li> <li> <p>Model Invocation (_call or _acall): Feeds the prepared input to the LLM and     retrieves the raw model output. There should be both synchronous and     asynchronous versions available.</p> </li> <li> <p>Standardizing Response (_standardize_response): Transforms the raw model     output into a consistent response format suitable for further processing or     delivery.</p> </li> <li> <p>Post-processing (_postprocess): Performs any final transformations or     adjustments to the standardized responses, making them ready for delivery.     (Optional)</p> </li> </ol> <p>Subclasses should implement each of the non-optional lifecycle steps in accordance with the specific requirements and behaviors of the target LLM.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>class Model(BaseModel, ABC):\n    \"\"\"\n    Abstract class for all Large Language Models.\n\n    This class outlines a standardized lifecycle for interacting with LLMs,\n    aimed at ensuring a consistent process for message trimming, pre-processing,\n    preparing requests for the model, invoking the model, standardizing the response,\n    and post-processing. The lifecycle is as follows:\n\n    Lifecycle:\n    1. Pre-processing (_preprocess): Performs any necessary transformations or\n        adjustments to the messages prior to trimming or preparing them for model input.\n        (Optional)\n\n    2. Preparing Request (_format_request): Converts the pre-processed messages\n        into a format suitable for model input.\n\n    3. Model Invocation (_call or _acall): Feeds the prepared input to the LLM and\n        retrieves the raw model output. There should be both synchronous and\n        asynchronous versions available.\n\n    4. Standardizing Response (_standardize_response): Transforms the raw model\n        output into a consistent response format suitable for further processing or\n        delivery.\n\n    5. Post-processing (_postprocess): Performs any final transformations or\n        adjustments to the standardized responses, making them ready for delivery.\n        (Optional)\n\n    Subclasses should implement each of the non-optional lifecycle steps in accordance\n    with the specific requirements and behaviors of the target LLM.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    max_response: int = Field(default=0)\n    api_key: str\n    name: str\n    #\n    role: Type[ChatRole]\n    type: ModelType\n    llm: Callable\n    llm_async: Callable\n    can_stream: bool\n    tokenizer: Tokenizer\n\n    @property\n    @abstractmethod\n    def max_tokens(self) -&gt; int:\n        \"\"\"\n        The maximum number of tokens that can be fed to the model at once.\n        \"\"\"\n        pass\n\n    def _process_input(self, messages: Messages) -&gt; Idearium:\n        if isinstance(messages, str):\n            notions = [Notion(content=messages, role=self.role.HUMAN)]\n        elif isinstance(messages, Notion):\n            notions = [messages]\n        elif isinstance(messages, Idearium):\n            return messages  # Already an Idearium, no need to convert\n        elif isinstance(messages, list):\n            notions = [\n                (\n                    Notion(content=msg, role=self.role.HUMAN)\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        else:\n            raise ValueError(\"Invalid input type for messages\")\n\n        return Idearium(self.tokenizer, self.max_tokens, notions)\n\n    def _convert_role(self, role: ChatRole) -&gt; str:\n        \"\"\"\n        Converts the standard ChatRole to the model-specific role.\n        \"\"\"\n        return str(self.role[role.name].value)\n\n    def _preprocess(self, messages: List[Notion]) -&gt; List[Notion]:\n        \"\"\"\n        Preprocesses the List of `Notions`, applying any effects necessary\n        before being prepped for input into an API.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        return [\n            Notion(msg.content, self._convert_role(msg.chat_role), msg.persistent)\n            for msg in messages\n        ]\n\n    @abstractmethod\n    def _format_request(\n        self, messages: List[Notion], *args, **kwargs\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Formats the List of `Notions` into a format suitable for model input.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _standardize_response(\n        self, response: Union[object, str, List[any]], *args, **kwargs\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Standardizes the raw response from the model into a List of Notions.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _postprocess(self, response: List[Notion], *args, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Postprocesses the response from the model, applying any final effects\n        before being returned.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _retry_call(\n        self,\n        input: Union[str, object, List[any]],\n        e: Exception,\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Retry logic for API calls used by `_common_call_logic`.\n        \"\"\"\n        pass\n\n    def _common_call_logic(\n        self,\n        input: Union[str, object, List[any]],\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        if input is None:\n            raise ValueError(\"No input provided.\")\n\n        try:\n            out = api_call(messages=input)\n            return out\n        except Exception as e:\n            logger.error(f\"Error calling LLM API: {e}\")\n            if retries &gt;= 3:\n                raise e\n\n            return self._retry_call(input, e, api_call, retries=retries)\n\n    def _call(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the raw response.\n\n        Should behave exactly as `_acall` does, but synchronously.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n\n        def api_call(**kwargs_):\n            return self.llm(**kwargs_, **kwargs)\n\n        return self._common_call_logic(input, api_call, retries)\n\n    async def _acall(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the\n        raw response asynchronously.\n\n        Should behave exactly as `_call` does, but asynchronously.\n\n        This is a lifecycle method that is called by the `agenerate` method.\n        \"\"\"\n\n        async def api_call(**kwargs_):\n            return await self.llm_async(**kwargs_, **kwargs)\n\n        result = self._common_call_logic(input, api_call, retries)\n        if asyncio.iscoroutine(result):\n            return await result\n        return result\n\n    def _common_generate_logic(\n        self,\n        messages: Messages,\n        is_async=False,\n        **kwargs,\n    ):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        call_method = self._acall if is_async else self._call\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n\n        if is_async:\n\n            async def call():\n                response = await call_method(input, **kwargs)\n                output = self._standardize_response(response)\n                return self._postprocess(output)\n\n            return call()\n        else:\n            response = call_method(input, **kwargs)\n            output = self._standardize_response(response)\n            return self._postprocess(output)\n\n    @abstractmethod\n    def generate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def agenerate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response\n        asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating async responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    def _common_stream_logic(self, messages: Messages):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        if not self.can_stream:\n            raise ValueError(\n                \"This model does not support streaming. \"\n                + \"Please use the `generate` method instead.\"\n            )\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n        return input\n\n    @abstractmethod\n    def stream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def astream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time, asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/core/#SilverLingua.core.templates.Model-attributes","title":"Attributes","text":"<code>max_tokens: int</code> <code>abstractmethod</code> <code>property</code> <p>The maximum number of tokens that can be fed to the model at once.</p>"},{"location":"api/core/#SilverLingua.core.templates.Model-functions","title":"Functions","text":"<code>agenerate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Calls the model with the given messages and returns the response asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating async responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response\n    asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating async responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>astream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Streams the model with the given messages and returns the response, one token at a time, asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def astream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time, asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre> <code>generate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Calls the model with the given messages and returns the response.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>stream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Streams the model with the given messages and returns the response, one token at a time.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef stream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#SilverLingua.core.templates-modules","title":"Modules","text":""},{"location":"api/core/#SilverLingua.core.templates.agent","title":"<code>agent</code>","text":""},{"location":"api/core/#SilverLingua.core.templates.agent-attributes","title":"Attributes","text":""},{"location":"api/core/#SilverLingua.core.templates.agent-classes","title":"Classes","text":"<code>Agent</code> <p>               Bases: <code>BaseModel</code></p> <p>A wrapper around a model that utilizes an Idearium and a set of Tools.</p> <p>This is a base class not meant to be used directly. It is meant to be subclassed by specific model implementations.</p> <p>However, there is limited boilerplate. The only thing that needs to be redefined in subclasses is the <code>_bind_tools</code> method.</p> <p>Additionally, the <code>_use_tools</code> method is a common method to redefine.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>class Agent(BaseModel):\n    \"\"\"\n    A wrapper around a model that utilizes an Idearium and a set of Tools.\n\n    This is a base class not meant to be used directly. It is meant to be\n    subclassed by specific model implementations.\n\n    However, there is limited boilerplate. The only thing that needs to be\n    redefined in subclasses is the `_bind_tools` method.\n\n    Additionally, the `_use_tools` method is a common method to redefine.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    model: Model\n    idearium: Idearium\n    \"\"\"\n    The Idearium used by the agent.\n    \"\"\"\n    tools: List[Tool]\n    \"\"\"\n    The tools used by the agent.\n\n    WARNING: Do not modify this list directly. Use `add_tool`, `add_tools`,\n    and `remove_tool` instead.\n    \"\"\"\n    auto_append_response: bool = True\n    \"\"\"\n    Whether to automatically append the response to the idearium after\n    generating a response.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Model,\n        idearium: Optional[Idearium] = None,\n        tools: Optional[List[Tool]] = None,\n        auto_append_response: bool = True,\n    ):\n        \"\"\"\n        Initializes the agent.\n\n        Args:\n            model (Model): The model to use.\n            idearium (Idearium, optional): The idearium to use.\n                If None, a new one will be created.\n            tools (List[Tool], optional): The tools to use.\n        \"\"\"\n        super().__init__(\n            model=model,\n            idearium=idearium\n            or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n            tools=tools or [],\n            auto_append_response=auto_append_response,\n        )\n\n    def model_post_init(self, __content):\n        self._bind_tools()\n\n    @property\n    def model(self) -&gt; Model:\n        \"\"\"\n        The model used by the agent.\n        \"\"\"\n        return self.model\n\n    @property\n    def role(self) -&gt; ChatRole:\n        \"\"\"\n        The ChatRole object for the model.\n        \"\"\"\n        return self.model.role\n\n    def _find_tool(self, name: str) -&gt; Tool | None:\n        \"\"\"\n        Finds a tool by name.\n        \"\"\"\n        for t in self.tools:\n            if t.name == name:\n                return t\n        return None\n\n    def _use_tools(self, tool_calls: ToolCalls) -&gt; List[Notion]:\n        \"\"\"\n        Uses Tools based on the given ToolCalls, returning Notions\n        containing ToolCallResponses.\n\n        Args:\n            tool_calls (ToolCalls): The ToolCalls to use.\n\n        Returns:\n            List[Notion]: The Notions containing ToolCallResponses.\n                Each Notion will have a role of ChatRole.TOOL_RESPONSE.\n        \"\"\"\n        responses: List[Notion] = []\n        for tool_call in tool_calls.list:\n            tool = self._find_tool(tool_call.function.name)\n            if tool is not None:\n                tc_function_response = {}\n                with contextlib.suppress(json.JSONDecodeError):\n                    tc_function_response = json.loads(tool_call.function.arguments)\n\n                tc_response = ToolCallResponse.from_tool_call(\n                    tool_call=tool_call, response=tool(**tc_function_response)\n                )\n                responses.append(\n                    Notion(\n                        content=tc_response.model_dump_json(exclude_none=True),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n            else:\n                responses.append(\n                    Notion(\n                        content=json.dumps(\n                            {\n                                \"tool_call_id\": tool_call.id,\n                                \"content\": \"Tool not found\",\n                                \"name\": \"error\",\n                            }\n                        ),\n                        role=str(self.role.TOOL_RESPONSE.value),\n                    )\n                )\n        return responses\n\n    def _bind_tools(self) -&gt; None:\n        \"\"\"\n        Called at the end of __init__ to bind the tools to the model.\n\n        This MUST be redefined in subclasses to dictate how\n        the tools are bound to the model.\n\n        Example:\n        ```python\n        # From OpenAIChatAgent\n        def _bind_tools(self) -&gt; None:\n            m_tools: List[ChatCompletionToolParam] = [\n                {\"type\": \"function\", \"function\": tool.description}\n                for tool in self.tools\n            ]\n\n            if len(m_tools) &gt; 0:\n                self.model.tools = m_tools\n        ```\n        \"\"\"\n        pass\n\n    def add_tool(self, tool: Tool) -&gt; None:\n        \"\"\"\n        Adds a tool to the agent.\n        \"\"\"\n        self.tools.append(tool)\n        self._bind_tools()\n\n    def add_tools(self, tools: List[Tool]) -&gt; None:\n        \"\"\"\n        Adds a list of tools to the agent.\n        \"\"\"\n        self.tools.extend(tools)\n        self._bind_tools()\n\n    def remove_tool(self, name: str) -&gt; None:\n        \"\"\"\n        Removes a tool from the agent.\n        \"\"\"\n        for i, tool in enumerate(self.tools):\n            if tool.name == name:\n                self.tools.pop(i)\n                break\n        self._bind_tools()\n\n    def _process_messages(self, messages: Messages) -&gt; List[Notion]:\n        \"\"\"Convert various message types into a list of Notions.\"\"\"\n        if isinstance(messages, str):\n            return [Notion(content=messages, role=str(self.role.HUMAN.value))]\n        elif isinstance(messages, Notion):\n            return [messages]\n        elif isinstance(messages, Idearium):\n            return messages.notions\n        elif isinstance(messages, list):\n            return [\n                (\n                    Notion(content=msg, role=str(self.role.HUMAN.value))\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        raise ValueError(f\"Unsupported message type: {type(messages)}\")\n\n    def _process_generation(\n        self, responses: List[Notion], is_async=False\n    ) -&gt; List[Notion]:\n        \"\"\"Wrapper around shared logic between generate and agenerate.\"\"\"\n        response = responses[0]\n        # logger.debug(f\"Response: {response}\")\n        if response.chat_role == ChatRole.TOOL_CALL:\n            # logger.debug(\"Tool call detected\")\n            # Add the tool call to the idearium\n            self.idearium.append(response)\n            # Call generate again with the tool response\n            tool_calls = ToolCalls.model_validate_json(\n                '{\"list\": ' + response.content + \"}\"\n            )\n            tool_response = self._use_tools(tool_calls)\n            # logger.debug(f\"Tool response: {tool_response}\")\n            if is_async:\n                return self.agenerate(tool_response)\n            else:\n                return self.generate(tool_response)\n        else:\n            return responses\n\n    def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Generates a response to the given messages by calling the\n        underlying model's generate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = self.model.generate(self.idearium, **kwargs)\n        result = self._process_generation(responses)\n\n        if self.auto_append_response:\n            self.idearium.extend(result)\n\n        return result\n\n    async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Asynchronously generates a response to the given messages by calling the\n        underlying model's agenerate method and checking/actualizing tool usage.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            List[Notion]: A list of responses to the given messages.\n                (Many times there will only be one response.)\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        responses = await self.model.agenerate(self.idearium, **kwargs)\n        result = self._process_generation(responses, True)\n        r = await result if asyncio.iscoroutine(result) else result\n\n        if self.auto_append_response:\n            self.idearium.extend(r)\n\n        return r\n\n    def _process_tool_calls(self, tool_calls: ToolCalls):\n        \"\"\"\n        Processes tool calls and returns the tool response.\n\n        Args:\n            tool_calls (ToolCalls): The tool calls to process.\n\n        Returns:\n            Optional[ToolCalls]: The tool response. If None, no tool calls were found.\n        \"\"\"\n        for i, tool_call in enumerate(tool_calls.list):\n            if not tool_call.id.startswith(\"call_\"):\n                # Something went wrong and this tool call is not valid\n                tool_calls.list.pop(i)\n                logger.error(\n                    \"Invalid tool call: \"\n                    + f\"{tool_call.model_dump_json(exclude_none=True)}\"\n                )\n\n        tc_dump = tool_calls.model_dump(exclude_none=True)\n        if tc_dump.get(\"list\"):\n            logger.debug(f\"Tool calls: {tc_dump}\")\n\n            # Create a new notion from the tool calls\n            tc_notion = Notion(\n                content=json.dumps(tc_dump.get(\"list\")),\n                role=str(ChatRole.TOOL_CALL.value),\n            )\n\n            # Add the tool call to the idearium\n            self.idearium.append(tc_notion)\n            # Call stream again with the tool response\n            tool_response = self._use_tools(tool_calls)\n            return tool_response\n        else:\n            logger.error(\"No tool calls found\")\n            return None\n\n    def stream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Streams a response to the given prompt by calling the\n        underlying model's stream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.stream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in stream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                for r in self.stream(tool_response):\n                    yield r\n\n    async def astream(self, messages: Messages, **kwargs):\n        \"\"\"\n        Asynchronously streams a response to the given prompt by calling the\n        underlying model's astream method and checking/actualizing tool usage.\n\n        NOTE: Will raise an exception if the underlying model does not support\n        streaming.\n\n        Args:\n            messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n            The messages to respond to.\n\n        Returns:\n            Generator[Notion, Any, None]: A generator of responses to the given\n                messages.\n        \"\"\"\n        self.idearium.extend(self._process_messages(messages))\n        response_stream = self.model.astream(self.idearium, **kwargs)\n\n        # Process stream directly\n        tool_calls: Optional[ToolCalls] = None\n\n        async for r in response_stream:\n            if r.chat_role == ChatRole.TOOL_CALL:\n                logger.debug(f\"Tool call detected: {r.content}\")\n                tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n                tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n                continue\n            elif r.content is not None:\n                logger.debug(f\"Got chunk in astream: {r.content!r}\")\n                if self.auto_append_response:\n                    self.idearium.append(r)\n                yield r\n\n        # Handle tool calls if any\n        if tool_calls is not None:\n            logger.debug(\"Moving to tool response stream\")\n            tool_response = self._process_tool_calls(tool_calls)\n            if tool_response is not None:\n                async for r in self.astream(tool_response):\n                    yield r\n</code></pre> Attributes <code>auto_append_response: bool = True</code> <code>class-attribute</code> <code>instance-attribute</code> <p>Whether to automatically append the response to the idearium after generating a response.</p> <code>idearium: Idearium</code> <code>instance-attribute</code> <p>The Idearium used by the agent.</p> <code>model: Model</code> <code>property</code> <p>The model used by the agent.</p> <code>role: ChatRole</code> <code>property</code> <p>The ChatRole object for the model.</p> <code>tools: List[Tool]</code> <code>instance-attribute</code> <p>The tools used by the agent.</p> <p>WARNING: Do not modify this list directly. Use <code>add_tool</code>, <code>add_tools</code>, and <code>remove_tool</code> instead.</p> Functions <code>__init__(model, idearium=None, tools=None, auto_append_response=True)</code> <p>Initializes the agent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to use.</p> required <code>idearium</code> <code>Idearium</code> <p>The idearium to use. If None, a new one will be created.</p> <code>None</code> <code>tools</code> <code>List[Tool]</code> <p>The tools to use.</p> <code>None</code> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def __init__(\n    self,\n    model: Model,\n    idearium: Optional[Idearium] = None,\n    tools: Optional[List[Tool]] = None,\n    auto_append_response: bool = True,\n):\n    \"\"\"\n    Initializes the agent.\n\n    Args:\n        model (Model): The model to use.\n        idearium (Idearium, optional): The idearium to use.\n            If None, a new one will be created.\n        tools (List[Tool], optional): The tools to use.\n    \"\"\"\n    super().__init__(\n        model=model,\n        idearium=idearium\n        or Idearium(tokenizer=model.tokenizer, max_tokens=model.max_tokens),\n        tools=tools or [],\n        auto_append_response=auto_append_response,\n    )\n</code></pre> <code>add_tool(tool)</code> <p>Adds a tool to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tool(self, tool: Tool) -&gt; None:\n    \"\"\"\n    Adds a tool to the agent.\n    \"\"\"\n    self.tools.append(tool)\n    self._bind_tools()\n</code></pre> <code>add_tools(tools)</code> <p>Adds a list of tools to the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def add_tools(self, tools: List[Tool]) -&gt; None:\n    \"\"\"\n    Adds a list of tools to the agent.\n    \"\"\"\n    self.tools.extend(tools)\n    self._bind_tools()\n</code></pre> <code>agenerate(messages, **kwargs)</code> <code>async</code> <p>Asynchronously generates a response to the given messages by calling the underlying model's agenerate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def agenerate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Asynchronously generates a response to the given messages by calling the\n    underlying model's agenerate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = await self.model.agenerate(self.idearium, **kwargs)\n    result = self._process_generation(responses, True)\n    r = await result if asyncio.iscoroutine(result) else result\n\n    if self.auto_append_response:\n        self.idearium.extend(r)\n\n    return r\n</code></pre> <code>astream(messages, **kwargs)</code> <code>async</code> <p>Asynchronously streams a response to the given prompt by calling the underlying model's astream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>async def astream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Asynchronously streams a response to the given prompt by calling the\n    underlying model's astream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.astream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    async for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in astream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            async for r in self.astream(tool_response):\n                yield r\n</code></pre> <code>generate(messages, **kwargs)</code> <p>Generates a response to the given messages by calling the underlying model's generate method and checking/actualizing tool usage.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <code>List[Notion]</code> <p>List[Notion]: A list of responses to the given messages. (Many times there will only be one response.)</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def generate(self, messages: Messages, **kwargs) -&gt; List[Notion]:\n    \"\"\"\n    Generates a response to the given messages by calling the\n    underlying model's generate method and checking/actualizing tool usage.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        List[Notion]: A list of responses to the given messages.\n            (Many times there will only be one response.)\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    responses = self.model.generate(self.idearium, **kwargs)\n    result = self._process_generation(responses)\n\n    if self.auto_append_response:\n        self.idearium.extend(result)\n\n    return result\n</code></pre> <code>remove_tool(name)</code> <p>Removes a tool from the agent.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def remove_tool(self, name: str) -&gt; None:\n    \"\"\"\n    Removes a tool from the agent.\n    \"\"\"\n    for i, tool in enumerate(self.tools):\n        if tool.name == name:\n            self.tools.pop(i)\n            break\n    self._bind_tools()\n</code></pre> <code>stream(messages, **kwargs)</code> <p>Streams a response to the given prompt by calling the underlying model's stream method and checking/actualizing tool usage.</p> <p>NOTE: Will raise an exception if the underlying model does not support streaming.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> required <p>Returns:</p> Type Description <p>Generator[Notion, Any, None]: A generator of responses to the given messages.</p> Source code in <code>src\\SilverLingua\\core\\templates\\agent.py</code> <pre><code>def stream(self, messages: Messages, **kwargs):\n    \"\"\"\n    Streams a response to the given prompt by calling the\n    underlying model's stream method and checking/actualizing tool usage.\n\n    NOTE: Will raise an exception if the underlying model does not support\n    streaming.\n\n    Args:\n        messages (Union[str, Notion, Idearium, List[Union[str, Notion]]]):\n        The messages to respond to.\n\n    Returns:\n        Generator[Notion, Any, None]: A generator of responses to the given\n            messages.\n    \"\"\"\n    self.idearium.extend(self._process_messages(messages))\n    response_stream = self.model.stream(self.idearium, **kwargs)\n\n    # Process stream directly\n    tool_calls: Optional[ToolCalls] = None\n\n    for r in response_stream:\n        if r.chat_role == ChatRole.TOOL_CALL:\n            logger.debug(f\"Tool call detected: {r.content}\")\n            tc_chunks = ToolCalls.model_validate_json('{\"list\": ' + r.content + \"}\")\n            tool_calls = tool_calls and tool_calls.concat(tc_chunks) or tc_chunks\n            continue\n        elif r.content is not None:\n            logger.debug(f\"Got chunk in stream: {r.content!r}\")\n            if self.auto_append_response:\n                self.idearium.append(r)\n            yield r\n\n    # Handle tool calls if any\n    if tool_calls is not None:\n        logger.debug(\"Moving to tool response stream\")\n        tool_response = self._process_tool_calls(tool_calls)\n        if tool_response is not None:\n            for r in self.stream(tool_response):\n                yield r\n</code></pre>"},{"location":"api/core/#SilverLingua.core.templates.model","title":"<code>model</code>","text":""},{"location":"api/core/#SilverLingua.core.templates.model-attributes","title":"Attributes","text":"<code>Messages = Union[str, Notion, Idearium, List[Union[str, Notion]]]</code> <code>module-attribute</code> <p>A type alias for the various types of messages that can be passed to a model.</p>"},{"location":"api/core/#SilverLingua.core.templates.model-classes","title":"Classes","text":"<code>Model</code> <p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract class for all Large Language Models.</p> <p>This class outlines a standardized lifecycle for interacting with LLMs, aimed at ensuring a consistent process for message trimming, pre-processing, preparing requests for the model, invoking the model, standardizing the response, and post-processing. The lifecycle is as follows:</p> <p>Lifecycle: 1. Pre-processing (_preprocess): Performs any necessary transformations or     adjustments to the messages prior to trimming or preparing them for model input.     (Optional)</p> <ol> <li> <p>Preparing Request (_format_request): Converts the pre-processed messages     into a format suitable for model input.</p> </li> <li> <p>Model Invocation (_call or _acall): Feeds the prepared input to the LLM and     retrieves the raw model output. There should be both synchronous and     asynchronous versions available.</p> </li> <li> <p>Standardizing Response (_standardize_response): Transforms the raw model     output into a consistent response format suitable for further processing or     delivery.</p> </li> <li> <p>Post-processing (_postprocess): Performs any final transformations or     adjustments to the standardized responses, making them ready for delivery.     (Optional)</p> </li> </ol> <p>Subclasses should implement each of the non-optional lifecycle steps in accordance with the specific requirements and behaviors of the target LLM.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>class Model(BaseModel, ABC):\n    \"\"\"\n    Abstract class for all Large Language Models.\n\n    This class outlines a standardized lifecycle for interacting with LLMs,\n    aimed at ensuring a consistent process for message trimming, pre-processing,\n    preparing requests for the model, invoking the model, standardizing the response,\n    and post-processing. The lifecycle is as follows:\n\n    Lifecycle:\n    1. Pre-processing (_preprocess): Performs any necessary transformations or\n        adjustments to the messages prior to trimming or preparing them for model input.\n        (Optional)\n\n    2. Preparing Request (_format_request): Converts the pre-processed messages\n        into a format suitable for model input.\n\n    3. Model Invocation (_call or _acall): Feeds the prepared input to the LLM and\n        retrieves the raw model output. There should be both synchronous and\n        asynchronous versions available.\n\n    4. Standardizing Response (_standardize_response): Transforms the raw model\n        output into a consistent response format suitable for further processing or\n        delivery.\n\n    5. Post-processing (_postprocess): Performs any final transformations or\n        adjustments to the standardized responses, making them ready for delivery.\n        (Optional)\n\n    Subclasses should implement each of the non-optional lifecycle steps in accordance\n    with the specific requirements and behaviors of the target LLM.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n    #\n    max_response: int = Field(default=0)\n    api_key: str\n    name: str\n    #\n    role: Type[ChatRole]\n    type: ModelType\n    llm: Callable\n    llm_async: Callable\n    can_stream: bool\n    tokenizer: Tokenizer\n\n    @property\n    @abstractmethod\n    def max_tokens(self) -&gt; int:\n        \"\"\"\n        The maximum number of tokens that can be fed to the model at once.\n        \"\"\"\n        pass\n\n    def _process_input(self, messages: Messages) -&gt; Idearium:\n        if isinstance(messages, str):\n            notions = [Notion(content=messages, role=self.role.HUMAN)]\n        elif isinstance(messages, Notion):\n            notions = [messages]\n        elif isinstance(messages, Idearium):\n            return messages  # Already an Idearium, no need to convert\n        elif isinstance(messages, list):\n            notions = [\n                (\n                    Notion(content=msg, role=self.role.HUMAN)\n                    if isinstance(msg, str)\n                    else msg\n                )\n                for msg in messages\n            ]\n        else:\n            raise ValueError(\"Invalid input type for messages\")\n\n        return Idearium(self.tokenizer, self.max_tokens, notions)\n\n    def _convert_role(self, role: ChatRole) -&gt; str:\n        \"\"\"\n        Converts the standard ChatRole to the model-specific role.\n        \"\"\"\n        return str(self.role[role.name].value)\n\n    def _preprocess(self, messages: List[Notion]) -&gt; List[Notion]:\n        \"\"\"\n        Preprocesses the List of `Notions`, applying any effects necessary\n        before being prepped for input into an API.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        return [\n            Notion(msg.content, self._convert_role(msg.chat_role), msg.persistent)\n            for msg in messages\n        ]\n\n    @abstractmethod\n    def _format_request(\n        self, messages: List[Notion], *args, **kwargs\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Formats the List of `Notions` into a format suitable for model input.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _standardize_response(\n        self, response: Union[object, str, List[any]], *args, **kwargs\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Standardizes the raw response from the model into a List of Notions.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _postprocess(self, response: List[Notion], *args, **kwargs) -&gt; List[Notion]:\n        \"\"\"\n        Postprocesses the response from the model, applying any final effects\n        before being returned.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _retry_call(\n        self,\n        input: Union[str, object, List[any]],\n        e: Exception,\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        \"\"\"\n        Retry logic for API calls used by `_common_call_logic`.\n        \"\"\"\n        pass\n\n    def _common_call_logic(\n        self,\n        input: Union[str, object, List[any]],\n        api_call: Callable,\n        retries: int = 0,\n    ) -&gt; Union[str, object]:\n        if input is None:\n            raise ValueError(\"No input provided.\")\n\n        try:\n            out = api_call(messages=input)\n            return out\n        except Exception as e:\n            logger.error(f\"Error calling LLM API: {e}\")\n            if retries &gt;= 3:\n                raise e\n\n            return self._retry_call(input, e, api_call, retries=retries)\n\n    def _call(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the raw response.\n\n        Should behave exactly as `_acall` does, but synchronously.\n\n        This is a lifecycle method that is called by the `generate` method.\n        \"\"\"\n\n        def api_call(**kwargs_):\n            return self.llm(**kwargs_, **kwargs)\n\n        return self._common_call_logic(input, api_call, retries)\n\n    async def _acall(\n        self, input: Union[str, object, List[any]], retries: int = 0, **kwargs\n    ) -&gt; object:\n        \"\"\"\n        Calls the model with the given input and returns the\n        raw response asynchronously.\n\n        Should behave exactly as `_call` does, but asynchronously.\n\n        This is a lifecycle method that is called by the `agenerate` method.\n        \"\"\"\n\n        async def api_call(**kwargs_):\n            return await self.llm_async(**kwargs_, **kwargs)\n\n        result = self._common_call_logic(input, api_call, retries)\n        if asyncio.iscoroutine(result):\n            return await result\n        return result\n\n    def _common_generate_logic(\n        self,\n        messages: Messages,\n        is_async=False,\n        **kwargs,\n    ):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        call_method = self._acall if is_async else self._call\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n\n        if is_async:\n\n            async def call():\n                response = await call_method(input, **kwargs)\n                output = self._standardize_response(response)\n                return self._postprocess(output)\n\n            return call()\n        else:\n            response = call_method(input, **kwargs)\n            output = self._standardize_response(response)\n            return self._postprocess(output)\n\n    @abstractmethod\n    def generate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def agenerate(\n        self,\n        messages: Messages,\n        *args,\n        **kwargs,\n    ) -&gt; List[Notion]:\n        \"\"\"\n        Calls the model with the given messages and returns the response\n        asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        This is the primary method for generating async responses from the model,\n        and is responsible for calling all of the lifecycle methods.\n        \"\"\"\n        pass\n\n    def _common_stream_logic(self, messages: Messages):\n        if messages is None:\n            raise ValueError(\"No messages provided.\")\n\n        if not self.can_stream:\n            raise ValueError(\n                \"This model does not support streaming. \"\n                + \"Please use the `generate` method instead.\"\n            )\n\n        idearium = self._process_input(messages)\n        input = self._format_request(self._preprocess(idearium))\n        return input\n\n    @abstractmethod\n    def stream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def astream(\n        self, messages: Messages, *args, **kwargs\n    ) -&gt; Generator[Notion, Any, None]:\n        \"\"\"\n        Streams the model with the given messages and returns the response,\n        one token at a time, asynchronously.\n\n        Messages can be any of:\n        string, list of strings, Notion, list of Notions, or Idearium.\n\n        If the model cannot be streamed, this will raise an exception.\n        \"\"\"\n        pass\n</code></pre> Attributes <code>max_tokens: int</code> <code>abstractmethod</code> <code>property</code> <p>The maximum number of tokens that can be fed to the model at once.</p> Functions <code>agenerate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Calls the model with the given messages and returns the response asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating async responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def agenerate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response\n    asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating async responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>astream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <code>async</code> <p>Streams the model with the given messages and returns the response, one token at a time, asynchronously.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\nasync def astream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time, asynchronously.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre> <code>generate(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Calls the model with the given messages and returns the response.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>This is the primary method for generating responses from the model, and is responsible for calling all of the lifecycle methods.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef generate(\n    self,\n    messages: Messages,\n    *args,\n    **kwargs,\n) -&gt; List[Notion]:\n    \"\"\"\n    Calls the model with the given messages and returns the response.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    This is the primary method for generating responses from the model,\n    and is responsible for calling all of the lifecycle methods.\n    \"\"\"\n    pass\n</code></pre> <code>stream(messages, *args, **kwargs)</code> <code>abstractmethod</code> <p>Streams the model with the given messages and returns the response, one token at a time.</p> <p>Messages can be any of: string, list of strings, Notion, list of Notions, or Idearium.</p> <p>If the model cannot be streamed, this will raise an exception.</p> Source code in <code>src\\SilverLingua\\core\\templates\\model.py</code> <pre><code>@abstractmethod\ndef stream(\n    self, messages: Messages, *args, **kwargs\n) -&gt; Generator[Notion, Any, None]:\n    \"\"\"\n    Streams the model with the given messages and returns the response,\n    one token at a time.\n\n    Messages can be any of:\n    string, list of strings, Notion, list of Notions, or Idearium.\n\n    If the model cannot be streamed, this will raise an exception.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/providers/anthropic/","title":"Anthropic Provider","text":"<p>options: members: true show_root_heading: true show_source: true show_submodules: true</p>"},{"location":"api/providers/anthropic/#SilverLingua.anthropic","title":"<code>SilverLingua.anthropic</code>","text":"<p>The Anthropic module provides implementations of SilverLingua's core components using the Anthropic API.</p> <p>This module includes: - AnthropicChatAgent: An agent that uses Anthropic's chat completion API - AnthropicModel: A model that uses Anthropic's API - AnthropicChatRole: Role definitions for Anthropic's chat format</p>"},{"location":"api/providers/anthropic/#SilverLingua.anthropic-classes","title":"Classes","text":""},{"location":"api/providers/openai/","title":"OpenAI Provider","text":"<p>options: members: true show_root_heading: true show_source: true show_submodules: true</p>"},{"location":"api/providers/openai/#SilverLingua.openai","title":"<code>SilverLingua.openai</code>","text":"<p>The OpenAI module provides implementations of SilverLingua's core components using the OpenAI API.</p> <p>This module includes: - OpenAIChatAgent: An agent that uses OpenAI's chat completion API - OpenAIModel: A model that uses OpenAI's API - OpenAIChatRole: Role definitions for OpenAI's chat format</p>"},{"location":"api/providers/openai/#SilverLingua.openai-classes","title":"Classes","text":""},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":""},{"location":"getting-started/quickstart/","title":"Quickstart","text":""},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#openai-example","title":"OpenAI Example","text":""}]}